{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "## å­¦ç¿’ç›®æ¨™\n",
    "- ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã™ã‚‹\n",
    "- æ¡ä»¶ä»˜ãåˆ†å¸ƒã®å°å‡ºæ–¹æ³•ã‚’å­¦ã¶\n",
    "- å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã§ã®å®Ÿè£…ã‚’ç¿’å¾—ã™ã‚‹\n",
    "- æ··åˆãƒ¢ãƒ‡ãƒ«ã‚„éšå±¤ãƒ¢ãƒ‡ãƒ«ã¸ã®å¿œç”¨ã‚’ç†è§£ã™ã‚‹\n",
    "- ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¦‚å¿µã‚’å­¦ã¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.1 ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šã€Œæ£„å´ã®ãªã„MCMCã€ã®å¨åŠ›\n\nã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€å¤šå¤‰é‡åˆ†å¸ƒ $p(x_1, x_2, ..., x_k)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã€æ¥µã‚ã¦ã‚¨ãƒ¬ã‚¬ãƒ³ãƒˆãªMCMCæ‰‹æ³•ã§ã™ã€‚ãã®æœ€å¤§ã®ç‰¹å¾´ã¯ã€Œ**å—ç†ç‡100%**ã€ã¨ã„ã†é©šç•°çš„ãªåŠ¹ç‡æ€§ã«ã‚ã‚Šã¾ã™ã€‚\n\n### åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢ï¼šè¤‡é›‘ãªå•é¡Œã®åˆ†è§£\n\né«˜æ¬¡å…ƒã®è¤‡é›‘ãªåŒæ™‚ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ã€ä¸€é€£ã®ã‚ˆã‚Šå˜ç´”ãª**1æ¬¡å…ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ã«åˆ†è§£ã™ã‚‹ã®ãŒæ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢ã§ã™ã€‚\n\n### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆkå¤‰æ•°ã®å ´åˆï¼‰\n1. **åˆæœŸå€¤è¨­å®š**: $(x_1^{(0)}, x_2^{(0)}, ..., x_k^{(0)})$ ã‚’é©å½“ã«è¨­å®š\n2. **å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** $t$ ã§ä»¥ä¸‹ã‚’é †æ¬¡å®Ÿè¡Œï¼š\n   - $x_1^{(t+1)} \\sim p(x_1 | x_2^{(t)}, x_3^{(t)}, ..., x_k^{(t)})$\n   - $x_2^{(t+1)} \\sim p(x_2 | x_1^{(t+1)}, x_3^{(t)}, ..., x_k^{(t)})$\n   - $\\vdots$\n   - $x_k^{(t+1)} \\sim p(x_k | x_1^{(t+1)}, x_2^{(t+1)}, ..., x_{k-1}^{(t+1)})$\n\nã“ã®ã‚¸ã‚°ã‚¶ã‚°ã¨ã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€ã‚µãƒ³ãƒ—ãƒ«åˆ—ã¯ç›®æ¨™ã¨ã™ã‚‹åŒæ™‚åˆ†å¸ƒã«åæŸã—ã¾ã™ã€‚\n\n### MHæ³•ã¨ã®æ ¹æœ¬çš„ãªé–¢ä¿‚\n\nã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€å®Ÿã¯**ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã®ç‰¹æ®Šã§åŠ¹ç‡çš„ãªã‚±ãƒ¼ã‚¹**ã§ã™ï¼š\n\n- **ææ¡ˆåˆ†å¸ƒ**: å®Œå…¨æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(x_i'|x_{-i}^{(t)})$ ãã®ã‚‚ã®ã‚’ä½¿ç”¨\n- **æ¡æŠç¢ºç‡**: $\\alpha = \\min(1, \\frac{p(x_i'|x_{-i}^{(t)}) \\cdot p(x_i'|x_{-i}^{(t)})}{p(x_i^{(t)}|x_{-i}^{(t)}) \\cdot p(x_i^{(t)}|x_{-i}^{(t)})}) = \\min(1, 1) = 1$\n\nã¤ã¾ã‚Šã€**ã™ã¹ã¦ã®ææ¡ˆãŒå¿…ãšæ¡æŠã•ã‚Œã‚‹ã€Œæ£„å´ã®ãªã„MHæ³•ã€**ãªã®ã§ã™ã€‚\n\n### é•·æ‰€ã¨åˆ¶ç´„\n\n**é•·æ‰€:**\n- âœ… **å—ç†ç‡100%**: ææ¡ˆåˆ†å¸ƒã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦\n- âœ… **è¨ˆç®—åŠ¹ç‡**: è¤‡é›‘ãªå—ç†ç¢ºç‡è¨ˆç®—ãŒä¸è¦\n- âœ… **è‡ªå‹•çš„ãªè©³ç´°é‡£ã‚Šåˆã„**: æ¡ä»¶ä»˜ãåˆ†å¸ƒã®æ€§è³ªã«ã‚ˆã‚Šè‡ªç„¶ã«æº€ãŸã•ã‚Œã‚‹\n\n**åˆ¶ç´„:**\n- âŒ **é©ç”¨ç¯„å›²ã®é™å®š**: ã™ã¹ã¦ã®å®Œå…¨æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒã€Œæ—¢çŸ¥ã®ã€ç¢ºç‡åˆ†å¸ƒã§ã‚ã‚‹å¿…è¦\n- âŒ **æ•°å­¦çš„é…æ…®**: å…±å½¹äº‹å‰åˆ†å¸ƒãªã©ã®æ…é‡ãªè¨­è¨ˆãŒå¿…è¦\n\nã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€Œé©ç”¨ã§ãã‚‹å ´åˆã«ã¯éå¸¸ã«åŠ¹ç‡çš„ã€ãªã€æ¡ä»¶ä»˜ãã®å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ä¾‹1ï¼š2å¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "ã¾ãšã€è§£æçš„ã«æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’å°å‡ºã§ãã‚‹2å¤‰é‡æ­£è¦åˆ†å¸ƒã§ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling_bivariate_normal(mu, cov, n_samples, initial_value=None):\n",
    "    \"\"\"\n",
    "    2å¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« [mu_x, mu_y]\n",
    "    - cov: å…±åˆ†æ•£è¡Œåˆ— [[var_x, cov_xy], [cov_xy, var_y]]\n",
    "    - n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    - initial_value: åˆæœŸå€¤ [x0, y0]\n",
    "    \n",
    "    Returns:\n",
    "    - samples: shape (n_samples, 2) ã®ã‚µãƒ³ãƒ—ãƒ«é…åˆ—\n",
    "    \"\"\"\n",
    "    samples = np.zeros((n_samples, 2))\n",
    "    \n",
    "    # æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’äº‹å‰è¨ˆç®—\n",
    "    mu_x, mu_y = mu[0], mu[1]\n",
    "    var_x, var_y = cov[0, 0], cov[1, 1]\n",
    "    cov_xy = cov[0, 1]\n",
    "    \n",
    "    # ç›¸é–¢ä¿‚æ•°\n",
    "    rho = cov_xy / np.sqrt(var_x * var_y)\n",
    "    \n",
    "    # æ¡ä»¶ä»˜ãåˆ†å¸ƒã®æ¨™æº–åå·®\n",
    "    sigma_x_given_y = np.sqrt(var_x * (1 - rho**2))\n",
    "    sigma_y_given_x = np.sqrt(var_y * (1 - rho**2))\n",
    "    \n",
    "    # åˆæœŸå€¤ã®è¨­å®š\n",
    "    if initial_value is None:\n",
    "        x, y = mu_x, mu_y\n",
    "    else:\n",
    "        x, y = initial_value[0], initial_value[1]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # x | y ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        # p(x|y) ~ N(mu_x + rho*(sigma_x/sigma_y)*(y - mu_y), sigma_x^2*(1-rho^2))\n",
    "        mu_x_given_y = mu_x + rho * np.sqrt(var_x / var_y) * (y - mu_y)\n",
    "        x = np.random.normal(mu_x_given_y, sigma_x_given_y)\n",
    "        \n",
    "        # y | x ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        # p(y|x) ~ N(mu_y + rho*(sigma_y/sigma_x)*(x - mu_x), sigma_y^2*(1-rho^2))\n",
    "        mu_y_given_x = mu_y + rho * np.sqrt(var_y / var_x) * (x - mu_x)\n",
    "        y = np.random.normal(mu_y_given_x, sigma_y_given_x)\n",
    "        \n",
    "        samples[i] = [x, y]\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "mu = np.array([1.0, 2.0])\n",
    "cov = np.array([[2.0, 1.5], [1.5, 3.0]])\n",
    "\n",
    "print(f\"ç›®æ¨™åˆ†å¸ƒã®å¹³å‡: {mu}\")\n",
    "print(f\"ç›®æ¨™åˆ†å¸ƒã®å…±åˆ†æ•£:\\n{cov}\")\n",
    "print(f\"ç›¸é–¢ä¿‚æ•°: {cov[0,1]/np.sqrt(cov[0,0]*cov[1,1]):.3f}\")\n",
    "\n",
    "# ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "samples_gibbs = gibbs_sampling_bivariate_normal(mu, cov, 10000)\n",
    "\n",
    "# çµæœã®çµ±è¨ˆ\n",
    "burnin = 1000\n",
    "sample_mean = np.mean(samples_gibbs[burnin:], axis=0)\n",
    "sample_cov = np.cov(samples_gibbs[burnin:].T)\n",
    "\n",
    "print(f\"\\nã‚µãƒ³ãƒ—ãƒ«å¹³å‡: {sample_mean}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£:\\n{sample_cov}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«ç›¸é–¢ä¿‚æ•°: {sample_cov[0,1]/np.sqrt(sample_cov[0,0]*sample_cov[1,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–\n",
    "def plot_gibbs_results(samples, mu, cov, title=\"Gibbs Sampling Results\"):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    burnin = len(samples) // 10\n",
    "    samples_clean = samples[burnin:]\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ã®è»Œè·¡ï¼ˆæœ€åˆã®500ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "    trajectory = samples[:500]\n",
    "    axes[0, 0].plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.7, linewidth=0.8)\n",
    "    axes[0, 0].plot(trajectory[:, 0], trajectory[:, 1], 'b.', markersize=2, alpha=0.8)\n",
    "    axes[0, 0].plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=8, label='Start')\n",
    "    axes[0, 0].plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=8, label='End')\n",
    "    axes[0, 0].set_title('Gibbs Sampling Trajectory (first 500)')\n",
    "    axes[0, 0].set_xlabel('X1')\n",
    "    axes[0, 0].set_ylabel('X2')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ•£å¸ƒå›³ã¨çœŸã®åˆ†å¸ƒã®ç­‰é«˜ç·š\n",
    "    axes[0, 1].scatter(samples_clean[::5, 0], samples_clean[::5, 1], alpha=0.6, s=1)\n",
    "    \n",
    "    # çœŸã®åˆ†å¸ƒã®ç­‰é«˜ç·š\n",
    "    x1_range = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 50)\n",
    "    x2_range = np.linspace(samples_clean[:, 1].min(), samples_clean[:, 1].max(), 50)\n",
    "    X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "    pos = np.dstack((X1, X2))\n",
    "    rv = stats.multivariate_normal(mu, cov)\n",
    "    axes[0, 1].contour(X1, X2, rv.pdf(pos), colors='red', alpha=0.8, linewidths=2)\n",
    "    axes[0, 1].set_title('Samples with True Distribution')\n",
    "    axes[0, 1].set_xlabel('X1')\n",
    "    axes[0, 1].set_ylabel('X2')\n",
    "    axes[0, 1].set_aspect('equal')\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    axes[0, 2].plot(samples[:2000, 0], alpha=0.7, label='X1', linewidth=0.8)\n",
    "    axes[0, 2].plot(samples[:2000, 1], alpha=0.7, label='X2', linewidth=0.8)\n",
    "    axes[0, 2].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n",
    "    axes[0, 2].set_title('Trace Plot')\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Value')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # X1ã®ãƒãƒ¼ã‚¸ãƒŠãƒ«åˆ†å¸ƒ\n",
    "    axes[1, 0].hist(samples_clean[:, 0], bins=50, density=True, alpha=0.7, \n",
    "                    color='lightblue', label='X1 samples')\n",
    "    x1_theory = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 100)\n",
    "    axes[1, 0].plot(x1_theory, stats.norm.pdf(x1_theory, mu[0], np.sqrt(cov[0, 0])), \n",
    "                    'r-', linewidth=2, label='X1 true')\n",
    "    axes[1, 0].set_title('Marginal Distribution X1')\n",
    "    axes[1, 0].set_xlabel('X1')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # X2ã®ãƒãƒ¼ã‚¸ãƒŠãƒ«åˆ†å¸ƒ\n",
    "    axes[1, 1].hist(samples_clean[:, 1], bins=50, density=True, alpha=0.7, \n",
    "                    color='lightgreen', label='X2 samples')\n",
    "    x2_theory = np.linspace(samples_clean[:, 1].min(), samples_clean[:, 1].max(), 100)\n",
    "    axes[1, 1].plot(x2_theory, stats.norm.pdf(x2_theory, mu[1], np.sqrt(cov[1, 1])), \n",
    "                    'r-', linewidth=2, label='X2 true')\n",
    "    axes[1, 1].set_title('Marginal Distribution X2')\n",
    "    axes[1, 1].set_xlabel('X2')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # è‡ªå·±ç›¸é–¢ï¼ˆX1ã«ã¤ã„ã¦ï¼‰\n",
    "    from statsmodels.tsa.stattools import acf\n",
    "    lags = min(100, len(samples_clean) // 10)\n",
    "    autocorr_x1 = acf(samples_clean[:, 0], nlags=lags, fft=True)\n",
    "    autocorr_x2 = acf(samples_clean[:, 1], nlags=lags, fft=True)\n",
    "    \n",
    "    axes[1, 2].plot(autocorr_x1, label='X1', alpha=0.8)\n",
    "    axes[1, 2].plot(autocorr_x2, label='X2', alpha=0.8)\n",
    "    axes[1, 2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].set_title('Autocorrelation Functions')\n",
    "    axes[1, 2].set_xlabel('Lag')\n",
    "    axes[1, 2].set_ylabel('ACF')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gibbs_results(samples_gibbs, mu, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã¨ã®æ¯”è¼ƒ\n",
    "\n",
    "åŒã˜åˆ†å¸ƒã«å¯¾ã—ã¦ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã‚‚é©ç”¨ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã®å®Ÿè£…ï¼ˆå‰ç« ã‹ã‚‰ï¼‰\n",
    "def multivariate_normal_log_pdf(x, mu, cov):\n",
    "    \"\"\"å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    k = len(mu)\n",
    "    diff = x - mu\n",
    "    \n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        log_det = 2 * np.sum(np.log(np.diag(chol)))\n",
    "        solve = np.linalg.solve(chol, diff)\n",
    "        mahalanobis_sq = np.sum(solve**2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf\n",
    "    \n",
    "    return -0.5 * (k * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n",
    "\n",
    "def mh_multivariate_normal(mu, cov, n_samples, step_size=0.5):\n",
    "    \"\"\"å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\"\"\"\n",
    "    samples = np.zeros((n_samples, len(mu)))\n",
    "    current = np.copy(mu)  # å¹³å‡ã‹ã‚‰é–‹å§‹\n",
    "    current_log_prob = multivariate_normal_log_pdf(current, mu, cov)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    cov_proposal = step_size * np.eye(len(mu))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # ææ¡ˆ\n",
    "        proposed = np.random.multivariate_normal(current, cov_proposal)\n",
    "        proposed_log_prob = multivariate_normal_log_pdf(proposed, mu, cov)\n",
    "        \n",
    "        # å—ç†ç¢ºç‡ï¼ˆå¯¾ç§°ææ¡ˆãªã®ã§ç°¡å˜ï¼‰\n",
    "        log_alpha = proposed_log_prob - current_log_prob\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # å—ç†/æ£„å´\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposed\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = current\n",
    "    \n",
    "    return samples, n_accepted / n_samples\n",
    "\n",
    "# MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "print(\"ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­...\")\n",
    "samples_mh, acceptance_rate = mh_multivariate_normal(mu, cov, 10000, step_size=0.8)\n",
    "\n",
    "print(f\"MHæ³•å—ç†ç‡: {acceptance_rate:.3f}\")\n",
    "\n",
    "# çµ±è¨ˆã®æ¯”è¼ƒ\n",
    "burnin = 1000\n",
    "\n",
    "# ã‚®ãƒ–ã‚¹çµ±è¨ˆ\n",
    "gibbs_mean = np.mean(samples_gibbs[burnin:], axis=0)\n",
    "gibbs_cov = np.cov(samples_gibbs[burnin:].T)\n",
    "\n",
    "# MHçµ±è¨ˆ\n",
    "mh_mean = np.mean(samples_mh[burnin:], axis=0)\n",
    "mh_cov = np.cov(samples_mh[burnin:].T)\n",
    "\n",
    "print(f\"\\n=== çµ±è¨ˆæ¯”è¼ƒ ===\")\n",
    "print(f\"çœŸã®å¹³å‡:     {mu}\")\n",
    "print(f\"ã‚®ãƒ–ã‚¹å¹³å‡:   {gibbs_mean}\")\n",
    "print(f\"MHå¹³å‡:       {mh_mean}\")\n",
    "print(f\"\\nçœŸã®å…±åˆ†æ•£:\")\n",
    "print(cov)\n",
    "print(f\"ã‚®ãƒ–ã‚¹å…±åˆ†æ•£:\")\n",
    "print(gibbs_cov)\n",
    "print(f\"MHå…±åˆ†æ•£:\")\n",
    "print(mh_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ¹ç‡ã®æ¯”è¼ƒ\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def compute_efficiency_metrics(samples, burnin_frac=0.1):\n",
    "    \"\"\"åŠ¹ç‡æŒ‡æ¨™ã®è¨ˆç®—\"\"\"\n",
    "    burnin = int(len(samples) * burnin_frac)\n",
    "    clean_samples = samples[burnin:]\n",
    "    \n",
    "    # å„æ¬¡å…ƒã®è‡ªå·±ç›¸é–¢æ™‚é–“ã‚’è¨ˆç®—\n",
    "    autocorr_times = []\n",
    "    eff_sample_sizes = []\n",
    "    \n",
    "    for dim in range(clean_samples.shape[1]):\n",
    "        data = clean_samples[:, dim]\n",
    "        lags = min(200, len(data) // 4)\n",
    "        autocorr = acf(data, nlags=lags, fft=True)\n",
    "        \n",
    "        # æœ€åˆã«é–¾å€¤ã‚’ä¸‹å›ã‚‹ãƒ©ã‚°ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        tau_int = 1\n",
    "        for lag in range(1, len(autocorr)):\n",
    "            if autocorr[lag] < 0.05:\n",
    "                tau_int = lag\n",
    "                break\n",
    "        \n",
    "        autocorr_times.append(tau_int)\n",
    "        eff_sample_sizes.append(len(data) / (2 * tau_int + 1))\n",
    "    \n",
    "    return autocorr_times, eff_sample_sizes\n",
    "\n",
    "# åŠ¹ç‡æ¯”è¼ƒ\n",
    "gibbs_autocorr, gibbs_eff = compute_efficiency_metrics(samples_gibbs)\n",
    "mh_autocorr, mh_eff = compute_efficiency_metrics(samples_mh)\n",
    "\n",
    "print(\"=== åŠ¹ç‡æ¯”è¼ƒ ===\")\n",
    "print(f\"{'Method':<10} {'Dim':<5} {'Autocorr Time':<15} {'Eff Sample Size':<18}\")\n",
    "print(\"-\" * 50)\n",
    "for dim in range(2):\n",
    "    print(f\"{'Gibbs':<10} {'X'+str(dim+1):<5} {gibbs_autocorr[dim]:<15d} {gibbs_eff[dim]:<18.1f}\")\n",
    "    print(f\"{'MH':<10} {'X'+str(dim+1):<5} {mh_autocorr[dim]:<15d} {mh_eff[dim]:<18.1f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"å¹³å‡åŠ¹ç‡ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º:\")\n",
    "print(f\"  ã‚®ãƒ–ã‚¹: {np.mean(gibbs_eff):.1f}\")\n",
    "print(f\"  MH:     {np.mean(mh_eff):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 ä¾‹2ï¼šæ··åˆæ­£è¦åˆ†å¸ƒã®æ¨å®š\n",
    "\n",
    "ã‚ˆã‚Šå®Ÿè·µçš„ãªä¾‹ã¨ã—ã¦ã€æ½œåœ¨å¤‰æ•°ã‚’æŒã¤æ··åˆæ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šã‚’ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¡Œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "def generate_mixture_data(n_samples, weights, means, variances):\n",
    "    \"\"\"\n",
    "    æ··åˆæ­£è¦åˆ†å¸ƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    - weights: æ··åˆé‡ã¿\n",
    "    - means: å„æˆåˆ†ã®å¹³å‡\n",
    "    - variances: å„æˆåˆ†ã®åˆ†æ•£\n",
    "    \"\"\"\n",
    "    n_components = len(weights)\n",
    "    \n",
    "    # æˆåˆ†ã®å‰²ã‚Šå½“ã¦\n",
    "    components = np.random.choice(n_components, size=n_samples, p=weights)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "    data = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        comp = components[i]\n",
    "        data[i] = np.random.normal(means[comp], np.sqrt(variances[comp]))\n",
    "    \n",
    "    return data, components\n",
    "\n",
    "# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "true_weights = np.array([0.3, 0.7])\n",
    "true_means = np.array([-2.0, 2.0])\n",
    "true_variances = np.array([0.5, 1.0])\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "n_obs = 200\n",
    "data, true_components = generate_mixture_data(n_obs, true_weights, true_means, true_variances)\n",
    "\n",
    "print(f\"çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
    "print(f\"  é‡ã¿: {true_weights}\")\n",
    "print(f\"  å¹³å‡: {true_means}\")\n",
    "print(f\"  åˆ†æ•£: {true_variances}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=30, density=True, alpha=0.7, color='lightblue')\n",
    "x_range = np.linspace(data.min(), data.max(), 1000)\n",
    "true_density = (true_weights[0] * stats.norm.pdf(x_range, true_means[0], np.sqrt(true_variances[0])) +\n",
    "                true_weights[1] * stats.norm.pdf(x_range, true_means[1], np.sqrt(true_variances[1])))\n",
    "plt.plot(x_range, true_density, 'r-', linewidth=2, label='True distribution')\n",
    "plt.title('Generated Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue']\n",
    "for k in range(2):\n",
    "    mask = true_components == k\n",
    "    plt.hist(data[mask], bins=15, alpha=0.7, color=colors[k], \n",
    "             label=f'Component {k+1}', density=True)\n",
    "plt.title('Data by True Components')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_mixture_gaussian(data, n_components, n_iterations, \n",
    "                          prior_alpha=1.0, prior_mu_var=10.0, \n",
    "                          prior_sigma_shape=1.0, prior_sigma_scale=1.0):\n",
    "    \"\"\"\n",
    "    æ··åˆæ­£è¦åˆ†å¸ƒã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    - data: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿\n",
    "    - n_components: æ··åˆæˆåˆ†æ•°\n",
    "    - n_iterations: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    - prior_*: äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "    n_obs = len(data)\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ç”¨é…åˆ—\n",
    "    weights_samples = np.zeros((n_iterations, n_components))\n",
    "    means_samples = np.zeros((n_iterations, n_components))\n",
    "    variances_samples = np.zeros((n_iterations, n_components))\n",
    "    components_samples = np.zeros((n_iterations, n_obs), dtype=int)\n",
    "    \n",
    "    # åˆæœŸå€¤\n",
    "    weights = np.ones(n_components) / n_components\n",
    "    means = np.random.normal(0, 2, n_components)\n",
    "    variances = np.ones(n_components)\n",
    "    components = np.random.choice(n_components, n_obs)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # 1. æ½œåœ¨å¤‰æ•°ï¼ˆæˆåˆ†å‰²ã‚Šå½“ã¦ï¼‰ã®æ›´æ–°\n",
    "        for i in range(n_obs):\n",
    "            # å„æˆåˆ†ã¸ã®æ‰€å±ç¢ºç‡ã‚’è¨ˆç®—\n",
    "            log_probs = np.zeros(n_components)\n",
    "            for k in range(n_components):\n",
    "                log_probs[k] = (np.log(weights[k]) + \n",
    "                               stats.norm.logpdf(data[i], means[k], np.sqrt(variances[k])))\n",
    "            \n",
    "            # å®‰å®šãªç¢ºç‡è¨ˆç®—\n",
    "            log_probs -= logsumexp(log_probs)\n",
    "            probs = np.exp(log_probs)\n",
    "            \n",
    "            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            components[i] = np.random.choice(n_components, p=probs)\n",
    "        \n",
    "        # 2. æ··åˆé‡ã¿ã®æ›´æ–°ï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬åˆ†å¸ƒã‹ã‚‰ï¼‰\n",
    "        counts = np.bincount(components, minlength=n_components)\n",
    "        weights = np.random.dirichlet(prior_alpha + counts)\n",
    "        \n",
    "        # 3. å¹³å‡ã®æ›´æ–°ï¼ˆæ­£è¦åˆ†å¸ƒã‹ã‚‰ï¼‰\n",
    "        for k in range(n_components):\n",
    "            mask = components == k\n",
    "            n_k = np.sum(mask)\n",
    "            \n",
    "            if n_k > 0:\n",
    "                data_k = data[mask]\n",
    "                sample_mean = np.mean(data_k)\n",
    "                \n",
    "                # äº‹å¾Œåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "                posterior_var = 1 / (1/prior_mu_var + n_k/variances[k])\n",
    "                posterior_mean = posterior_var * (n_k * sample_mean / variances[k])\n",
    "                \n",
    "                means[k] = np.random.normal(posterior_mean, np.sqrt(posterior_var))\n",
    "            else:\n",
    "                # ãƒ‡ãƒ¼ã‚¿ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„å ´åˆã¯äº‹å‰åˆ†å¸ƒã‹ã‚‰\n",
    "                means[k] = np.random.normal(0, np.sqrt(prior_mu_var))\n",
    "        \n",
    "        # 4. åˆ†æ•£ã®æ›´æ–°ï¼ˆé€†ã‚¬ãƒ³ãƒåˆ†å¸ƒã‹ã‚‰ï¼‰\n",
    "        for k in range(n_components):\n",
    "            mask = components == k\n",
    "            n_k = np.sum(mask)\n",
    "            \n",
    "            if n_k > 0:\n",
    "                data_k = data[mask]\n",
    "                ss = np.sum((data_k - means[k])**2)\n",
    "                \n",
    "                # äº‹å¾Œåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "                posterior_shape = prior_sigma_shape + n_k / 2\n",
    "                posterior_scale = prior_sigma_scale + ss / 2\n",
    "                \n",
    "                # é€†ã‚¬ãƒ³ãƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚¬ãƒ³ãƒã®é€†æ•°ï¼‰\n",
    "                variances[k] = 1 / np.random.gamma(posterior_shape, 1/posterior_scale)\n",
    "            else:\n",
    "                # ãƒ‡ãƒ¼ã‚¿ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„å ´åˆã¯äº‹å‰åˆ†å¸ƒã‹ã‚‰\n",
    "                variances[k] = 1 / np.random.gamma(prior_sigma_shape, 1/prior_sigma_scale)\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜\n",
    "        weights_samples[iteration] = weights\n",
    "        means_samples[iteration] = means\n",
    "        variances_samples[iteration] = variances\n",
    "        components_samples[iteration] = components\n",
    "        \n",
    "        if iteration % 200 == 0:\n",
    "            print(f\"Iteration {iteration}: weights={weights:.3f}, \"\n",
    "                  f\"means=[{means[0]:.2f}, {means[1]:.2f}], \"\n",
    "                  f\"vars=[{variances[0]:.2f}, {variances[1]:.2f}]\")\n",
    "    \n",
    "    return {\n",
    "        'weights': weights_samples,\n",
    "        'means': means_samples,\n",
    "        'variances': variances_samples,\n",
    "        'components': components_samples\n",
    "    }\n",
    "\n",
    "# ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "print(\"æ··åˆæ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šä¸­...\")\n",
    "results = gibbs_mixture_gaussian(data, n_components=2, n_iterations=2000)\n",
    "\n",
    "# çµæœã®çµ±è¨ˆ\n",
    "burnin = 500\n",
    "weights_post = results['weights'][burnin:]\n",
    "means_post = results['means'][burnin:]\n",
    "variances_post = results['variances'][burnin:]\n",
    "\n",
    "print(f\"\\n=== æ¨å®šçµæœ ===\")\n",
    "print(f\"é‡ã¿:\")\n",
    "print(f\"  çœŸå€¤:   {true_weights}\")\n",
    "print(f\"  æ¨å®šå€¤: {np.mean(weights_post, axis=0)}\")\n",
    "print(f\"å¹³å‡:\")\n",
    "print(f\"  çœŸå€¤:   {true_means}\")\n",
    "print(f\"  æ¨å®šå€¤: {np.mean(means_post, axis=0)}\")\n",
    "print(f\"åˆ†æ•£:\")\n",
    "print(f\"  çœŸå€¤:   {true_variances}\")\n",
    "print(f\"  æ¨å®šå€¤: {np.mean(variances_post, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··åˆãƒ¢ãƒ‡ãƒ«çµæœã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "iterations = np.arange(len(results['weights']))\n",
    "\n",
    "# é‡ã¿\n",
    "axes[0, 0].plot(iterations, results['weights'][:, 0], alpha=0.8, label='Component 1')\n",
    "axes[0, 0].plot(iterations, results['weights'][:, 1], alpha=0.8, label='Component 2')\n",
    "axes[0, 0].axhline(true_weights[0], color='red', linestyle='--', alpha=0.7)\n",
    "axes[0, 0].axhline(true_weights[1], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[0, 0].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "axes[0, 0].set_title('Mixture Weights')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Weight')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# å¹³å‡\n",
    "axes[0, 1].plot(iterations, results['means'][:, 0], alpha=0.8, label='Component 1')\n",
    "axes[0, 1].plot(iterations, results['means'][:, 1], alpha=0.8, label='Component 2')\n",
    "axes[0, 1].axhline(true_means[0], color='red', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].axhline(true_means[1], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "axes[0, 1].set_title('Component Means')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Mean')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# åˆ†æ•£\n",
    "axes[0, 2].plot(iterations, results['variances'][:, 0], alpha=0.8, label='Component 1')\n",
    "axes[0, 2].plot(iterations, results['variances'][:, 1], alpha=0.8, label='Component 2')\n",
    "axes[0, 2].axhline(true_variances[0], color='red', linestyle='--', alpha=0.7)\n",
    "axes[0, 2].axhline(true_variances[1], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[0, 2].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "axes[0, 2].set_title('Component Variances')\n",
    "axes[0, 2].set_xlabel('Iteration')\n",
    "axes[0, 2].set_ylabel('Variance')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# äº‹å¾Œåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n",
    "axes[1, 0].hist(weights_post[:, 0], bins=30, alpha=0.7, density=True, label='Component 1')\n",
    "axes[1, 0].hist(weights_post[:, 1], bins=30, alpha=0.7, density=True, label='Component 2')\n",
    "axes[1, 0].axvline(true_weights[0], color='red', linestyle='--', label='True values')\n",
    "axes[1, 0].axvline(true_weights[1], color='blue', linestyle='--')\n",
    "axes[1, 0].set_title('Posterior Distribution of Weights')\n",
    "axes[1, 0].set_xlabel('Weight')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].hist(means_post[:, 0], bins=30, alpha=0.7, density=True, label='Component 1')\n",
    "axes[1, 1].hist(means_post[:, 1], bins=30, alpha=0.7, density=True, label='Component 2')\n",
    "axes[1, 1].axvline(true_means[0], color='red', linestyle='--', label='True values')\n",
    "axes[1, 1].axvline(true_means[1], color='blue', linestyle='--')\n",
    "axes[1, 1].set_title('Posterior Distribution of Means')\n",
    "axes[1, 1].set_xlabel('Mean')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# æ¨å®šåˆ†å¸ƒ vs çœŸã®åˆ†å¸ƒ\n",
    "x_range = np.linspace(data.min(), data.max(), 1000)\n",
    "\n",
    "# äº‹å¾Œå¹³å‡ã‚’ä½¿ã£ãŸæ¨å®šåˆ†å¸ƒ\n",
    "est_weights = np.mean(weights_post, axis=0)\n",
    "est_means = np.mean(means_post, axis=0)\n",
    "est_variances = np.mean(variances_post, axis=0)\n",
    "\n",
    "est_density = (est_weights[0] * stats.norm.pdf(x_range, est_means[0], np.sqrt(est_variances[0])) +\n",
    "               est_weights[1] * stats.norm.pdf(x_range, est_means[1], np.sqrt(est_variances[1])))\n",
    "\n",
    "true_density = (true_weights[0] * stats.norm.pdf(x_range, true_means[0], np.sqrt(true_variances[0])) +\n",
    "                true_weights[1] * stats.norm.pdf(x_range, true_means[1], np.sqrt(true_variances[1])))\n",
    "\n",
    "axes[1, 2].hist(data, bins=30, density=True, alpha=0.7, color='lightgray', label='Data')\n",
    "axes[1, 2].plot(x_range, true_density, 'r-', linewidth=2, label='True distribution')\n",
    "axes[1, 2].plot(x_range, est_density, 'b--', linewidth=2, label='Estimated distribution')\n",
    "axes[1, 2].set_title('Distribution Comparison')\n",
    "axes[1, 2].set_xlabel('Value')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "å¤‰æ•°ã‚’å€‹åˆ¥ã«æ›´æ–°ã™ã‚‹ä»£ã‚ã‚Šã«ã€å¤‰æ•°ã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_gibbs_bivariate_normal(mu, cov, n_samples, block_prob=0.5):\n",
    "    \"\"\"\n",
    "    ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ2å¤‰é‡æ­£è¦åˆ†å¸ƒï¼‰\n",
    "    \n",
    "    Parameters:\n",
    "    - mu, cov: åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    - n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    - block_prob: ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–°ã‚’è¡Œã†ç¢ºç‡\n",
    "    \"\"\"\n",
    "    samples = np.zeros((n_samples, 2))\n",
    "    \n",
    "    # æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ï¼‰\n",
    "    mu_x, mu_y = mu[0], mu[1]\n",
    "    var_x, var_y = cov[0, 0], cov[1, 1]\n",
    "    cov_xy = cov[0, 1]\n",
    "    rho = cov_xy / np.sqrt(var_x * var_y)\n",
    "    sigma_x_given_y = np.sqrt(var_x * (1 - rho**2))\n",
    "    sigma_y_given_x = np.sqrt(var_y * (1 - rho**2))\n",
    "    \n",
    "    # åˆæœŸå€¤\n",
    "    x, y = mu_x, mu_y\n",
    "    \n",
    "    block_updates = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if np.random.rand() < block_prob:\n",
    "            # ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–°ï¼šä¸¡å¤‰æ•°ã‚’åŒæ™‚ã«æ›´æ–°\n",
    "            new_sample = np.random.multivariate_normal(mu, cov)\n",
    "            x, y = new_sample[0], new_sample[1]\n",
    "            block_updates += 1\n",
    "        else:\n",
    "            # é€šå¸¸ã®ã‚®ãƒ–ã‚¹æ›´æ–°\n",
    "            # x | y\n",
    "            mu_x_given_y = mu_x + rho * np.sqrt(var_x / var_y) * (y - mu_y)\n",
    "            x = np.random.normal(mu_x_given_y, sigma_x_given_y)\n",
    "            \n",
    "            # y | x\n",
    "            mu_y_given_x = mu_y + rho * np.sqrt(var_y / var_x) * (x - mu_x)\n",
    "            y = np.random.normal(mu_y_given_x, sigma_y_given_x)\n",
    "        \n",
    "        samples[i] = [x, y]\n",
    "    \n",
    "    print(f\"ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–°ã®å‰²åˆ: {block_updates / n_samples:.2%}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# é«˜ã„ç›¸é–¢ã‚’æŒã¤åˆ†å¸ƒã§ãƒ†ã‚¹ãƒˆ\n",
    "high_corr_cov = np.array([[1.0, 0.95], [0.95, 1.0]])\n",
    "\n",
    "print(\"é«˜ç›¸é–¢åˆ†å¸ƒã§ã®æ¯”è¼ƒ...\")\n",
    "print(f\"ç›¸é–¢ä¿‚æ•°: {high_corr_cov[0,1]/np.sqrt(high_corr_cov[0,0]*high_corr_cov[1,1]):.3f}\")\n",
    "\n",
    "# é€šå¸¸ã®ã‚®ãƒ–ã‚¹\n",
    "samples_gibbs_high = gibbs_sampling_bivariate_normal(mu, high_corr_cov, 5000)\n",
    "\n",
    "# ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ï¼ˆ50%ã®ç¢ºç‡ã§ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–°ï¼‰\n",
    "samples_block_gibbs = block_gibbs_bivariate_normal(mu, high_corr_cov, 5000, block_prob=0.5)\n",
    "\n",
    "# åŠ¹ç‡æ¯”è¼ƒ\n",
    "gibbs_high_autocorr, gibbs_high_eff = compute_efficiency_metrics(samples_gibbs_high)\n",
    "block_autocorr, block_eff = compute_efficiency_metrics(samples_block_gibbs)\n",
    "\n",
    "print(f\"\\n=== é«˜ç›¸é–¢ã§ã®åŠ¹ç‡æ¯”è¼ƒ ===\")\n",
    "print(f\"{'Method':<15} {'Dim':<5} {'Autocorr Time':<15} {'Eff Sample Size':<18}\")\n",
    "print(\"-\" * 55)\n",
    "for dim in range(2):\n",
    "    print(f\"{'Regular Gibbs':<15} {'X'+str(dim+1):<5} {gibbs_high_autocorr[dim]:<15d} {gibbs_high_eff[dim]:<18.1f}\")\n",
    "    print(f\"{'Block Gibbs':<15} {'X'+str(dim+1):<5} {block_autocorr[dim]:<15d} {block_eff[dim]:<18.1f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"å¹³å‡åŠ¹ç‡ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º:\")\n",
    "print(f\"  é€šå¸¸ã‚®ãƒ–ã‚¹:   {np.mean(gibbs_high_eff):.1f}\")\n",
    "print(f\"  ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹: {np.mean(block_eff):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 æ¼”ç¿’å•é¡Œ\n",
    "\n",
    "### å•é¡Œ1ï¼š3å¤‰é‡æ­£è¦åˆ†å¸ƒã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "3å¤‰é‡æ­£è¦åˆ†å¸ƒã«å¯¾ã—ã¦ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ãªã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ1ã®è§£ç­”æ¬„\n",
    "def gibbs_sampling_trivariate_normal(mu, cov, n_samples):\n",
    "    \"\"\"\n",
    "    3å¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \n",
    "    ãƒ’ãƒ³ãƒˆï¼š\n",
    "    3å¤‰é‡æ­£è¦åˆ†å¸ƒ N(Î¼, Î£) ã«ãŠã„ã¦ã€\n",
    "    X1 | X2, X3 ~ N(Î¼1 + Î£12 Î£22^-1 (X23 - Î¼23), Î£11 - Î£12 Î£22^-1 Î£21)\n",
    "    ã“ã“ã§ X23 = [X2, X3]^T, Î¼23 = [Î¼2, Î¼3]^T\n",
    "    \"\"\"\n",
    "    # ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "    pass  # å­¦ç¿’è€…ãŒå®Ÿè£…\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "mu_3d = np.array([1.0, 2.0, 3.0])\n",
    "cov_3d = np.array([\n",
    "    [2.0, 0.8, 0.3],\n",
    "    [0.8, 1.5, 0.6],\n",
    "    [0.3, 0.6, 1.0]\n",
    "])\n",
    "\n",
    "# samples_3d = gibbs_sampling_trivariate_normal(mu_3d, cov_3d, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### å•é¡Œ2ï¼šãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å®Œå…¨å®Ÿè£…\n\næ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§è©³ã—ãè§£èª¬ã•ã‚Œã¦ã„ãŸãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã‚’å®Œå…¨å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n#### ãƒ¢ãƒ‡ãƒ«è¨­å®š\n- **å°¤åº¦**: $y = X\\beta + \\epsilon$, $\\epsilon \\sim N(0, \\sigma^2 I)$\n- **å›å¸°ä¿‚æ•°ã®äº‹å‰åˆ†å¸ƒ**: $\\beta \\sim N(0, \\tau_0^{-1} I)$  \n- **ç²¾åº¦ã®äº‹å‰åˆ†å¸ƒ**: $\\tau = 1/\\sigma^2 \\sim \\text{Gamma}(\\alpha, \\beta)$\n\n#### å®Œå…¨æ¡ä»¶ä»˜ãåˆ†å¸ƒ\nã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ ¸å¿ƒã¯ã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®Œå…¨æ¡ä»¶ä»˜ãäº‹å¾Œåˆ†å¸ƒã‚’å°å‡ºã™ã‚‹ã“ã¨ã§ã™ï¼š\n\n1. **$p(\\beta|\\tau, y, X)$**: å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ\n2. **$p(\\tau|\\beta, y, X)$**: ã‚¬ãƒ³ãƒåˆ†å¸ƒ\n\nã“ã‚Œã‚‰ã®åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ‡ãƒ¼ã‚¿ã¨ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¾åœ¨å€¤ã‹ã‚‰è¨ˆç®—ã§ãã¾ã™ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gibbs_bayesian_regression(X, y, n_iterations, \n                             prior_beta_precision=0.0001, \n                             prior_tau_shape=2.0, prior_tau_rate=1.0):\n    \"\"\"\n    ç·šå½¢å›å¸°ã®ãƒ™ã‚¤ã‚ºæ¨å®šï¼ˆã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰\n    \n    ãƒ¢ãƒ‡ãƒ«: y = X Î² + Îµ, Îµ ~ N(0, Ï„^{-1})\n    äº‹å‰åˆ†å¸ƒ: \n    - Î² ~ N(0, prior_beta_precision^{-1} * I)\n    - Ï„ ~ Gamma(prior_tau_shape, prior_tau_rate)\n    \n    Parameters:\n    - X: è¨ˆç”»è¡Œåˆ— (n Ã— p)\n    - y: è¦³æ¸¬å€¤ (n Ã— 1)\n    - n_iterations: åå¾©å›æ•°\n    - prior_*: äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n    \n    Returns:\n    - samples: {'beta': beta_samples, 'tau': tau_samples, 'sigma': sigma_samples}\n    \"\"\"\n    n, p = X.shape\n    \n    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨\n    beta_samples = np.zeros((n_iterations, p))\n    tau_samples = np.zeros(n_iterations)\n    sigma_samples = np.zeros(n_iterations)\n    \n    # åˆæœŸå€¤\n    beta_current = np.zeros(p)\n    tau_current = 1.0\n    \n    # äº‹å‰è¨ˆç®—ï¼ˆè¨ˆç®—åŠ¹ç‡ã®ãŸã‚ï¼‰\n    XtX = X.T @ X\n    Xty = X.T @ y\n    \n    for i in range(n_iterations):\n        # 1. Î² | Ï„, y ã®æ›´æ–°ï¼ˆå¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ï¼‰\n        # äº‹å¾Œç²¾åº¦è¡Œåˆ—ã¨å¹³å‡\n        posterior_precision = prior_beta_precision * np.eye(p) + tau_current * XtX\n        try:\n            posterior_cov = np.linalg.inv(posterior_precision)\n            posterior_mean = posterior_cov @ (tau_current * Xty)\n            \n            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n            beta_current = np.random.multivariate_normal(posterior_mean, posterior_cov)\n        except np.linalg.LinAlgError:\n            # æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆã¯Choleskyã‚’ä½¿ç”¨\n            try:\n                L = np.linalg.cholesky(posterior_precision)\n                z = np.random.randn(p)\n                v = np.linalg.solve(L, tau_current * Xty)\n                w = np.linalg.solve(L, z)\n                beta_current = np.linalg.solve(L.T, v) + np.linalg.solve(L.T, w)\n            except:\n                # ãã‚Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯å‰ã®å€¤ã‚’ä¿æŒ\n                pass\n        \n        # 2. Ï„ | Î², y ã®æ›´æ–°ï¼ˆã‚¬ãƒ³ãƒåˆ†å¸ƒã‹ã‚‰ï¼‰\n        # æ®‹å·®ã®è¨ˆç®—\n        residuals = y - X @ beta_current\n        sum_squared_residuals = np.sum(residuals**2)\n        \n        # äº‹å¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n        posterior_shape = prior_tau_shape + n / 2.0\n        posterior_rate = prior_tau_rate + sum_squared_residuals / 2.0\n        \n        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n        tau_current = np.random.gamma(posterior_shape, 1.0 / posterior_rate)\n        \n        # çµæœã®ä¿å­˜\n        beta_samples[i] = beta_current\n        tau_samples[i] = tau_current\n        sigma_samples[i] = 1.0 / np.sqrt(tau_current)\n        \n        if i > 0 and i % 500 == 0:\n            print(f\"Iteration {i}: Î²Ì‚ = {beta_current[:3]}, ÏƒÌ‚ = {1/np.sqrt(tau_current):.3f}\")\n    \n    return {\n        'beta': beta_samples,\n        'tau': tau_samples,\n        'sigma': sigma_samples\n    }\n\n# åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\nnp.random.seed(42)\nn_obs = 100\nn_predictors = 3\n\n# è¨­è¨ˆè¡Œåˆ—ï¼ˆåˆ‡ç‰‡é …ã‚’å«ã‚€ï¼‰\nX_reg = np.column_stack([\n    np.ones(n_obs),  # åˆ‡ç‰‡\n    np.random.randn(n_obs),  # äºˆæ¸¬å­1\n    np.random.randn(n_obs)   # äºˆæ¸¬å­2\n])\n\n# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\ntrue_beta = np.array([2.5, 1.8, -1.2])  # [åˆ‡ç‰‡, å‚¾ã1, å‚¾ã2]\ntrue_sigma = 1.5\n\n# è¦³æ¸¬å€¤ã®ç”Ÿæˆ\ny_reg = X_reg @ true_beta + np.random.normal(0, true_sigma, n_obs)\n\nprint(f\"=== ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ ===\")\nprint(f\"ã‚µãƒ³ãƒ—ãƒ«æ•°: {n_obs}\")\nprint(f\"çœŸã®å›å¸°ä¿‚æ•°: {true_beta}\")\nprint(f\"çœŸã®èª¤å·®æ¨™æº–åå·®: {true_sigma}\")\n\n# ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\nprint(f\"\\n=== ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ ===\")\nresults_reg = gibbs_bayesian_regression(X_reg, y_reg, 5000)\n\n# çµæœã®åˆ†æ\nburnin = 1000\nbeta_post = results_reg['beta'][burnin:]\nsigma_post = results_reg['sigma'][burnin:]\n\nprint(f\"\\n=== æ¨å®šçµæœ ===\")\nprint(f\"äº‹å¾Œå¹³å‡ï¼ˆå›å¸°ä¿‚æ•°ï¼‰:\")\nfor i, (true_val, est_val) in enumerate(zip(true_beta, np.mean(beta_post, axis=0))):\n    param_name = \"åˆ‡ç‰‡\" if i == 0 else f\"å‚¾ã{i}\"\n    print(f\"  {param_name:>4}: çœŸå€¤={true_val:6.2f}, æ¨å®šå€¤={est_val:6.2f}\")\n\nprint(f\"\\nèª¤å·®æ¨™æº–åå·®:\")\nprint(f\"  çœŸå€¤={true_sigma:6.2f}, æ¨å®šå€¤={np.mean(sigma_post):6.2f}\")\n\n# ä¿¡é ¼åŒºé–“\nprint(f\"\\n=== 95%ä¿¡é ¼åŒºé–“ ===\")\nfor i, true_val in enumerate(true_beta):\n    param_name = \"åˆ‡ç‰‡\" if i == 0 else f\"å‚¾ã{i}\"\n    ci_lower = np.percentile(beta_post[:, i], 2.5)\n    ci_upper = np.percentile(beta_post[:, i], 97.5)\n    in_ci = ci_lower <= true_val <= ci_upper\n    print(f\"  {param_name:>4}: [{ci_lower:6.2f}, {ci_upper:6.2f}] {'âœ“' if in_ci else 'âœ—'}\")\n\nsigma_ci_lower = np.percentile(sigma_post, 2.5)\nsigma_ci_upper = np.percentile(sigma_post, 97.5)\nsigma_in_ci = sigma_ci_lower <= true_sigma <= sigma_ci_upper\nprint(f\"  {'Ïƒ':>4}: [{sigma_ci_lower:6.2f}, {sigma_ci_upper:6.2f}] {'âœ“' if sigma_in_ci else 'âœ—'}\")"
  },
  {
   "cell_type": "code",
   "source": "# ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°çµæœã®å¯è¦–åŒ–\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\nparam_names = ['åˆ‡ç‰‡', 'å‚¾ã1', 'å‚¾ã2']\nfor i in range(3):\n    axes[0, i].plot(results_reg['beta'][:, i], alpha=0.8, linewidth=0.8)\n    axes[0, i].axhline(true_beta[i], color='red', linestyle='--', linewidth=2, \n                       label=f'çœŸå€¤ = {true_beta[i]:.2f}')\n    axes[0, i].axvline(burnin, color='gray', linestyle=':', alpha=0.7, label='Burn-in')\n    axes[0, i].set_title(f'{param_names[i]}ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ')\n    axes[0, i].set_xlabel('Iteration')\n    axes[0, i].set_ylabel(f'{param_names[i]}')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n\n# äº‹å¾Œåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \nfor i in range(3):\n    axes[1, i].hist(beta_post[:, i], bins=50, density=True, alpha=0.7, \n                    color=f'C{i}', label=f'{param_names[i]} äº‹å¾Œåˆ†å¸ƒ')\n    axes[1, i].axvline(true_beta[i], color='red', linestyle='--', linewidth=2,\n                       label=f'çœŸå€¤ = {true_beta[i]:.2f}')\n    axes[1, i].axvline(np.mean(beta_post[:, i]), color='blue', linestyle='-', linewidth=2,\n                       label=f'äº‹å¾Œå¹³å‡ = {np.mean(beta_post[:, i]):.2f}')\n    axes[1, i].set_title(f'{param_names[i]}ã®äº‹å¾Œåˆ†å¸ƒ')\n    axes[1, i].set_xlabel(f'{param_names[i]}')\n    axes[1, i].set_ylabel('å¯†åº¦')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# äºˆæ¸¬ã¨æ®‹å·®ã®åˆ†æ\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# è¦³æ¸¬å€¤ vs äºˆæ¸¬å€¤\ny_pred_mean = X_reg @ np.mean(beta_post, axis=0)\naxes[0].scatter(y_reg, y_pred_mean, alpha=0.7)\nmin_val = min(y_reg.min(), y_pred_mean.min())\nmax_val = max(y_reg.max(), y_pred_mean.max())\naxes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='y=x')\naxes[0].set_xlabel('è¦³æ¸¬å€¤')\naxes[0].set_ylabel('äºˆæ¸¬å€¤ï¼ˆäº‹å¾Œå¹³å‡ï¼‰')\naxes[0].set_title('è¦³æ¸¬å€¤ vs äºˆæ¸¬å€¤')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ\nresiduals_mean = y_reg - y_pred_mean\naxes[1].scatter(y_pred_mean, residuals_mean, alpha=0.7)\naxes[1].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[1].set_xlabel('äºˆæ¸¬å€¤')\naxes[1].set_ylabel('æ®‹å·®')\naxes[1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')\naxes[1].grid(True, alpha=0.3)\n\n# Ïƒã®äº‹å¾Œåˆ†å¸ƒ\naxes[2].hist(sigma_post, bins=50, density=True, alpha=0.7, color='orange')\naxes[2].axvline(true_sigma, color='red', linestyle='--', linewidth=2,\n                label=f'çœŸå€¤ = {true_sigma:.2f}')\naxes[2].axvline(np.mean(sigma_post), color='blue', linestyle='-', linewidth=2,\n                label=f'äº‹å¾Œå¹³å‡ = {np.mean(sigma_post):.2f}')\naxes[2].set_title('Ïƒã®äº‹å¾Œåˆ†å¸ƒ')\naxes[2].set_xlabel('Ïƒ')\naxes[2].set_ylabel('å¯†åº¦')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# äºˆæ¸¬åŒºé–“ã®å¯è¦–åŒ–ï¼ˆ1æ¬¡å…ƒã®å ´åˆã®ä¾‹ï¼‰\nif X_reg.shape[1] == 3:  # åˆ‡ç‰‡ + 2ã¤ã®äºˆæ¸¬å­ã®å ´åˆ\n    # ç¬¬1äºˆæ¸¬å­ã‚’å›ºå®šã—ã¦ç¬¬2äºˆæ¸¬å­ã¨ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–\n    x1_fixed = 0  # ç¬¬1äºˆæ¸¬å­ã‚’0ã«å›ºå®š\n    x2_range = np.linspace(-3, 3, 50)\n    X_pred = np.column_stack([np.ones(len(x2_range)), \n                             np.full(len(x2_range), x1_fixed), \n                             x2_range])\n    \n    # äº‹å¾Œã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰äºˆæ¸¬åˆ†å¸ƒã‚’è¨ˆç®—\n    n_pred_samples = 100\n    pred_samples = np.zeros((n_pred_samples, len(x2_range)))\n    \n    for i in range(n_pred_samples):\n        idx = np.random.randint(len(beta_post))\n        beta_sample = beta_post[idx]\n        sigma_sample = sigma_post[idx]\n        \n        mean_pred = X_pred @ beta_sample\n        pred_samples[i] = np.random.normal(mean_pred, sigma_sample)\n    \n    # äºˆæ¸¬åŒºé–“ã®è¨ˆç®—\n    pred_mean = np.mean(pred_samples, axis=0)\n    pred_lower = np.percentile(pred_samples, 2.5, axis=0)\n    pred_upper = np.percentile(pred_samples, 97.5, axis=0)\n    \n    plt.figure(figsize=(10, 6))\n    plt.fill_between(x2_range, pred_lower, pred_upper, alpha=0.3, color='lightblue', \n                     label='95%äºˆæ¸¬åŒºé–“')\n    plt.plot(x2_range, pred_mean, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')\n    \n    # ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç¬¬1äºˆæ¸¬å­ãŒ0ã«è¿‘ã„ã‚‚ã®ï¼‰\n    mask = np.abs(X_reg[:, 1] - x1_fixed) < 0.5\n    if np.any(mask):\n        plt.scatter(X_reg[mask, 2], y_reg[mask], color='red', alpha=0.7, \n                   label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿', s=50)\n    \n    plt.xlabel('äºˆæ¸¬å­2')\n    plt.ylabel('å¿œç­”å¤‰æ•°')\n    plt.title(f'äºˆæ¸¬åŒºé–“ï¼ˆäºˆæ¸¬å­1 = {x1_fixed}ã§å›ºå®šï¼‰')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ã¾ã¨ã‚ï¼šã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œå…¨ç†è§£\n\nã“ã®ç« ã§ã¯ã€ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸï¼š\n\n### ğŸ§  æ ¸å¿ƒçš„ç†è§£\n\n1. **åŸºæœ¬åŸç†**ï¼šè¤‡é›‘ãªåŒæ™‚åˆ†å¸ƒã‚’æ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã«åˆ†è§£\n2. **æ•°å­¦çš„ç¾ã—ã•**ï¼šã€Œå—ç†ç‡100%ã€ã®é©šç•°çš„åŠ¹ç‡æ€§\n3. **MHæ³•ã¨ã®é–¢ä¿‚**ï¼šè©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ç‰¹æ®ŠãªMHæ³•\n\n### ğŸ”§ å®Ÿè·µçš„ã‚¹ã‚­ãƒ«\n\n4. **å®Ÿè£…æŠ€è¡“**ï¼š\n   - å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã§ã®è§£æçš„å°å‡º\n   - æ··åˆãƒ¢ãƒ‡ãƒ«ã§ã®æ½œåœ¨å¤‰æ•°ã®æ‰±ã„\n   - ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–\n   - ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã§ã®å®Œå…¨ãªäº‹å¾Œåˆ†å¸ƒæ¨å®š\n\n### ğŸ“Š é‡è¦ãªå¿œç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n5. **é©ç”¨å ´é¢**ï¼š\n   - **å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ**: è§£æè§£ãŒåˆ©ç”¨ã§ãã‚‹ç†æƒ³çš„ã‚±ãƒ¼ã‚¹\n   - **æ··åˆãƒ¢ãƒ‡ãƒ«**: æ½œåœ¨å¤‰æ•°ã¨è¦³æ¸¬å¤‰æ•°ã®äº¤äº’æ›´æ–°\n   - **éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å±¤ã®åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n   - **å›å¸°åˆ†æ**: å…±å½¹äº‹å‰åˆ†å¸ƒã«ã‚ˆã‚‹é«˜é€Ÿæ¨å®š\n\n## ğŸ¯ å®Ÿè·µçš„MCMCã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠã‚¬ã‚¤ãƒ‰\n\n### æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ\n\n```\nå•é¡Œè¨­å®š\n    â†“\nã™ã¹ã¦ã®å®Œå…¨æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒæ—¢çŸ¥ï¼Ÿ\n    â†“           â†“\n   Yes         No\n    â†“           â†“\nã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°   å¤‰æ•°é–“ã®ç›¸é–¢ã¯ï¼Ÿ\n    â†“           â†“        â†“\næ€§èƒ½ã¯æº€è¶³ï¼Ÿ    é«˜ã„      ä½ã„\n    â†“           â†“        â†“\n   Yes   ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹   MHæ³•\n    â†“           â†“        â†“\n  å®Œäº†      åŠ¹æœã‚ã‚Šï¼Ÿ    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰\n              â†“        æ¤œè¨\n             Yes\n              â†“\n            å®Œäº†\n```\n\n### ğŸ“‹ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠãƒãƒˆãƒªãƒƒã‚¯ã‚¹\n\n| çŠ¶æ³ | æ¨å¥¨æ‰‹æ³• | ç†ç”± | æ³¨æ„ç‚¹ |\n|------|----------|------|--------|\n| **ã™ã¹ã¦å…±å½¹** | ã‚®ãƒ–ã‚¹ | å—ç†ç‡100% | é«˜ç›¸é–¢æ™‚ã¯è¦æ³¨æ„ |\n| **ä¸€éƒ¨éå…±å½¹** | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ | åŠ¹ç‡ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ | å®Ÿè£…è¤‡é›‘åº¦up |\n| **é«˜æ¬¡å…ƒãƒ»é«˜ç›¸é–¢** | ãƒ–ãƒ­ãƒƒã‚¯ã‚®ãƒ–ã‚¹ | ç›¸é–¢æ§‹é€ ã‚’è€ƒæ…® | ãƒ–ãƒ­ãƒƒã‚¯è¨­è¨ˆãŒé‡è¦ |\n| **è¤‡é›‘å°¤åº¦** | MHæ³• | æ±ç”¨æ€§ãŒé«˜ã„ | ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¿…è¦ |\n| **éšå±¤æ§‹é€ ** | ã‚®ãƒ–ã‚¹ | è‡ªç„¶ãªåˆ†è§£å¯èƒ½ | ãƒ¬ãƒ™ãƒ«é–“ã®è¨­è¨ˆ |\n| **æ··åˆãƒ¢ãƒ‡ãƒ«** | ã‚®ãƒ–ã‚¹ | æ½œåœ¨å¤‰æ•°ãŒè‡ªç„¶ | ãƒ©ãƒ™ãƒ«ã‚¹ã‚¤ãƒƒãƒãƒ³ã‚° |\n\n### ğŸš€ æ€§èƒ½æœ€é©åŒ–ã®æˆ¦ç•¥\n\n#### Phase 1: åŸºæœ¬å®Ÿè£…\n1. **ç´”ç²‹ã‚®ãƒ–ã‚¹**: æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒã™ã¹ã¦æ—¢çŸ¥ãªã‚‰æœ€åˆã«è©¦ã™\n2. **è¨ºæ–­**: åæŸã¨æ··åˆã®ç¢ºèª\n3. **å•é¡Œç‰¹å®š**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç™ºè¦‹\n\n#### Phase 2: é«˜åº¦åŒ–\n1. **ãƒ–ãƒ­ãƒƒã‚¯åŒ–**: ç›¸é–¢ã®ã‚ã‚‹å¤‰æ•°ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n2. **é©å¿œçš„æ›´æ–°**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å¿œã˜ãŸå‹•çš„èª¿æ•´\n3. **ä¸¦åˆ—åŒ–**: ç‹¬ç«‹ãªéƒ¨åˆ†ã®åŒæ™‚å‡¦ç†\n\n#### Phase 3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰\n1. **éƒ¨åˆ†MH**: å›°é›£ãªå¤‰æ•°ã®ã¿MHæ³•ã«\n2. **éšå±¤æœ€é©åŒ–**: ãƒ¬ãƒ™ãƒ«åˆ¥ã®æ‰‹æ³•é¸æŠ\n3. **å‹•çš„åˆ‡ã‚Šæ›¿ãˆ**: æ¡ä»¶ã«å¿œã˜ãŸæ‰‹æ³•å¤‰æ›´\n\n### âš¡ åŠ¹ç‡æ€§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æŒ‡æ¨™\n\n| æŒ‡æ¨™ | è¨ˆç®—å¼ | ç›®æ¨™å€¤ | \n|------|--------|--------|\n| **æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º** | N / (2Ï„ + 1) | > 400 |\n| **æ··åˆåŠ¹ç‡** | ESS / è¨ˆç®—æ™‚é–“ | å•é¡Œä¾å­˜ |\n| **åæŸé€Ÿåº¦** | R-hat < 1.01 é”æˆæ™‚é–“ | æ—©ã„ã»ã©è‰¯ã„ |\n\n### ğŸ¨ å®Ÿè£…å“è³ªã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n\n#### æ•°å€¤å®‰å®šæ€§\n- [ ] å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—ã§æ•°å€¤ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿\n- [ ] æ¡ä»¶ä»˜ãåˆ†æ•£ã®æ­£å®šå€¤æ€§ç¢ºä¿\n- [ ] ä¾‹å¤–å‡¦ç†ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹\n\n#### ã‚³ãƒ¼ãƒ‰å“è³ª\n- [ ] ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆï¼šå„æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ã®ç‹¬ç«‹åŒ–\n- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ï¼šå®Ÿè¡Œæ™‚è¨ºæ–­æ©Ÿèƒ½\n- [ ] æ‹¡å¼µæ€§ï¼šæ–°ã—ã„å¤‰æ•°ã®å®¹æ˜“ãªè¿½åŠ \n\n#### è¨ºæ–­æ©Ÿèƒ½\n- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åæŸç›£è¦–\n- [ ] è‡ªå‹•ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º\n- [ ] è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›\n\n### ğŸ”® Advanced Topics\n\n#### æœ€æ–°ã®ç™ºå±•\n- **é©å¿œçš„ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°**: ãƒ‡ãƒ¼ã‚¿é§†å‹•çš„ãƒ–ãƒ­ãƒƒã‚¯æ±ºå®š\n- **å¹¾ä½•å­¦çš„ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§**: ç†è«–çš„åæŸä¿è¨¼\n- **GPUä¸¦åˆ—åŒ–**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®é«˜é€ŸåŒ–\n\n#### ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢\n- **å¤‰åˆ†ã‚®ãƒ–ã‚¹**: è¿‘ä¼¼æ¨è«–ã¨ã®èåˆ\n- **é‡å­ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°**: é‡å­è¨ˆç®—ã¨ã®çµ„ã¿åˆã‚ã›\n- **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¸ã®é©ç”¨\n\n### ğŸ’¡ æˆåŠŸã®ç§˜è¨£\n\n> **ã€Œé©åˆ‡ãªåˆ†è§£ãŒåŠ¹ç‡ã®9å‰²ã‚’æ±ºã‚ã‚‹ã€** - å•é¡Œã®æ•°å­¦çš„æ§‹é€ ã‚’ç†è§£ã—ã€æœ€ã‚‚è‡ªç„¶ãªå¤‰æ•°åˆ†å‰²ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆåŠŸã®éµã§ã™ã€‚\n\n**è¨˜æ†¶ã™ã¹ãåŸå‰‡:**\n- ğŸ¯ **å…±å½¹æ€§ã®æ´»ç”¨**: è§£æè§£ãŒå¾—ã‚‰ã‚Œã‚‹éƒ¨åˆ†ã¯ç©æ¥µçš„ã«åˆ©ç”¨\n- ğŸ”— **ç›¸é–¢ã®ç†è§£**: å¤‰æ•°é–“ã®ä¾å­˜æ§‹é€ ã‚’æŠŠæ¡ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯è¨­è¨ˆ\n- âš–ï¸ **ãƒãƒ©ãƒ³ã‚¹é‡è¦–**: åŠ¹ç‡æ€§ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•\n- ğŸ”„ **åå¾©æ”¹å–„**: åˆæœŸå®Ÿè£…ã‹ã‚‰æ®µéšçš„ãªæœ€é©åŒ–\n\n### ğŸŒ å®Ÿä¸–ç•Œã§ã®å¿œç”¨ä¾‹\n\n**ğŸ”¬ ç§‘å­¦ç ”ç©¶:**\n- éºä¼å­ç™ºç¾ãƒ‡ãƒ¼ã‚¿ã®éšå±¤ãƒ¢ãƒ‡ãƒ«\n- æ°—å€™å¤‰å‹•ã®ç©ºé–“æ™‚é–“ãƒ¢ãƒ‡ãƒ«\n- ç–«å­¦ãƒ‡ãƒ¼ã‚¿ã®å¤šãƒ¬ãƒ™ãƒ«åˆ†æ\n\n**ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹:**\n- é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³\n- éœ€è¦äºˆæ¸¬ã®éšå±¤ãƒ¢ãƒ‡ãƒ«\n- A/Bãƒ†ã‚¹ãƒˆã®å¤šå¤‰é‡è§£æ\n\n**ğŸ¤– æ©Ÿæ¢°å­¦ç¿’:**\n- ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼ˆLDAï¼‰\n- ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ\n- æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n\nã‚ãªãŸã¯ä»Šã€**ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®çœŸã®åŠ›**ã‚’ç†è§£ã—ã€å®Ÿéš›ã®å•é¡Œã«å¿œç”¨ã§ãã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚æ¬¡ã®ç« ã§å­¦ã¶åæŸè¨ºæ–­ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ä¿¡é ¼æ€§ã®é«˜ã„ãƒ™ã‚¤ã‚ºæ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã§ã—ã‚‡ã†ï¼\n\n### ğŸ“ Next Challenge\n\nè¤‡é›‘ãªå®Ÿå•é¡Œï¼ˆä¾‹ï¼šå¤šå¤‰é‡æ™‚ç³»åˆ—ã€ç©ºé–“çµ±è¨ˆã€ç”Ÿå­˜è§£æï¼‰ã«ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ã€**åŠ¹ç‡æ€§ã¨ç²¾åº¦ã®ä¸¡ç«‹**ã‚’å®Ÿç¾ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ã“ã‚ŒãŒã€çœŸã®å°‚é–€æ€§ã®è¨¼æ˜ã¨ãªã‚Šã¾ã™ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}