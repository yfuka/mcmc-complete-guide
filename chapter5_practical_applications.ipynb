{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 5: å®Ÿè·µçš„å¿œç”¨ä¾‹ - å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n\n## å­¦ç¿’ç›®æ¨™\n- **å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ™ã‚¤ã‚ºæ¨è«–**ã‚’MCMCã§å®Œå…¨å®Ÿè£…ã™ã‚‹\n- **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’æ§‹ç¯‰ã§ãã‚‹\n- **å®Ÿãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‹ã‚‰è§£é‡ˆã¾ã§**ã®å…¨å·¥ç¨‹ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹\n- **è¨ºæ–­ãƒ»æ¤œè¨¼ãƒ»äºˆæ¸¬**ã®çµ±åˆçš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç¿’å¾—ã™ã‚‹\n- **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤**ã«ç¹‹ãŒã‚‹å®Ÿè·µçš„ãªåˆ†æã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹\n- **å†ç¾å¯èƒ½ãªç ”ç©¶**ã®ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å­¦ã¶\n\n## ğŸ¯ æœ¬ç« ã®ç‰¹å¾´ï¼šã€Œæ•™ç§‘æ›¸ã‹ã‚‰å®Ÿå‹™ã¸ã€\n\n### å¾“æ¥ã®æ•™æã¨ã®æ±ºå®šçš„ãªé•ã„\n\n**ğŸ“š å¾“æ¥ã®æ•™æ:**\n- äººå·¥çš„ãªã‚·ãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿\n- ç†è«–çš„èª¬æ˜ãŒä¸­å¿ƒ  \n- å®Ÿè£…ã®æ–­ç‰‡çš„ãªä¾‹\n- çµæœã®è¡¨é¢çš„ãªè§£é‡ˆ\n\n**ğŸš€ æœ¬ç« ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**\n- **å®Ÿãƒ‡ãƒ¼ã‚¿**ã‚’ä½¿ã£ãŸæœ¬æ ¼çš„åˆ†æ\n- **å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**ã®å®Ÿè£…\n- **æ¥­å‹™ã§ä½¿ãˆã‚‹å“è³ª**ã®ã‚³ãƒ¼ãƒ‰\n- **æ„æ€æ±ºå®šã«æ´»ç”¨**ã§ãã‚‹çµæœ\n\n### å®Ÿå‹™ã§æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«\n\n1. **ãƒ‡ãƒ¼ã‚¿ç†è§£**: æ¬ æå€¤ã€å¤–ã‚Œå€¤ã€åˆ†å¸ƒã®ç‰¹æ€§æŠŠæ¡\n2. **å‰å‡¦ç†è¨­è¨ˆ**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€å¤‰æ•°å¤‰æ›ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°  \n3. **ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**: é©åˆ‡ãªäº‹å‰åˆ†å¸ƒã¨å°¤åº¦ã®é¸æŠ\n4. **è¨ºæ–­ã¨æ¤œè¨¼**: åæŸã€é©åˆåº¦ã€äºˆæ¸¬æ€§èƒ½ã®ç·åˆè©•ä¾¡\n5. **çµæœè§£é‡ˆ**: ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®èª¬æ˜å¯èƒ½ãªæ´å¯Ÿ\n6. **å®Ÿè£…å“è³ª**: å†ç¾å¯èƒ½ã§ä¿å®ˆã—ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰\n\n### ğŸ”„ å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ§‹æˆ\n\n```\nå®Ÿãƒ‡ãƒ¼ã‚¿ â†’ å‰å‡¦ç† â†’ ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ â†’ MCMCå®Ÿè¡Œ â†’ è¨ºæ–­ â†’ æ¤œè¨¼ â†’ äºˆæ¸¬ â†’ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤\n   â†“         â†“         â†“         â†“       â†“     â†“     â†“        â†“\n EDA    ç‰¹å¾´é‡     äº‹å‰åˆ†å¸ƒ    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°  åæŸ  é©åˆåº¦  ä¸ç¢ºå®Ÿæ€§  æ„æ€æ±ºå®š\n      ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢   è¨­è¨ˆ      æœ€é©åŒ–      è¨ºæ–­   è©•ä¾¡   å®šé‡åŒ–   æ”¯æ´\n```\n\nã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ã‚ãªãŸã¯**å³æˆ¦åŠ›ã¨ãªã‚‹ãƒ™ã‚¤ã‚ºæ¨è«–ã®å®Ÿè·µè€…**ã«ãªã‚Šã¾ã™ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.special import expit, logsumexp\nfrom scipy.optimize import minimize\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.datasets import make_classification, make_regression, load_wine, load_boston\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score, classification_report\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã¨ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š\nplt.rcParams['font.family'] = 'DejaVu Sans'\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['savefig.dpi'] = 150\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"husl\")\n\n# å†ç¾æ€§ã®ç¢ºä¿\nnp.random.seed(42)\n\n# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š\nPROJECT_CONFIG = {\n    'random_seed': 42,\n    'test_size': 0.2,\n    'n_mcmc_samples': 8000,\n    'burnin_ratio': 0.25,\n    'target_acceptance_rate': 0.44,\n    'convergence_threshold': 1.01,\n    'min_eff_samples': 400\n}\n\nprint(\"ğŸš€ å®Ÿè·µçš„ãƒ™ã‚¤ã‚ºåˆ†æç’°å¢ƒã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\nprint(f\"ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š: {PROJECT_CONFIG}\")\nprint(f\"ğŸ² ä¹±æ•°ã‚·ãƒ¼ãƒ‰: {PROJECT_CONFIG['random_seed']}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°\n",
    "\n",
    "ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’MCMCã§å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "$$y_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "### äº‹å‰åˆ†å¸ƒ\n",
    "- $\\boldsymbol{\\beta} \\sim N(\\mathbf{0}, \\tau^2 \\mathbf{I})$\n",
    "- $\\sigma^2 \\sim \\text{InvGamma}(a, b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "def generate_regression_data(n_samples=100, n_features=3, noise_std=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    å›å¸°ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # çœŸã®å›å¸°ä¿‚æ•°\n",
    "    true_beta = np.array([2.0, -1.5, 0.8, 1.2])  # intercept + 3 features\n",
    "    \n",
    "    # èª¬æ˜å¤‰æ•°\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    X = np.column_stack([np.ones(n_samples), X])  # intercepté …ã‚’è¿½åŠ \n",
    "    \n",
    "    # ç›®çš„å¤‰æ•°\n",
    "    y = X @ true_beta + np.random.normal(0, noise_std, n_samples)\n",
    "    \n",
    "    return X, y, true_beta, noise_std**2\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X, y, true_beta, true_sigma2 = generate_regression_data()\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {n_samples} x {n_features}\")\n",
    "print(f\"çœŸã®å›å¸°ä¿‚æ•°: {true_beta}\")\n",
    "print(f\"çœŸã®èª¤å·®åˆ†æ•£: {true_sigma2:.3f}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ\n",
    "axes[0].hist(y, bins=20, alpha=0.7, density=True)\n",
    "axes[0].set_title('Distribution of y')\n",
    "axes[0].set_xlabel('y')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚ï¼ˆæœ€åˆã®èª¬æ˜å¤‰æ•°ã®ã¿ï¼‰\n",
    "axes[1].scatter(X[:, 1], y, alpha=0.6)\n",
    "axes[1].set_title('y vs X1')\n",
    "axes[1].set_xlabel('X1')\n",
    "axes[1].set_ylabel('y')\n",
    "\n",
    "# ç›¸é–¢è¡Œåˆ—\n",
    "corr_matrix = np.corrcoef(X[:, 1:].T, y)\n",
    "im = axes[2].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[2].set_title('Correlation Matrix')\n",
    "axes[2].set_xticks(range(n_features))\n",
    "axes[2].set_yticks(range(n_features))\n",
    "axes[2].set_xticklabels(['X1', 'X2', 'X3', 'y'])\n",
    "axes[2].set_yticklabels(['X1', 'X2', 'X3', 'y'])\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_linear_regression_gibbs(X, y, n_iterations=5000, \n",
    "                                   prior_beta_var=10.0,\n",
    "                                   prior_sigma_shape=1.0, prior_sigma_rate=1.0):\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    - X: èª¬æ˜å¤‰æ•°è¡Œåˆ— (n x p)\n",
    "    - y: ç›®çš„å¤‰æ•° (n,)\n",
    "    - n_iterations: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    - prior_beta_var: å›å¸°ä¿‚æ•°ã®äº‹å‰åˆ†æ•£\n",
    "    - prior_sigma_shape, prior_sigma_rate: èª¤å·®åˆ†æ•£ã®äº‹å‰åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \n",
    "    Returns:\n",
    "    - beta_samples: å›å¸°ä¿‚æ•°ã®ã‚µãƒ³ãƒ—ãƒ« (n_iterations x p)\n",
    "    - sigma2_samples: èª¤å·®åˆ†æ•£ã®ã‚µãƒ³ãƒ—ãƒ« (n_iterations,)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    \n",
    "    # äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    prior_beta_precision = np.eye(p) / prior_beta_var\n",
    "    \n",
    "    # åˆæœŸå€¤\n",
    "    beta = np.zeros(p)\n",
    "    sigma2 = 1.0\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨\n",
    "    beta_samples = np.zeros((n_iterations, p))\n",
    "    sigma2_samples = np.zeros(n_iterations)\n",
    "    \n",
    "    # äº‹å‰è¨ˆç®—ï¼ˆåŠ¹ç‡åŒ–ã®ãŸã‚ï¼‰\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Step 1: beta | sigma2, y ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        # äº‹å¾Œåˆ†å¸ƒã¯å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ\n",
    "        posterior_precision = XtX / sigma2 + prior_beta_precision\n",
    "        posterior_cov = np.linalg.inv(posterior_precision)\n",
    "        posterior_mean = posterior_cov @ (Xty / sigma2)\n",
    "        \n",
    "        beta = np.random.multivariate_normal(posterior_mean, posterior_cov)\n",
    "        \n",
    "        # Step 2: sigma2 | beta, y ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        # äº‹å¾Œåˆ†å¸ƒã¯é€†ã‚¬ãƒ³ãƒåˆ†å¸ƒ\n",
    "        residuals = y - X @ beta\n",
    "        posterior_shape = prior_sigma_shape + n / 2\n",
    "        posterior_rate = prior_sigma_rate + np.sum(residuals**2) / 2\n",
    "        \n",
    "        # é€†ã‚¬ãƒ³ãƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚¬ãƒ³ãƒã®é€†æ•°ï¼‰\n",
    "        sigma2 = 1 / np.random.gamma(posterior_shape, 1 / posterior_rate)\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜\n",
    "        beta_samples[i] = beta\n",
    "        sigma2_samples[i] = sigma2\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Iteration {i+1}/{n_iterations}\")\n",
    "    \n",
    "    return beta_samples, sigma2_samples\n",
    "\n",
    "# ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å®Ÿè¡Œ\n",
    "print(\"ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "beta_samples, sigma2_samples = bayesian_linear_regression_gibbs(X, y)\n",
    "\n",
    "# çµæœã®çµ±è¨ˆ\n",
    "burnin = 1000\n",
    "beta_post_mean = np.mean(beta_samples[burnin:], axis=0)\n",
    "beta_post_std = np.std(beta_samples[burnin:], axis=0)\n",
    "sigma2_post_mean = np.mean(sigma2_samples[burnin:])\n",
    "sigma2_post_std = np.std(sigma2_samples[burnin:])\n",
    "\n",
    "print(f\"\\n=== ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ ===\")\n",
    "print(f\"{'Parameter':<10} {'True':<8} {'Post Mean':<12} {'Post Std':<10} {'95% CI':<20}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(len(true_beta)):\n",
    "    param_samples = beta_samples[burnin:, i]\n",
    "    ci_lower = np.percentile(param_samples, 2.5)\n",
    "    ci_upper = np.percentile(param_samples, 97.5)\n",
    "    \n",
    "    param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "    print(f\"{param_name:<10} {true_beta[i]:<8.3f} {beta_post_mean[i]:<12.3f} \"\n",
    "          f\"{beta_post_std[i]:<10.3f} [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "sigma2_ci_lower = np.percentile(sigma2_samples[burnin:], 2.5)\n",
    "sigma2_ci_upper = np.percentile(sigma2_samples[burnin:], 97.5)\n",
    "print(f\"{'Sigma2':<10} {true_sigma2:<8.3f} {sigma2_post_mean:<12.3f} \"\n",
    "      f\"{sigma2_post_std:<10.3f} [{sigma2_ci_lower:.3f}, {sigma2_ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°çµæœã®å¯è¦–åŒ–\n",
    "def plot_bayesian_regression_results(beta_samples, sigma2_samples, true_beta, true_sigma2, \n",
    "                                    X, y, burnin=1000):\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°çµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    n_params = beta_samples.shape[1]\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    for i in range(min(4, n_params)):\n",
    "        axes[0, i].plot(beta_samples[:, i], alpha=0.8, linewidth=0.8)\n",
    "        axes[0, i].axhline(true_beta[i], color='red', linestyle='--', linewidth=2)\n",
    "        axes[0, i].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "        param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "        axes[0, i].set_title(f'{param_name} Trace')\n",
    "        axes[0, i].set_xlabel('Iteration')\n",
    "        axes[0, i].set_ylabel('Value')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒ\n",
    "    for i in range(min(4, n_params)):\n",
    "        param_samples = beta_samples[burnin:, i]\n",
    "        axes[1, i].hist(param_samples, bins=50, density=True, alpha=0.7, color='skyblue')\n",
    "        axes[1, i].axvline(true_beta[i], color='red', linestyle='--', linewidth=2, label='True')\n",
    "        axes[1, i].axvline(np.mean(param_samples), color='blue', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "        \n",
    "        param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "        axes[1, i].set_title(f'{param_name} Posterior')\n",
    "        axes[1, i].set_xlabel('Value')\n",
    "        axes[1, i].set_ylabel('Density')\n",
    "        axes[1, i].legend()\n",
    "    \n",
    "    # èª¤å·®åˆ†æ•£ã®çµæœ\n",
    "    # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    axes[2, 0].plot(sigma2_samples, alpha=0.8, linewidth=0.8, color='green')\n",
    "    axes[2, 0].axhline(true_sigma2, color='red', linestyle='--', linewidth=2)\n",
    "    axes[2, 0].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[2, 0].set_title('Sigma2 Trace')\n",
    "    axes[2, 0].set_xlabel('Iteration')\n",
    "    axes[2, 0].set_ylabel('Sigma2')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # äº‹å¾Œåˆ†å¸ƒ\n",
    "    sigma2_post = sigma2_samples[burnin:]\n",
    "    axes[2, 1].hist(sigma2_post, bins=50, density=True, alpha=0.7, color='lightgreen')\n",
    "    axes[2, 1].axvline(true_sigma2, color='red', linestyle='--', linewidth=2, label='True')\n",
    "    axes[2, 1].axvline(np.mean(sigma2_post), color='green', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "    axes[2, 1].set_title('Sigma2 Posterior')\n",
    "    axes[2, 1].set_xlabel('Sigma2')\n",
    "    axes[2, 1].set_ylabel('Density')\n",
    "    axes[2, 1].legend()\n",
    "    \n",
    "    # äºˆæ¸¬åŒºé–“ã®å¯è¦–åŒ–ï¼ˆæœ€åˆã®èª¬æ˜å¤‰æ•°ã«ã¤ã„ã¦ï¼‰\n",
    "    if X.shape[1] > 1:  # interceptä»¥å¤–ã®å¤‰æ•°ãŒã‚ã‚‹å ´åˆ\n",
    "        x_pred_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "        \n",
    "        # ä»–ã®å¤‰æ•°ã¯å¹³å‡å€¤ã«å›ºå®š\n",
    "        X_pred = np.zeros((len(x_pred_range), X.shape[1]))\n",
    "        X_pred[:, 0] = 1  # intercept\n",
    "        X_pred[:, 1] = x_pred_range  # å¯¾è±¡å¤‰æ•°\n",
    "        for j in range(2, X.shape[1]):\n",
    "            X_pred[:, j] = np.mean(X[:, j])  # ä»–ã®å¤‰æ•°ã¯å¹³å‡å€¤\n",
    "        \n",
    "        # äº‹å¾Œã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰äºˆæ¸¬åˆ†å¸ƒã‚’è¨ˆç®—\n",
    "        n_pred_samples = min(500, len(beta_samples[burnin:]))\n",
    "        pred_samples = np.zeros((n_pred_samples, len(x_pred_range)))\n",
    "        \n",
    "        for i in range(n_pred_samples):\n",
    "            idx = burnin + i\n",
    "            beta_i = beta_samples[idx]\n",
    "            sigma2_i = sigma2_samples[idx]\n",
    "            \n",
    "            # å¹³å‡ã®äºˆæ¸¬\n",
    "            mu_pred = X_pred @ beta_i\n",
    "            # äºˆæ¸¬åˆ†å¸ƒï¼ˆè¦³æ¸¬ãƒã‚¤ã‚ºã‚’å«ã‚€ï¼‰\n",
    "            pred_samples[i] = np.random.normal(mu_pred, np.sqrt(sigma2_i))\n",
    "        \n",
    "        # äºˆæ¸¬åŒºé–“ã®è¨ˆç®—\n",
    "        pred_mean = np.mean(pred_samples, axis=0)\n",
    "        pred_lower = np.percentile(pred_samples, 2.5, axis=0)\n",
    "        pred_upper = np.percentile(pred_samples, 97.5, axis=0)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        axes[2, 2].scatter(X[:, 1], y, alpha=0.6, s=20, label='Data')\n",
    "        axes[2, 2].plot(x_pred_range, pred_mean, 'b-', linewidth=2, label='Posterior mean')\n",
    "        axes[2, 2].fill_between(x_pred_range, pred_lower, pred_upper, alpha=0.3, color='blue', label='95% Prediction interval')\n",
    "        \n",
    "        # çœŸã®é–¢ä¿‚\n",
    "        true_pred = X_pred @ true_beta\n",
    "        axes[2, 2].plot(x_pred_range, true_pred, 'r--', linewidth=2, label='True relationship')\n",
    "        \n",
    "        axes[2, 2].set_title('Prediction with Uncertainty')\n",
    "        axes[2, 2].set_xlabel('X1')\n",
    "        axes[2, 2].set_ylabel('y')\n",
    "        axes[2, 2].legend()\n",
    "        axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ®‹å·®åˆ†æ\n",
    "    # äº‹å¾Œå¹³å‡ã‚’ä½¿ã£ãŸäºˆæ¸¬\n",
    "    beta_mean = np.mean(beta_samples[burnin:], axis=0)\n",
    "    y_pred = X @ beta_mean\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    axes[2, 3].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[2, 3].axhline(0, color='red', linestyle='--')\n",
    "    axes[2, 3].set_title('Residuals vs Fitted')\n",
    "    axes[2, 3].set_xlabel('Fitted values')\n",
    "    axes[2, 3].set_ylabel('Residuals')\n",
    "    axes[2, 3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# çµæœã®å¯è¦–åŒ–\n",
    "plot_bayesian_regression_results(beta_samples, sigma2_samples, true_beta, true_sigma2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°\n",
    "\n",
    "äºŒå€¤åˆ†é¡å•é¡Œã«å¯¾ã™ã‚‹ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "$$P(y_i = 1 | \\mathbf{x}_i) = \\text{logit}^{-1}(\\mathbf{x}_i^T \\boldsymbol{\\beta})$$\n",
    "\n",
    "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§ã¯è§£æçš„ãªå…±å½¹äº‹å‰åˆ†å¸ƒãŒãªã„ãŸã‚ã€ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "def generate_logistic_data(n_samples=200, n_features=2, seed=42):\n",
    "    \"\"\"\n",
    "    ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # çœŸã®å›å¸°ä¿‚æ•°\n",
    "    true_beta = np.array([0.5, 2.0, -1.5])  # intercept + 2 features\n",
    "    \n",
    "    # èª¬æ˜å¤‰æ•°\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    X = np.column_stack([np.ones(n_samples), X])  # intercepté …ã‚’è¿½åŠ \n",
    "    \n",
    "    # ãƒ­ã‚¸ãƒƒãƒˆå¤‰æ›\n",
    "    logits = X @ true_beta\n",
    "    probabilities = expit(logits)  # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯é–¢æ•°\n",
    "    \n",
    "    # äºŒå€¤ç›®çš„å¤‰æ•°\n",
    "    y = np.random.binomial(1, probabilities)\n",
    "    \n",
    "    return X, y, true_beta, probabilities\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X_log, y_log, true_beta_log, true_probs = generate_logistic_data()\n",
    "n_samples_log, n_features_log = X_log.shape\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {n_samples_log} x {n_features_log}\")\n",
    "print(f\"çœŸã®å›å¸°ä¿‚æ•°: {true_beta_log}\")\n",
    "print(f\"æ­£ä¾‹ã®å‰²åˆ: {np.mean(y_log):.3f}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹åˆ¥ã®æ•£å¸ƒå›³\n",
    "class_0 = y_log == 0\n",
    "class_1 = y_log == 1\n",
    "\n",
    "axes[0].scatter(X_log[class_0, 1], X_log[class_0, 2], alpha=0.7, c='red', label='Class 0')\n",
    "axes[0].scatter(X_log[class_1, 1], X_log[class_1, 2], alpha=0.7, c='blue', label='Class 1')\n",
    "axes[0].set_title('Data Distribution by Class')\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# çœŸã®ç¢ºç‡ vs èª¬æ˜å¤‰æ•°\n",
    "axes[1].scatter(X_log[:, 1], true_probs, c=y_log, cmap='RdYlBu', alpha=0.7)\n",
    "axes[1].set_title('True Probabilities vs X1')\n",
    "axes[1].set_xlabel('X1')\n",
    "axes[1].set_ylabel('P(y=1)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ\n",
    "axes[2].bar([0, 1], [np.sum(y_log == 0), np.sum(y_log == 1)], alpha=0.7, color=['red', 'blue'])\n",
    "axes[2].set_title('Class Distribution')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xticks([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_logistic_regression_mh(X, y, n_iterations=10000, \n",
    "                                   prior_beta_var=10.0, step_size=0.1):\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•\n",
    "    \n",
    "    Parameters:\n",
    "    - X: èª¬æ˜å¤‰æ•°è¡Œåˆ— (n x p)\n",
    "    - y: äºŒå€¤ç›®çš„å¤‰æ•° (n,)\n",
    "    - n_iterations: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    - prior_beta_var: å›å¸°ä¿‚æ•°ã®äº‹å‰åˆ†æ•£\n",
    "    - step_size: ææ¡ˆåˆ†å¸ƒã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\n",
    "    \n",
    "    Returns:\n",
    "    - beta_samples: å›å¸°ä¿‚æ•°ã®ã‚µãƒ³ãƒ—ãƒ« (n_iterations x p)\n",
    "    - acceptance_rate: å—ç†ç‡\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    \n",
    "    def log_likelihood(beta):\n",
    "        \"\"\"å¯¾æ•°å°¤åº¦é–¢æ•°\"\"\"\n",
    "        logits = X @ beta\n",
    "        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®è¨ˆç®—\n",
    "        log_lik = np.sum(y * logits - np.log(1 + np.exp(np.clip(logits, -500, 500))))\n",
    "        return log_lik\n",
    "    \n",
    "    def log_prior(beta):\n",
    "        \"\"\"å¯¾æ•°äº‹å‰åˆ†å¸ƒ\"\"\"\n",
    "        return -0.5 * np.sum(beta**2) / prior_beta_var\n",
    "    \n",
    "    def log_posterior(beta):\n",
    "        \"\"\"å¯¾æ•°äº‹å¾Œåˆ†å¸ƒ\"\"\"\n",
    "        return log_likelihood(beta) + log_prior(beta)\n",
    "    \n",
    "    # åˆæœŸå€¤ï¼ˆæœ€å°¤æ¨å®šã‹ã‚‰é–‹å§‹ï¼‰\n",
    "    try:\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        def neg_log_lik(beta):\n",
    "            return -log_likelihood(beta)\n",
    "        \n",
    "        result = minimize(neg_log_lik, np.zeros(p), method='BFGS')\n",
    "        beta_current = result.x\n",
    "    except:\n",
    "        beta_current = np.zeros(p)\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨\n",
    "    beta_samples = np.zeros((n_iterations, p))\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # ææ¡ˆåˆ†å¸ƒã®å…±åˆ†æ•£è¡Œåˆ—ï¼ˆé©å¿œçš„ã«èª¿æ•´ï¼‰\n",
    "    proposal_cov = step_size * np.eye(p)\n",
    "    \n",
    "    current_log_posterior = log_posterior(beta_current)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # ææ¡ˆ\n",
    "        beta_proposed = np.random.multivariate_normal(beta_current, proposal_cov)\n",
    "        proposed_log_posterior = log_posterior(beta_proposed)\n",
    "        \n",
    "        # å—ç†ç¢ºç‡\n",
    "        log_alpha = proposed_log_posterior - current_log_posterior\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # å—ç†/æ£„å´\n",
    "        if np.random.rand() < alpha:\n",
    "            beta_current = beta_proposed\n",
    "            current_log_posterior = proposed_log_posterior\n",
    "            n_accepted += 1\n",
    "        \n",
    "        beta_samples[i] = beta_current\n",
    "        \n",
    "        # é©å¿œçš„ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            recent_acceptance = n_accepted / i\n",
    "            if recent_acceptance < 0.2:  # å—ç†ç‡ãŒä½ã™ãã‚‹\n",
    "                proposal_cov *= 0.9\n",
    "            elif recent_acceptance > 0.6:  # å—ç†ç‡ãŒé«˜ã™ãã‚‹\n",
    "                proposal_cov *= 1.1\n",
    "        \n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"Iteration {i+1}/{n_iterations}, Acceptance rate: {n_accepted/(i+1):.3f}\")\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_iterations\n",
    "    return beta_samples, acceptance_rate\n",
    "\n",
    "# ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®å®Ÿè¡Œ\n",
    "print(\"ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "beta_samples_log, acceptance_rate_log = bayesian_logistic_regression_mh(X_log, y_log)\n",
    "\n",
    "print(f\"\\næœ€çµ‚å—ç†ç‡: {acceptance_rate_log:.3f}\")\n",
    "\n",
    "# çµæœã®çµ±è¨ˆ\n",
    "burnin_log = 2000\n",
    "beta_post_mean_log = np.mean(beta_samples_log[burnin_log:], axis=0)\n",
    "beta_post_std_log = np.std(beta_samples_log[burnin_log:], axis=0)\n",
    "\n",
    "print(f\"\\n=== ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœ ===\")\n",
    "print(f\"{'Parameter':<10} {'True':<8} {'Post Mean':<12} {'Post Std':<10} {'95% CI':<20}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(len(true_beta_log)):\n",
    "    param_samples = beta_samples_log[burnin_log:, i]\n",
    "    ci_lower = np.percentile(param_samples, 2.5)\n",
    "    ci_upper = np.percentile(param_samples, 97.5)\n",
    "    \n",
    "    param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "    print(f\"{param_name:<10} {true_beta_log[i]:<8.3f} {beta_post_mean_log[i]:<12.3f} \"\n",
    "          f\"{beta_post_std_log[i]:<10.3f} [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœã®å¯è¦–åŒ–\n",
    "def plot_logistic_regression_results(beta_samples, X, y, true_beta, burnin=2000):\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœã®å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    n_params = beta_samples.shape[1]\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    for i in range(min(3, n_params)):\n",
    "        axes[0, i].plot(beta_samples[:, i], alpha=0.8, linewidth=0.8)\n",
    "        axes[0, i].axhline(true_beta[i], color='red', linestyle='--', linewidth=2)\n",
    "        axes[0, i].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "        param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "        axes[0, i].set_title(f'{param_name} Trace')\n",
    "        axes[0, i].set_xlabel('Iteration')\n",
    "        axes[0, i].set_ylabel('Value')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒ\n",
    "    for i in range(min(3, n_params)):\n",
    "        param_samples = beta_samples[burnin:, i]\n",
    "        axes[1, i].hist(param_samples, bins=50, density=True, alpha=0.7, color='lightcoral')\n",
    "        axes[1, i].axvline(true_beta[i], color='red', linestyle='--', linewidth=2, label='True')\n",
    "        axes[1, i].axvline(np.mean(param_samples), color='darkred', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "        \n",
    "        param_name = 'Intercept' if i == 0 else f'Beta_{i}'\n",
    "        axes[1, i].set_title(f'{param_name} Posterior')\n",
    "        axes[1, i].set_xlabel('Value')\n",
    "        axes[1, i].set_ylabel('Density')\n",
    "        axes[1, i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒã®å ´åˆï¼‰\n",
    "    if X.shape[1] == 3:  # intercept + 2 features\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # äº‹å¾Œå¹³å‡ã«ã‚ˆã‚‹æ±ºå®šå¢ƒç•Œ\n",
    "        beta_mean = np.mean(beta_samples[burnin:], axis=0)\n",
    "        \n",
    "        # ã‚°ãƒªãƒƒãƒ‰ã®ä½œæˆ\n",
    "        x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        x2_min, x2_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                               np.linspace(x2_min, x2_max, 100))\n",
    "        \n",
    "        grid_points = np.c_[np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()]\n",
    "        \n",
    "        # äº‹å¾Œå¹³å‡ã«ã‚ˆã‚‹äºˆæ¸¬ç¢ºç‡\n",
    "        prob_mean = expit(grid_points @ beta_mean).reshape(xx1.shape)\n",
    "        \n",
    "        # ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        class_0 = y == 0\n",
    "        class_1 = y == 1\n",
    "        \n",
    "        axes[0].contourf(xx1, xx2, prob_mean, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "        axes[0].contour(xx1, xx2, prob_mean, levels=[0.5], colors='black', linewidths=2)\n",
    "        axes[0].scatter(X[class_0, 1], X[class_0, 2], c='red', alpha=0.7, label='Class 0')\n",
    "        axes[0].scatter(X[class_1, 1], X[class_1, 2], c='blue', alpha=0.7, label='Class 1')\n",
    "        axes[0].set_title('Decision Boundary (Posterior Mean)')\n",
    "        axes[0].set_xlabel('X1')\n",
    "        axes[0].set_ylabel('X2')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # ä¸ç¢ºå®Ÿæ€§ã®å¯è¦–åŒ–\n",
    "        # è¤‡æ•°ã®äº‹å¾Œã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰äºˆæ¸¬ç¢ºç‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’è¨ˆç®—\n",
    "        n_uncertainty_samples = min(100, len(beta_samples[burnin:]))\n",
    "        prob_samples = np.zeros((n_uncertainty_samples, xx1.size))\n",
    "        \n",
    "        for i in range(n_uncertainty_samples):\n",
    "            beta_i = beta_samples[burnin + i]\n",
    "            prob_samples[i] = expit(grid_points @ beta_i)\n",
    "        \n",
    "        prob_std = np.std(prob_samples, axis=0).reshape(xx1.shape)\n",
    "        \n",
    "        im = axes[1].contourf(xx1, xx2, prob_std, levels=20, cmap='Reds')\n",
    "        axes[1].scatter(X[class_0, 1], X[class_0, 2], c='white', edgecolors='black', alpha=0.7, label='Class 0')\n",
    "        axes[1].scatter(X[class_1, 1], X[class_1, 2], c='black', alpha=0.7, label='Class 1')\n",
    "        axes[1].set_title('Prediction Uncertainty (Std Dev)')\n",
    "        axes[1].set_xlabel('X1')\n",
    "        axes[1].set_ylabel('X2')\n",
    "        axes[1].legend()\n",
    "        plt.colorbar(im, ax=axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# çµæœã®å¯è¦–åŒ–\n",
    "plot_logistic_regression_results(beta_samples_log, X_log, y_log, true_beta_log)\n",
    "\n",
    "# äºˆæ¸¬æ€§èƒ½ã®è©•ä¾¡\n",
    "burnin_log = 2000\n",
    "beta_mean = np.mean(beta_samples_log[burnin_log:], axis=0)\n",
    "prob_pred = expit(X_log @ beta_mean)\n",
    "y_pred = (prob_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_log)\n",
    "print(f\"\\näºˆæ¸¬ç²¾åº¦: {accuracy:.3f}\")\n",
    "\n",
    "# æ··åŒè¡Œåˆ—\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_log, y_pred)\n",
    "print(f\"\\næ··åŒè¡Œåˆ—:\")\n",
    "print(cm)\n",
    "print(f\"\\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
    "print(classification_report(y_log, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆHierarchical Modelï¼‰\n",
    "\n",
    "ç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§å…±é€šã®æ§‹é€ ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒãˆã¾ã™ã€‚å­¦æ ¡åˆ¥ã®å­¦ç”Ÿæˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ä¾‹ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "- ãƒ¬ãƒ™ãƒ«1ï¼ˆå€‹äººãƒ¬ãƒ™ãƒ«ï¼‰: $y_{ij} \\sim N(\\mu_j, \\sigma^2)$\n",
    "- ãƒ¬ãƒ™ãƒ«2ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ï¼‰: $\\mu_j \\sim N(\\alpha, \\tau^2)$\n",
    "\nã“ã“ã§ã€$i$ã¯å€‹äººã€$j$ã¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¡¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšå±¤ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "def generate_hierarchical_data(n_groups=8, n_per_group=15, seed=42):\n",
    "    \"\"\"\n",
    "    éšå±¤æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå­¦æ ¡åˆ¥å­¦ç”Ÿæˆç¸¾ã®ä¾‹ï¼‰\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # è¶…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    alpha_true = 75.0  # å…¨ä½“å¹³å‡\n",
    "    tau_true = 8.0     # ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã°ã‚‰ã¤ã\n",
    "    sigma_true = 5.0   # å€‹äººå†…ã®ã°ã‚‰ã¤ã\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡\n",
    "    group_means = np.random.normal(alpha_true, tau_true, n_groups)\n",
    "    \n",
    "    # å€‹äººãƒ‡ãƒ¼ã‚¿\n",
    "    data = []\n",
    "    group_ids = []\n",
    "    \n",
    "    for j in range(n_groups):\n",
    "        group_data = np.random.normal(group_means[j], sigma_true, n_per_group)\n",
    "        data.extend(group_data)\n",
    "        group_ids.extend([j] * n_per_group)\n",
    "    \n",
    "    return np.array(data), np.array(group_ids), group_means, alpha_true, tau_true, sigma_true\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "y_hier, group_ids, true_group_means, true_alpha, true_tau, true_sigma = generate_hierarchical_data()\n",
    "n_groups = len(np.unique(group_ids))\n",
    "n_total = len(y_hier)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {n_total} observations, {n_groups} groups\")\n",
    "print(f\"çœŸã®è¶…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Î± = {true_alpha:.1f}, Ï„ = {true_tau:.1f}, Ïƒ = {true_sigma:.1f}\")\n",
    "print(f\"çœŸã®ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡: {true_group_means.round(1)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "group_data = [y_hier[group_ids == j] for j in range(n_groups)]\n",
    "axes[0].boxplot(group_data, labels=[f'Group {j+1}' for j in range(n_groups)])\n",
    "axes[0].scatter(range(1, n_groups+1), true_group_means, color='red', s=50, label='True means')\n",
    "axes[0].axhline(true_alpha, color='red', linestyle='--', alpha=0.7, label='Overall mean')\n",
    "axes[0].set_title('Data by Group')\n",
    "axes[0].set_xlabel('Group')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# å…¨ä½“åˆ†å¸ƒ\n",
    "axes[1].hist(y_hier, bins=30, alpha=0.7, density=True, color='skyblue')\n",
    "axes[1].axvline(true_alpha, color='red', linestyle='--', linewidth=2, label='True overall mean')\n",
    "axes[1].axvline(np.mean(y_hier), color='blue', linestyle='-', linewidth=2, label='Sample mean')\n",
    "axes[1].set_title('Overall Distribution')\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "\n",
    "# ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡ã®åˆ†å¸ƒ\n",
    "sample_group_means = [np.mean(y_hier[group_ids == j]) for j in range(n_groups)]\n",
    "axes[2].scatter(range(1, n_groups+1), sample_group_means, color='blue', s=50, label='Sample means')\n",
    "axes[2].scatter(range(1, n_groups+1), true_group_means, color='red', s=50, label='True means')\n",
    "axes[2].axhline(true_alpha, color='red', linestyle='--', alpha=0.7, label='Overall mean')\n",
    "axes[2].set_title('Group Means Comparison')\n",
    "axes[2].set_xlabel('Group')\n",
    "axes[2].set_ylabel('Mean Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ\n",
    "print(f\"\\n=== ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ ===\")\n",
    "print(f\"{'Group':<8} {'N':<5} {'Sample Mean':<12} {'True Mean':<10} {'Sample Std':<12}\")\n",
    "print(\"-\" * 55)\n",
    "for j in range(n_groups):\n",
    "    group_data_j = y_hier[group_ids == j]\n",
    "    print(f\"{j+1:<8} {len(group_data_j):<5} {np.mean(group_data_j):<12.2f} \"\n",
    "          f\"{true_group_means[j]:<10.2f} {np.std(group_data_j, ddof=1):<12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model_gibbs(y, group_ids, n_iterations=8000,\n",
    "                           prior_alpha_mean=70, prior_alpha_var=100,\n",
    "                           prior_tau_shape=1, prior_tau_rate=1,\n",
    "                           prior_sigma_shape=1, prior_sigma_rate=1):\n",
    "    \"\"\"\n",
    "    éšå±¤ãƒ¢ãƒ‡ãƒ«ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    - y: è¦³æ¸¬å€¤\n",
    "    - group_ids: ã‚°ãƒ«ãƒ¼ãƒ—ID\n",
    "    - n_iterations: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    - prior_*: äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \n",
    "    Returns:\n",
    "    - alpha_samples: å…¨ä½“å¹³å‡ã®ã‚µãƒ³ãƒ—ãƒ«\n",
    "    - tau2_samples: ã‚°ãƒ«ãƒ¼ãƒ—é–“åˆ†æ•£ã®ã‚µãƒ³ãƒ—ãƒ«\n",
    "    - sigma2_samples: å€‹äººå†…åˆ†æ•£ã®ã‚µãƒ³ãƒ—ãƒ«\n",
    "    - mu_samples: ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡ã®ã‚µãƒ³ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "    n_groups = len(np.unique(group_ids))\n",
    "    n_total = len(y)\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    group_data = [y[group_ids == j] for j in range(n_groups)]\n",
    "    group_sizes = [len(group_data[j]) for j in range(n_groups)]\n",
    "    \n",
    "    # åˆæœŸå€¤\n",
    "    alpha = np.mean(y)\n",
    "    tau2 = 1.0\n",
    "    sigma2 = 1.0\n",
    "    mu = np.array([np.mean(group_data[j]) for j in range(n_groups)])\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨\n",
    "    alpha_samples = np.zeros(n_iterations)\n",
    "    tau2_samples = np.zeros(n_iterations)\n",
    "    sigma2_samples = np.zeros(n_iterations)\n",
    "    mu_samples = np.zeros((n_iterations, n_groups))\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Step 1: Î¼_j | Î±, Ï„Â², ÏƒÂ², y ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        for j in range(n_groups):\n",
    "            n_j = group_sizes[j]\n",
    "            y_j_bar = np.mean(group_data[j])\n",
    "            \n",
    "            # äº‹å¾Œåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            posterior_precision = 1/tau2 + n_j/sigma2\n",
    "            posterior_var = 1 / posterior_precision\n",
    "            posterior_mean = posterior_var * (alpha/tau2 + n_j*y_j_bar/sigma2)\n",
    "            \n",
    "            mu[j] = np.random.normal(posterior_mean, np.sqrt(posterior_var))\n",
    "        \n",
    "        # Step 2: Î± | Î¼, Ï„Â² ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        posterior_precision_alpha = 1/prior_alpha_var + n_groups/tau2\n",
    "        posterior_var_alpha = 1 / posterior_precision_alpha\n",
    "        posterior_mean_alpha = posterior_var_alpha * (prior_alpha_mean/prior_alpha_var + np.sum(mu)/tau2)\n",
    "        \n",
    "        alpha = np.random.normal(posterior_mean_alpha, np.sqrt(posterior_var_alpha))\n",
    "        \n",
    "        # Step 3: Ï„Â² | Î±, Î¼ ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        posterior_shape_tau = prior_tau_shape + n_groups / 2\n",
    "        posterior_rate_tau = prior_tau_rate + np.sum((mu - alpha)**2) / 2\n",
    "        \n",
    "        tau2 = 1 / np.random.gamma(posterior_shape_tau, 1/posterior_rate_tau)\n",
    "        \n",
    "        # Step 4: ÏƒÂ² | Î¼, y ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        total_ss = 0\n",
    "        for j in range(n_groups):\n",
    "            total_ss += np.sum((group_data[j] - mu[j])**2)\n",
    "        \n",
    "        posterior_shape_sigma = prior_sigma_shape + n_total / 2\n",
    "        posterior_rate_sigma = prior_sigma_rate + total_ss / 2\n",
    "        \n",
    "        sigma2 = 1 / np.random.gamma(posterior_shape_sigma, 1/posterior_rate_sigma)\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜\n",
    "        alpha_samples[i] = alpha\n",
    "        tau2_samples[i] = tau2\n",
    "        sigma2_samples[i] = sigma2\n",
    "        mu_samples[i] = mu\n",
    "        \n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"Iteration {i+1}/{n_iterations}\")\n",
    "    \n",
    "    return alpha_samples, tau2_samples, sigma2_samples, mu_samples\n",
    "\n",
    "# éšå±¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ\n",
    "print(\"éšå±¤ãƒ¢ãƒ‡ãƒ«ã®ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "alpha_samples, tau2_samples, sigma2_samples, mu_samples = hierarchical_model_gibbs(y_hier, group_ids)\n",
    "\n",
    "# çµæœã®çµ±è¨ˆ\n",
    "burnin_hier = 2000\n",
    "\n",
    "alpha_post_mean = np.mean(alpha_samples[burnin_hier:])\n",
    "tau_post_mean = np.sqrt(np.mean(tau2_samples[burnin_hier:]))\n",
    "sigma_post_mean = np.sqrt(np.mean(sigma2_samples[burnin_hier:]))\n",
    "mu_post_mean = np.mean(mu_samples[burnin_hier:], axis=0)\n",
    "\n",
    "print(f\"\\n=== éšå±¤ãƒ¢ãƒ‡ãƒ«æ¨å®šçµæœ ===\")\n",
    "print(f\"{'Parameter':<15} {'True':<8} {'Post Mean':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# è¶…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "alpha_ci = np.percentile(alpha_samples[burnin_hier:], [2.5, 97.5])\n",
    "print(f\"{'Alpha (mean)':<15} {true_alpha:<8.2f} {alpha_post_mean:<12.2f} \"\n",
    "      f\"[{alpha_ci[0]:.2f}, {alpha_ci[1]:.2f}]\")\n",
    "\n",
    "tau_ci = np.percentile(np.sqrt(tau2_samples[burnin_hier:]), [2.5, 97.5])\n",
    "print(f\"{'Tau (group SD)':<15} {true_tau:<8.2f} {tau_post_mean:<12.2f} \"\n",
    "      f\"[{tau_ci[0]:.2f}, {tau_ci[1]:.2f}]\")\n",
    "\n",
    "sigma_ci = np.percentile(np.sqrt(sigma2_samples[burnin_hier:]), [2.5, 97.5])\n",
    "print(f\"{'Sigma (ind SD)':<15} {true_sigma:<8.2f} {sigma_post_mean:<12.2f} \"\n",
    "      f\"[{sigma_ci[0]:.2f}, {sigma_ci[1]:.2f}]\")\n",
    "\n",
    "print(\"\\n=== ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡ã®æ¨å®š ===\")\n",
    "print(f\"{'Group':<8} {'True':<8} {'Post Mean':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for j in range(n_groups):\n",
    "    mu_j_ci = np.percentile(mu_samples[burnin_hier:, j], [2.5, 97.5])\n",
    "    print(f\"{j+1:<8} {true_group_means[j]:<8.2f} {mu_post_mean[j]:<12.2f} \"\n",
    "          f\"[{mu_j_ci[0]:.2f}, {mu_j_ci[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšå±¤ãƒ¢ãƒ‡ãƒ«çµæœã®å¯è¦–åŒ–\n",
    "def plot_hierarchical_results(alpha_samples, tau2_samples, sigma2_samples, mu_samples,\n",
    "                            y, group_ids, true_alpha, true_tau, true_sigma, true_group_means,\n",
    "                            burnin=2000):\n",
    "    \"\"\"\n",
    "    éšå±¤ãƒ¢ãƒ‡ãƒ«çµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    n_groups = mu_samples.shape[1]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    \n",
    "    # è¶…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    axes[0, 0].plot(alpha_samples, alpha=0.8, linewidth=0.8)\n",
    "    axes[0, 0].axhline(true_alpha, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[0, 0].set_title('Alpha (Overall Mean)')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Alpha')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(np.sqrt(tau2_samples), alpha=0.8, linewidth=0.8, color='green')\n",
    "    axes[0, 1].axhline(true_tau, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[0, 1].set_title('Tau (Between-group SD)')\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Tau')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 2].plot(np.sqrt(sigma2_samples), alpha=0.8, linewidth=0.8, color='orange')\n",
    "    axes[0, 2].axhline(true_sigma, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 2].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[0, 2].set_title('Sigma (Within-group SD)')\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Sigma')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # åˆ†æ•£æˆåˆ†ã®æ¯”è¼ƒ\n",
    "    tau_post = np.sqrt(tau2_samples[burnin:])\n",
    "    sigma_post = np.sqrt(sigma2_samples[burnin:])\n",
    "    icc_post = tau2_samples[burnin:] / (tau2_samples[burnin:] + sigma2_samples[burnin:])\n",
    "    \n",
    "    axes[0, 3].hist(icc_post, bins=50, alpha=0.7, density=True, color='purple')\n",
    "    true_icc = true_tau**2 / (true_tau**2 + true_sigma**2)\n",
    "    axes[0, 3].axvline(true_icc, color='red', linestyle='--', linewidth=2, label='True ICC')\n",
    "    axes[0, 3].axvline(np.mean(icc_post), color='purple', linestyle='-', linewidth=2, label='Post mean')\n",
    "    axes[0, 3].set_title('Intraclass Correlation (ICC)')\n",
    "    axes[0, 3].set_xlabel('ICC')\n",
    "    axes[0, 3].set_ylabel('Density')\n",
    "    axes[0, 3].legend()\n",
    "    \n",
    "    # è¶…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒ\n",
    "    axes[1, 0].hist(alpha_samples[burnin:], bins=50, alpha=0.7, density=True, color='skyblue')\n",
    "    axes[1, 0].axvline(true_alpha, color='red', linestyle='--', linewidth=2, label='True')\n",
    "    axes[1, 0].axvline(np.mean(alpha_samples[burnin:]), color='blue', linestyle='-', linewidth=2, label='Post mean')\n",
    "    axes[1, 0].set_title('Alpha Posterior')\n",
    "    axes[1, 0].set_xlabel('Alpha')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].hist(tau_post, bins=50, alpha=0.7, density=True, color='lightgreen')\n",
    "    axes[1, 1].axvline(true_tau, color='red', linestyle='--', linewidth=2, label='True')\n",
    "    axes[1, 1].axvline(np.mean(tau_post), color='green', linestyle='-', linewidth=2, label='Post mean')\n",
    "    axes[1, 1].set_title('Tau Posterior')\n",
    "    axes[1, 1].set_xlabel('Tau')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    axes[1, 2].hist(sigma_post, bins=50, alpha=0.7, density=True, color='lightsalmon')\n",
    "    axes[1, 2].axvline(true_sigma, color='red', linestyle='--', linewidth=2, label='True')\n",
    "    axes[1, 2].axvline(np.mean(sigma_post), color='orange', linestyle='-', linewidth=2, label='Post mean')\n",
    "    axes[1, 2].set_title('Sigma Posterior')\n",
    "    axes[1, 2].set_xlabel('Sigma')\n",
    "    axes[1, 2].set_ylabel('Density')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡ã®åæŸï¼ˆæœ€åˆã®4ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, min(4, n_groups)))\n",
    "    for j in range(min(4, n_groups)):\n",
    "        axes[1, 3].plot(mu_samples[:, j], alpha=0.7, color=colors[j], \n",
    "                       linewidth=0.8, label=f'Group {j+1}')\n",
    "        axes[1, 3].axhline(true_group_means[j], color=colors[j], linestyle='--', alpha=0.7)\n",
    "    axes[1, 3].axvline(burnin, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[1, 3].set_title('Group Means Convergence')\n",
    "    axes[1, 3].set_xlabel('Iteration')\n",
    "    axes[1, 3].set_ylabel('Group Mean')\n",
    "    axes[1, 3].legend()\n",
    "    axes[1, 3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ç¸®å°åŠ¹æœã®å¯è¦–åŒ–\n",
    "    sample_group_means = np.array([np.mean(y[group_ids == j]) for j in range(n_groups)])\n",
    "    posterior_group_means = np.mean(mu_samples[burnin:], axis=0)\n",
    "    \n",
    "    axes[2, 0].scatter(sample_group_means, posterior_group_means, alpha=0.7, s=50)\n",
    "    axes[2, 0].plot([sample_group_means.min(), sample_group_means.max()], \n",
    "                   [sample_group_means.min(), sample_group_means.max()], \n",
    "                   'r--', alpha=0.7, label='No shrinkage')\n",
    "    axes[2, 0].set_title('Shrinkage Effect')\n",
    "    axes[2, 0].set_xlabel('Sample Group Mean')\n",
    "    axes[2, 0].set_ylabel('Posterior Group Mean')\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡ã®æ¯”è¼ƒ\n",
    "    x_pos = np.arange(n_groups)\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[2, 1].bar(x_pos - width, true_group_means, width, label='True', alpha=0.7, color='red')\n",
    "    axes[2, 1].bar(x_pos, sample_group_means, width, label='Sample', alpha=0.7, color='blue')\n",
    "    axes[2, 1].bar(x_pos + width, posterior_group_means, width, label='Posterior', alpha=0.7, color='green')\n",
    "    \n",
    "    # ä¿¡é ¼åŒºé–“ã®è¿½åŠ \n",
    "    for j in range(n_groups):\n",
    "        ci = np.percentile(mu_samples[burnin:, j], [2.5, 97.5])\n",
    "        axes[2, 1].errorbar(j + width, posterior_group_means[j], \n",
    "                           yerr=[[posterior_group_means[j] - ci[0]], [ci[1] - posterior_group_means[j]]], \n",
    "                           fmt='none', color='black', capsize=3)\n",
    "    \n",
    "    axes[2, 1].set_title('Group Means Comparison')\n",
    "    axes[2, 1].set_xlabel('Group')\n",
    "    axes[2, 1].set_ylabel('Mean')\n",
    "    axes[2, 1].set_xticks(x_pos)\n",
    "    axes[2, 1].set_xticklabels([f'G{j+1}' for j in range(n_groups)])\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # äºˆæ¸¬åˆ†å¸ƒ\n",
    "    # æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã®äºˆæ¸¬åˆ†å¸ƒ\n",
    "    new_group_samples = np.random.normal(alpha_samples[burnin:], np.sqrt(tau2_samples[burnin:]))\n",
    "    \n",
    "    axes[2, 2].hist(new_group_samples, bins=50, alpha=0.7, density=True, color='lightcyan', label='New group prediction')\n",
    "    axes[2, 2].axvline(true_alpha, color='red', linestyle='--', linewidth=2, label='True overall mean')\n",
    "    axes[2, 2].set_title('Prediction for New Group')\n",
    "    axes[2, 2].set_xlabel('Predicted Group Mean')\n",
    "    axes[2, 2].set_ylabel('Density')\n",
    "    axes[2, 2].legend()\n",
    "    \n",
    "    # å€‹äººãƒ¬ãƒ™ãƒ«äºˆæ¸¬\n",
    "    # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã®æ–°ã—ã„å€‹äººã®äºˆæ¸¬ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—1ã®ä¾‹ï¼‰\n",
    "    group_0_individual_pred = np.random.normal(mu_samples[burnin:, 0], np.sqrt(sigma2_samples[burnin:]))\n",
    "    \n",
    "    axes[2, 3].hist(group_0_individual_pred, bins=50, alpha=0.7, density=True, color='lightpink', \n",
    "                   label='New individual in Group 1')\n",
    "    axes[2, 3].axvline(true_group_means[0], color='red', linestyle='--', linewidth=2, label='True Group 1 mean')\n",
    "    axes[2, 3].set_title('Individual Prediction (Group 1)')\n",
    "    axes[2, 3].set_xlabel('Predicted Score')\n",
    "    axes[2, 3].set_ylabel('Density')\n",
    "    axes[2, 3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# çµæœã®å¯è¦–åŒ–\n",
    "plot_hierarchical_results(alpha_samples, tau2_samples, sigma2_samples, mu_samples,\n",
    "                         y_hier, group_ids, true_alpha, true_tau, true_sigma, true_group_means)\n",
    "\n",
    "# ç¸®å°åŠ¹æœã®å®šé‡åŒ–\n",
    "burnin_hier = 2000\n",
    "sample_group_means = np.array([np.mean(y_hier[group_ids == j]) for j in range(n_groups)])\n",
    "posterior_group_means = np.mean(mu_samples[burnin_hier:], axis=0)\n",
    "overall_mean = np.mean(y_hier)\n",
    "\n",
    "# ç¸®å°ç‡ã®è¨ˆç®—\n",
    "shrinkage_factors = 1 - (posterior_group_means - overall_mean) / (sample_group_means - overall_mean)\n",
    "\n",
    "print(f\"\\n=== ç¸®å°åŠ¹æœ ===\")\n",
    "print(f\"{'Group':<8} {'Sample Mean':<12} {'Posterior Mean':<15} {'Shrinkage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for j in range(n_groups):\n",
    "    print(f\"{j+1:<8} {sample_group_means[j]:<12.2f} {posterior_group_means[j]:<15.2f} {shrinkage_factors[j]:<10.3f}\")\n",
    "\n",
    "print(f\"\\nå¹³å‡ç¸®å°ç‡: {np.mean(np.abs(shrinkage_factors)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 æ¼”ç¿’å•é¡Œ\n",
    "\n",
    "### å•é¡Œ1ï¼šå¤‰åŒ–ç‚¹æ¤œå‡º\n",
    "æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦å¹³å‡ãŒå¤‰åŒ–ã™ã‚‹ç‚¹ã‚’æ¤œå‡ºã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ1: å¤‰åŒ–ç‚¹æ¤œå‡ºãƒ¢ãƒ‡ãƒ«\n",
    "def generate_changepoint_data(n=100, changepoint=50, mu1=0, mu2=3, sigma=1, seed=42):\n",
    "    \"\"\"\n",
    "    å¤‰åŒ–ç‚¹ã®ã‚ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    y = np.zeros(n)\n",
    "    y[:changepoint] = np.random.normal(mu1, sigma, changepoint)\n",
    "    y[changepoint:] = np.random.normal(mu2, sigma, n - changepoint)\n",
    "    \n",
    "    return y, changepoint\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "y_cp, true_cp = generate_changepoint_data()\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_cp, 'b-', alpha=0.7, linewidth=1)\n",
    "plt.axvline(true_cp, color='red', linestyle='--', linewidth=2, label=f'True changepoint: {true_cp}')\n",
    "plt.title('Time Series with Changepoint')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ã“ã“ã«å¤‰åŒ–ç‚¹æ¤œå‡ºã®MCMCãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "# ãƒ’ãƒ³ãƒˆï¼š\n",
    "# 1. å¤‰åŒ–ç‚¹ã®ä½ç½®ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†\n",
    "# 2. å¤‰åŒ–ç‚¹å‰å¾Œã®å¹³å‡ã‚’åˆ¥ã€…ã«ãƒ¢ãƒ‡ãƒ«åŒ–\n",
    "# 3. å¤‰åŒ–ç‚¹ã®äº‹å‰åˆ†å¸ƒã‚’è¨­å®šï¼ˆä¸€æ§˜åˆ†å¸ƒãªã©ï¼‰\n",
    "\n",
    "def changepoint_detection_mcmc(y, n_iterations=5000):\n",
    "    \"\"\"\n",
    "    å¤‰åŒ–ç‚¹æ¤œå‡ºã®MCMCå®Ÿè£…\n",
    "    \n",
    "    ãƒ¢ãƒ‡ãƒ«:\n",
    "    y_t ~ N(mu1, sigma^2) for t < tau\n",
    "    y_t ~ N(mu2, sigma^2) for t >= tau\n",
    "    \n",
    "    tau ~ Uniform(1, n-1)\n",
    "    mu1, mu2 ~ N(0, 100)\n",
    "    sigma^2 ~ InvGamma(1, 1)\n",
    "    \"\"\"\n",
    "    # ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "    pass  # å­¦ç¿’è€…ãŒå®Ÿè£…\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "# cp_samples, mu1_samples, mu2_samples, sigma2_samples = changepoint_detection_mcmc(y_cp)\n",
    "# # çµæœã®åˆ†æã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å•é¡Œ2ï¼šãƒã‚¢ã‚½ãƒ³å›å¸°\n",
    "ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ™ã‚¤ã‚ºãƒã‚¢ã‚½ãƒ³å›å¸°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ2: ãƒ™ã‚¤ã‚ºãƒã‚¢ã‚½ãƒ³å›å¸°\n",
    "def generate_poisson_data(n_samples=150, n_features=2, seed=42):\n",
    "    \"\"\"\n",
    "    ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # çœŸã®å›å¸°ä¿‚æ•°\n",
    "    true_beta = np.array([1.0, 0.5, -0.3])  # intercept + 2 features\n",
    "    \n",
    "    # èª¬æ˜å¤‰æ•°\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    X = np.column_stack([np.ones(n_samples), X])  # intercepté …ã‚’è¿½åŠ \n",
    "    \n",
    "    # ãƒªãƒ³ã‚¯é–¢æ•°ï¼ˆå¯¾æ•°ãƒªãƒ³ã‚¯ï¼‰\n",
    "    log_lambda = X @ true_beta\n",
    "    lambda_param = np.exp(log_lambda)\n",
    "    \n",
    "    # ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    y = np.random.poisson(lambda_param)\n",
    "    \n",
    "    return X, y, true_beta\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X_pois, y_pois, true_beta_pois = generate_poisson_data()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒ\n",
    "axes[0].hist(y_pois, bins=range(max(y_pois)+2), alpha=0.7, density=False)\n",
    "axes[0].set_title('Count Data Distribution')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# èª¬æ˜å¤‰æ•°ã¨ã®é–¢ä¿‚\n",
    "axes[1].scatter(X_pois[:, 1], y_pois, alpha=0.6)\n",
    "axes[1].set_title('Count vs X1')\n",
    "axes[1].set_xlabel('X1')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "axes[2].scatter(X_pois[:, 2], y_pois, alpha=0.6)\n",
    "axes[2].set_title('Count vs X2')\n",
    "axes[2].set_xlabel('X2')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_pois.shape[0]} x {X_pois.shape[1]}\")\n",
    "print(f\"çœŸã®å›å¸°ä¿‚æ•°: {true_beta_pois}\")\n",
    "print(f\"ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {np.mean(y_pois):.2f}\")\n",
    "print(f\"ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æ•£: {np.var(y_pois):.2f}\")\n",
    "\n",
    "# ã“ã“ã«ãƒ™ã‚¤ã‚ºãƒã‚¢ã‚½ãƒ³å›å¸°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "# ãƒ’ãƒ³ãƒˆï¼š\n",
    "# 1. å¯¾æ•°å°¤åº¦: Î£[y_i * log(Î»_i) - Î»_i - log(y_i!)]\n",
    "# 2. Î»_i = exp(X_i @ Î²)\n",
    "# 3. MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼ˆè§£æçš„å…±å½¹ãŒãªã„ãŸã‚ï¼‰\n",
    "\n",
    "def bayesian_poisson_regression_mh(X, y, n_iterations=8000, prior_beta_var=10.0, step_size=0.1):\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¤ã‚ºãƒã‚¢ã‚½ãƒ³å›å¸°ã®ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•\n",
    "    \n",
    "    ãƒ¢ãƒ‡ãƒ«:\n",
    "    y_i ~ Poisson(Î»_i)\n",
    "    log(Î»_i) = X_i @ Î²\n",
    "    Î²_j ~ N(0, prior_beta_var)\n",
    "    \"\"\"\n",
    "    # ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "    pass  # å­¦ç¿’è€…ãŒå®Ÿè£…\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "# beta_samples_pois, acceptance_rate_pois = bayesian_poisson_regression_mh(X_pois, y_pois)\n",
    "# # çµæœã®åˆ†æã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ã¾ã¨ã‚ï¼šå®Ÿè·µãƒ™ã‚¤ã‚ºåˆ†æã®ãƒã‚¹ã‚¿ãƒ¼\n\nã“ã®ç« ã§ã¯ã€MCMCã‚’ç”¨ã„ãŸ**å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ™ã‚¤ã‚ºæ¨è«–**ã‚’åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸã€‚\n\n### ğŸ¯ ç¿’å¾—ã—ãŸå®Ÿè·µã‚¹ã‚­ãƒ«\n\n#### 1. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**\n- **ãƒ‡ãƒ¼ã‚¿ç†è§£**: EDAã€æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤æ¤œå‡º\n- **å‰å‡¦ç†è¨­è¨ˆ**: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥\n- **ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**: é©åˆ‡ãªäº‹å‰åˆ†å¸ƒã¨å°¤åº¦ã®é¸æŠ\n- **å“è³ªä¿è¨¼**: è¨ºæ–­ãƒ»æ¤œè¨¼ãƒ»äºˆæ¸¬ã®çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n\n#### 2. **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã®å®Ÿè£…**\n- **ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°**: ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹åŠ¹ç‡çš„å®Ÿè£…\n- **ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°**: MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨é©å¿œçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n- **éšå±¤ãƒ¢ãƒ‡ãƒ«**: å¤šãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±å€Ÿç”¨ï¼ˆinformation borrowingï¼‰\n- **æ™‚ç³»åˆ—åˆ†æ**: å¤‰åŒ–ç‚¹æ¤œå‡ºã¨çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«\n\n#### 3. **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®å‰µå‡º**\n- **äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–**: æ„æ€æ±ºå®šãƒªã‚¹ã‚¯ã®è©•ä¾¡\n- **ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ**: ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿã®æŠ½å‡º\n- **ã‚·ãƒŠãƒªã‚ªåˆ†æ**: What-ifåˆ†æã«ã‚ˆã‚‹æˆ¦ç•¥ç«‹æ¡ˆæ”¯æ´\n- **è§£é‡ˆå¯èƒ½æ€§**: ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®èª¬æ˜è²¬ä»»\n\n### ğŸ› ï¸ å®Ÿè£…ã«ãŠã‘ã‚‹é‡è¦æŠ€è¡“\n\n#### æŠ€è¡“çš„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n- **æ•°å€¤å®‰å®šæ€§**: å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿\n- **åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: äº‹å‰è¨ˆç®—ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–\n- **é©å¿œçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: å—ç†ç‡ã«åŸºã¥ãè‡ªå‹•ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºèª¿æ•´\n- **è¨ºæ–­ã®è‡ªå‹•åŒ–**: åæŸãƒ»åŠ¹ç‡ãƒ»å“è³ªã®çµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ \n\n#### ãƒ¢ãƒ‡ãƒªãƒ³ã‚°å“²å­¦\n- **äº‹å‰æƒ…å ±ã®æ´»ç”¨**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ•°å­¦çš„è¡¨ç¾\n- **éšå±¤åŒ–ã®åŠ›**: ã‚°ãƒ«ãƒ¼ãƒ—é–“æƒ…å ±ã®åŠ¹æœçš„ãªå€Ÿç”¨\n- **ä¸ç¢ºå®Ÿæ€§ã®æ˜ç¤º**: ç‚¹æ¨å®šã‚’è¶…ãˆãŸåˆ†å¸ƒæ¨è«–\n- **å …ç‰¢æ€§ã®ç¢ºä¿**: å¤–ã‚Œå€¤ã‚„ä»®å®šé•åã¸ã®å¯¾å¿œ\n\n### ğŸ“Š å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãã®é©ç”¨å ´é¢\n\n| ãƒ¢ãƒ‡ãƒ« | é©ç”¨å ´é¢ | ä¸»è¦æŠ€è¡“ | ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ |\n|--------|----------|----------|--------------|\n| **ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°** | å£²ä¸Šäºˆæ¸¬ã€ã‚³ã‚¹ãƒˆåˆ†æ | ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | ä¿¡é ¼åŒºé–“ä»˜ãäºˆæ¸¬ |\n| **ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°** | é¡§å®¢é›¢åã€å“è³ªåˆ¤å®š | MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | ãƒªã‚¹ã‚¯ç¢ºç‡ã®å®šé‡åŒ– |\n| **éšå±¤ãƒ¢ãƒ‡ãƒ«** | åœ°åŸŸåˆ¥åˆ†æã€A/Bãƒ†ã‚¹ãƒˆ | æƒ…å ±å€Ÿç”¨ | å°ã‚µãƒ³ãƒ—ãƒ«ç¾¤ã®æ”¹å–„ |\n| **å¤‰åŒ–ç‚¹æ¤œå‡º** | ç•°å¸¸æ¤œçŸ¥ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ | çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ« | æ—©æœŸè­¦å‘Šã‚·ã‚¹ãƒ†ãƒ  |\n| **ãƒã‚¢ã‚½ãƒ³å›å¸°** | ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿåˆ†æ | æŒ‡æ•°æ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | ãƒªã‚¹ã‚¯ç®¡ç† |\n\n### ğŸ¯ å®Ÿå‹™ã§ã®å¿œç”¨æˆ¦ç•¥\n\n#### æ¥­ç•Œåˆ¥æ´»ç”¨ä¾‹\n\n**ğŸ¢ è£½é€ æ¥­:**\n- å“è³ªç®¡ç†ã§ã®ä¸è‰¯ç‡äºˆæ¸¬\n- è¨­å‚™æ•…éšœã®äºˆå…†æ¤œçŸ¥\n- ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æœ€é©åŒ–\n\n**ğŸª å°å£²æ¥­:**\n- éœ€è¦äºˆæ¸¬ã¨åœ¨åº«æœ€é©åŒ–  \n- é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³\n- ä¾¡æ ¼å¼¾æ€§åˆ†æ\n\n**ğŸ¥ ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢:**\n- æ²»ç™‚åŠ¹æœã®å€‹äººå·®åˆ†æ\n- è‡¨åºŠè©¦é¨“ãƒ‡ãƒ¼ã‚¿ã®è§£æ\n- ãƒªã‚¹ã‚¯å±¤åˆ¥åŒ–\n\n**ğŸ’° é‡‘èæ¥­:**\n- ä¿¡ç”¨ãƒªã‚¹ã‚¯è©•ä¾¡\n- ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–\n- ä¸æ­£æ¤œçŸ¥\n\n### ğŸ”® æ¬¡ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—\n\n#### Advanced Topicsï¼ˆChapter 6ï¼‰ã¸ã®æº–å‚™\n- **ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­**: é«˜æ¬¡å…ƒã§ã®åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n- **å¤‰åˆ†æ¨è«–**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®è¿‘ä¼¼æ¨è«–\n- **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n- **è¨ˆç®—æœ€é©åŒ–**: GPUåŠ é€Ÿã¨ã‚¯ãƒ©ã‚¦ãƒ‰æ´»ç”¨\n\n#### ç¶™ç¶šçš„å­¦ç¿’ã®ãŸã‚ã«\n- **è«–æ–‡èª­è§£**: æœ€æ–°ç ”ç©¶å‹•å‘ã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—\n- **OSSè²¢çŒ®**: PyMCã€Stanã€NumPyroã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®å‚åŠ \n- **å®Ÿãƒ‡ãƒ¼ã‚¿çµŒé¨“**: Kaggleã€ç ”ç©¶ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®å®Ÿè·µ\n- **çŸ¥è­˜å…±æœ‰**: ãƒ–ãƒ­ã‚°ã€å‹‰å¼·ä¼šã§ã®ç™ºä¿¡\n\n### ğŸ’¡ æˆåŠŸã®ç§˜è¨£\n\n> **ã€Œç†è«–ã¨å®Ÿè·µã®æ©‹æ¸¡ã—ã€** ãŒç¾ä»£ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®æ ¸å¿ƒã‚¹ã‚­ãƒ«ã§ã™ã€‚MCMCã¯å˜ãªã‚‹è¨ˆç®—æ‰‹æ³•ã§ã¯ãªãã€**ä¸ç¢ºå®Ÿæ€§ã¨å‘ãåˆã†å“²å­¦**ãã®ã‚‚ã®ã§ã™ã€‚\n\n**è¨˜æ†¶ã™ã¹ãåŸå‰‡:**\n- ğŸ¯ **å•é¡Œè¨­å®šã®æ˜ç¢ºåŒ–**: ä½•ã‚’çŸ¥ã‚ŠãŸã„ã®ã‹ã€ä½•ã‚’æ±ºã‚ãŸã„ã®ã‹\n- ğŸ“ **ãƒ¢ãƒ‡ãƒ«ã®é©åˆ‡æ€§**: ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©ã—ãŸè¨­è¨ˆ\n- ğŸ” **è¨ºæ–­ã®å¾¹åº•**: åæŸãƒ»å¦¥å½“æ€§ãƒ»äºˆæ¸¬æ€§èƒ½ã®å¤šè§’çš„è©•ä¾¡\n- ğŸ’¬ **è§£é‡ˆã®ä¸å¯§ã•**: ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã®åŠ¹æœçš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³\n- ğŸ”„ **åå¾©çš„æ”¹å–„**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ã‹ã—ãŸãƒ¢ãƒ‡ãƒ«é€²åŒ–\n\nã‚ãªãŸã¯ä»Šã€**å®Ÿè·µçš„ãƒ™ã‚¤ã‚ºåˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ**ã¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¾¡å€¤ã‚’ç”Ÿã¿å‡ºã™æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚MCMCã¨ã„ã†å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã€ä¸ç¢ºå®Ÿãªä¸–ç•Œã§ã®æ„æ€æ±ºå®šã‚’æ”¯æ´ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼\n\n### ğŸ“ Final Challenge\n\nå®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã«æœ¬ç« ã®æ‰‹æ³•ã‚’é©ç”¨ã—ã€**å®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆ**ã‚’ä½œæˆã—ã¦ã¿ã¦ãã ã•ã„ï¼š\n1. å•é¡Œè¨­å®šã¨ä»®èª¬\n2. ãƒ‡ãƒ¼ã‚¿ç†è§£ã¨EDA  \n3. ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã¨å®Ÿè£…\n4. è¨ºæ–­ã¨æ¤œè¨¼\n5. ãƒ“ã‚¸ãƒã‚¹ææ¡ˆã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n\nã“ã‚Œã“ããŒã€**çœŸã®å®Ÿè·µåŠ›**ã®è¨¼æ˜ã§ã™ï¼"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}