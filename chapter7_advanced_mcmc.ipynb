{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 7: 高度なMCMC手法\n\n## 学習目標\n- 適応的MCMC手法を習得する\n- アンサンブルサンプラーの原理を理解する\n- 並列・分散MCMCの概念を理解する\n- 変分推論との比較を学ぶ\n- ベイズ最適化とMCMCの関係を学ぶ\n- 実用的なMCMCライブラリの使用方法を習得する"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.1 適応的MCMC手法\n\n適応的MCMCは、サンプリング中にアルゴリズムのパラメータを自動調整する手法群です。固定パラメータの限界を克服し、より効率的なサンプリングを実現します。\n\n### 主要なアプローチ\n- **適応的メトロポリス法（AM）**：提案分布の共分散を適応的に調整\n- **遅延棄却適応的メトロポリス（DRAM）**：棄却時に縮小提案を試行\n- **適応的独立性サンプラー**：独立提案分布を動的に最適化\n- **ロバスト適応的メトロポリス（RAM）**：ロバストな共分散推定"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AdaptiveMetropolis:\n    \"\"\"\n    適応的メトロポリス法（AM）\n    Haario et al. (2001) による手法\n    \"\"\"\n    \n    def __init__(self, log_prob_fn, initial_cov_scale=0.1, adaptation_start=100):\n        self.log_prob_fn = log_prob_fn\n        self.initial_cov_scale = initial_cov_scale\n        self.adaptation_start = adaptation_start\n        \n    def sample(self, initial_value, n_samples):\n        \"\"\"\n        適応的サンプリングの実行\n        \"\"\"\n        dim = len(initial_value)\n        samples = np.zeros((n_samples, dim))\n        \n        # 初期設定\n        current = initial_value.copy()\n        current_log_prob = self.log_prob_fn(current)\n        \n        # 適応的共分散行列\n        cov_matrix = self.initial_cov_scale**2 * np.eye(dim)\n        epsilon = 1e-6  # 数値安定性のため\n        sd_scale = 2.4**2 / dim  # 最適スケーリング\n        \n        n_accepted = 0\n        \n        for i in range(n_samples):\n            # 提案分布の更新\n            if i >= self.adaptation_start:\n                # 経験共分散行列の計算\n                sample_mean = np.mean(samples[:i], axis=0)\n                sample_cov = np.cov(samples[:i].T, ddof=1)\n                \n                # Regularization（数値安定性のため）\n                if np.any(np.isnan(sample_cov)) or np.any(np.isinf(sample_cov)):\n                    cov_matrix = self.initial_cov_scale**2 * np.eye(dim)\n                else:\n                    cov_matrix = sd_scale * (sample_cov + epsilon * np.eye(dim))\n            \n            # 提案\n            try:\n                proposed = np.random.multivariate_normal(current, cov_matrix)\n            except np.linalg.LinAlgError:\n                # 共分散行列が特異の場合\n                proposed = current + np.random.normal(0, self.initial_cov_scale, dim)\n            \n            proposed_log_prob = self.log_prob_fn(proposed)\n            \n            # 受理確率\n            log_alpha = proposed_log_prob - current_log_prob\n            alpha = min(1.0, np.exp(log_alpha))\n            \n            # 受理/棄却\n            if np.random.rand() < alpha:\n                current = proposed\n                current_log_prob = proposed_log_prob\n                n_accepted += 1\n            \n            samples[i] = current\n            \n            if (i + 1) % 1000 == 0:\n                print(f\"Iteration {i+1}/{n_samples}, Acceptance rate: {n_accepted/(i+1):.3f}\")\n        \n        acceptance_rate = n_accepted / n_samples\n        return samples, acceptance_rate, cov_matrix\n\n# テスト用の2次元正規分布\ndef multivariate_normal_log_prob(x, mu, cov):\n    \"\"\"多変量正規分布の対数確率密度\"\"\"\n    diff = x - mu\n    try:\n        chol = cholesky(cov, lower=True)\n        log_det = 2 * np.sum(np.log(np.diag(chol)))\n        solve = solve_triangular(chol, diff, lower=True)\n        mahalanobis_sq = np.sum(solve**2)\n    except np.linalg.LinAlgError:\n        return -np.inf\n    \n    return -0.5 * (len(mu) * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n\n# パラメータ設定\nmu_target = np.array([0.0, 0.0])\ncov_target = np.array([[1.0, 0.8], [0.8, 1.0]])\n\n# 適応的メトロポリス法の実行\nprint(\"適応的メトロポリス法サンプリング実行中...\")\nam = AdaptiveMetropolis(\n    log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target)\n)\n\nam_samples, am_acceptance_rate, final_cov = am.sample(\n    initial_value=np.array([0.0, 0.0]),\n    n_samples=5000\n)\n\nprint(f\"\\n適応的メトロポリス法受理率: {am_acceptance_rate:.3f}\")\nprint(f\"\\n最終提案共分散行列:\")\nprint(final_cov)\nprint(f\"\\n真の共分散行列:\")\nprint(cov_target)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 適応的メトロポリス法の結果可視化\ndef plot_adaptive_results(samples, final_cov, mu_target, cov_target, title=\"Adaptive Metropolis Results\"):\n    \"\"\"\n    適応的メトロポリス法結果の可視化\n    \"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    burnin = 500\n    samples_clean = samples[burnin:]\n    \n    # 1. トレースプロット\n    axes[0, 0].plot(samples[:2000, 0], alpha=0.8, label='X1', linewidth=0.8)\n    axes[0, 0].plot(samples[:2000, 1], alpha=0.8, label='X2', linewidth=0.8)\n    axes[0, 0].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n    axes[0, 0].set_title('Trace Plot')\n    axes[0, 0].set_xlabel('Iteration')\n    axes[0, 0].set_ylabel('Value')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # 2. 散布図と真の分布の等高線\n    axes[0, 1].scatter(samples_clean[::5, 0], samples_clean[::5, 1], alpha=0.6, s=1)\n    \n    # 真の分布の等高線\n    x1_range = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 50)\n    x2_range = np.linspace(samples_clean[:, 1].min(), samples_clean[:, 1].max(), 50)\n    X1, X2 = np.meshgrid(x1_range, x2_range)\n    pos = np.dstack((X1, X2))\n    rv = stats.multivariate_normal(mu_target, cov_target)\n    axes[0, 1].contour(X1, X2, rv.pdf(pos), colors='red', alpha=0.8, linewidths=2)\n    axes[0, 1].set_title('Samples with True Distribution')\n    axes[0, 1].set_xlabel('X1')\n    axes[0, 1].set_ylabel('X2')\n    axes[0, 1].set_aspect('equal')\n    \n    # 3. 共分散行列の進化\n    n_points = min(1000, len(samples) // 5)\n    cov_evolution = []\n    \n    for i in range(100, n_points, 50):\n        if i >= 100:\n            sample_cov = np.cov(samples[:i].T, ddof=1)\n            if not (np.any(np.isnan(sample_cov)) or np.any(np.isinf(sample_cov))):\n                cov_evolution.append(sample_cov[0, 1])  # 共分散要素\n    \n    if cov_evolution:\n        axes[0, 2].plot(range(100, 100 + len(cov_evolution) * 50, 50), cov_evolution, 'b-', alpha=0.8)\n        axes[0, 2].axhline(cov_target[0, 1], color='red', linestyle='--', label='True covariance')\n        axes[0, 2].set_title('Covariance Evolution')\n        axes[0, 2].set_xlabel('Iteration')\n        axes[0, 2].set_ylabel('Cov(X1, X2)')\n        axes[0, 2].legend()\n        axes[0, 2].grid(True, alpha=0.3)\n    \n    # 4. マージナル分布の比較\n    axes[1, 0].hist(samples_clean[:, 0], bins=50, density=True, alpha=0.7, \n                   color='lightblue', label='X1 samples')\n    x1_theory = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 100)\n    axes[1, 0].plot(x1_theory, stats.norm.pdf(x1_theory, mu_target[0], np.sqrt(cov_target[0, 0])), \n                   'r-', linewidth=2, label='X1 true')\n    axes[1, 0].set_title('Marginal Distribution X1')\n    axes[1, 0].set_xlabel('X1')\n    axes[1, 0].set_ylabel('Density')\n    axes[1, 0].legend()\n    \n    # 5. 自己相関関数\n    from statsmodels.tsa.stattools import acf\n    lags = min(100, len(samples_clean) // 10)\n    autocorr_x1 = acf(samples_clean[:, 0], nlags=lags, fft=True)\n    autocorr_x2 = acf(samples_clean[:, 1], nlags=lags, fft=True)\n    \n    axes[1, 1].plot(autocorr_x1, label='X1', alpha=0.8)\n    axes[1, 1].plot(autocorr_x2, label='X2', alpha=0.8)\n    axes[1, 1].axhline(0, color='k', linestyle='--', alpha=0.5)\n    axes[1, 1].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n    axes[1, 1].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n    axes[1, 1].set_title('Autocorrelation Functions')\n    axes[1, 1].set_xlabel('Lag')\n    axes[1, 1].set_ylabel('ACF')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    # 6. 統計サマリー\n    axes[1, 2].axis('off')\n    sample_mean = np.mean(samples_clean, axis=0)\n    sample_cov = np.cov(samples_clean.T)\n    \n    summary_text = f\"Statistical Summary\\n\\n\"\n    summary_text += f\"True mean: [{mu_target[0]:.3f}, {mu_target[1]:.3f}]\\n\"\n    summary_text += f\"Sample mean: [{sample_mean[0]:.3f}, {sample_mean[1]:.3f}]\\n\\n\"\n    summary_text += f\"True covariance:\\n[[{cov_target[0,0]:.3f}, {cov_target[0,1]:.3f}],\\n\"\n    summary_text += f\" [{cov_target[1,0]:.3f}, {cov_target[1,1]:.3f}]]\\n\\n\"\n    summary_text += f\"Sample covariance:\\n[[{sample_cov[0,0]:.3f}, {sample_cov[0,1]:.3f}],\\n\"\n    summary_text += f\" [{sample_cov[1,0]:.3f}, {sample_cov[1,1]:.3f}]]\\n\\n\"\n    summary_text += f\"Final proposal cov:\\n[[{final_cov[0,0]:.3f}, {final_cov[0,1]:.3f}],\\n\"\n    summary_text += f\" [{final_cov[1,0]:.3f}, {final_cov[1,1]:.3f}]]\"\n    \n    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes, \n                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n    \n    plt.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# 結果の可視化\nplot_adaptive_results(am_samples, final_cov, mu_target, cov_target)\n\n# 効率の計算\ndef compute_efficiency_metrics(samples, burnin_frac=0.1):\n    \"\"\"\n    効率指標の計算\n    \"\"\"\n    burnin = int(len(samples) * burnin_frac)\n    clean_samples = samples[burnin:]\n    \n    # 各次元の自己相関時間を計算\n    autocorr_times = []\n    eff_sample_sizes = []\n    \n    for dim in range(clean_samples.shape[1]):\n        data = clean_samples[:, dim]\n        lags = min(200, len(data) // 4)\n        autocorr = acf(data, nlags=lags, fft=True)\n        \n        # 統合自己相関時間\n        tau_int = 1.0\n        for lag in range(1, len(autocorr)):\n            if autocorr[lag] > 0.01:\n                tau_int += 2 * autocorr[lag]\n            else:\n                break\n        \n        autocorr_times.append(tau_int)\n        eff_sample_sizes.append(len(data) / (2 * tau_int + 1))\n    \n    return autocorr_times, eff_sample_sizes\n\nam_autocorr, am_eff = compute_efficiency_metrics(am_samples)\n\nprint(f\"\\n=== 適応的メトロポリス法効率指標 ===\")\nprint(f\"平均自己相関時間: {np.mean(am_autocorr):.2f}\")\nprint(f\"平均有効サンプルサイズ: {np.mean(am_eff):.1f}\")\nprint(f\"効率: {np.mean(am_eff)/len(am_samples[500:]):.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.2 アンサンブルサンプラー\n\nアンサンブルサンプラーは、複数のウォーカー（サンプラー）を同時に動かして効率的にサンプリングを行う手法です。特に高次元問題で威力を発揮します。\n\n### Affine Invariant Ensemble Sampler\n- Goodman & Weare (2010) により提案\n- アフィン変換に不変な性質\n- 複数のウォーカーが相互作用しながらサンプリング\n- emceeライブラリで実装されている"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleEnsembleSampler:\n    \"\"\"\n    簡易版Affine Invariant Ensemble Sampler\n    \n    注：これは教育目的の簡易実装です。\n    実際のemceeは最適化されたC実装を含みます。\n    \"\"\"\n    \n    def __init__(self, log_prob_fn, n_walkers, n_dim):\n        self.log_prob_fn = log_prob_fn\n        self.n_walkers = n_walkers\n        self.n_dim = n_dim\n        \n        if n_walkers < 2 * n_dim:\n            raise ValueError(f\"n_walkers ({n_walkers}) must be >= 2 * n_dim ({2 * n_dim})\")\n    \n    def sample(self, initial_positions, n_steps):\n        \"\"\"\n        アンサンブルサンプリングの実行\n        \n        Parameters:\n        - initial_positions: 初期位置 (n_walkers, n_dim)\n        - n_steps: ステップ数\n        \n        Returns:\n        - samples: サンプル配列 (n_steps, n_walkers, n_dim)\n        - acceptance_rates: 各ウォーカーの受理率\n        \"\"\"\n        samples = np.zeros((n_steps, self.n_walkers, self.n_dim))\n        current_positions = initial_positions.copy()\n        current_log_probs = np.array([self.log_prob_fn(pos) for pos in current_positions])\n        \n        n_accepted = np.zeros(self.n_walkers)\n        \n        for step in range(n_steps):\n            # 各ウォーカーに対して提案を生成\n            for k in range(self.n_walkers):\n                # 補完ウォーカーをランダム選択（自分以外）\n                others = list(range(self.n_walkers))\n                others.remove(k)\n                j = np.random.choice(others)\n                \n                # ストレッチファクターの生成\n                a = 2.0  # ストレッチパラメータ\n                z = ((a - 1.0) * np.random.rand() + 1.0) ** 2 / a\n                \n                # 提案位置の計算\n                proposed = current_positions[j] + z * (current_positions[k] - current_positions[j])\n                proposed_log_prob = self.log_prob_fn(proposed)\n                \n                # 受理確率（次元数を考慮）\n                log_alpha = (self.n_dim - 1) * np.log(z) + proposed_log_prob - current_log_probs[k]\n                alpha = min(1.0, np.exp(log_alpha))\n                \n                # 受理/棄却\n                if np.random.rand() < alpha:\n                    current_positions[k] = proposed\n                    current_log_probs[k] = proposed_log_prob\n                    n_accepted[k] += 1\n            \n            samples[step] = current_positions.copy()\n            \n            if (step + 1) % 1000 == 0:\n                avg_acceptance = np.mean(n_accepted) / (step + 1)\n                print(f\"Step {step+1}/{n_steps}, Average acceptance rate: {avg_acceptance:.3f}\")\n        \n        acceptance_rates = n_accepted / n_steps\n        return samples, acceptance_rates\n\n# アンサンブルサンプラーのテスト\nprint(\"アンサンブルサンプラー実行中...\")\n\n# 初期位置の設定（複数のウォーカー）\nn_walkers = 20\nn_dim = 2\ninitial_positions = np.random.normal(0, 1, (n_walkers, n_dim))\n\nensemble_sampler = SimpleEnsembleSampler(\n    log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target),\n    n_walkers=n_walkers,\n    n_dim=n_dim\n)\n\nensemble_samples, ensemble_acceptance_rates = ensemble_sampler.sample(\n    initial_positions=initial_positions,\n    n_steps=3000\n)\n\nprint(f\"\\n平均受理率: {np.mean(ensemble_acceptance_rates):.3f}\")\nprint(f\"受理率の標準偏差: {np.std(ensemble_acceptance_rates):.3f}\")\n\n# ウォーカー間の相関を確認\nprint(f\"\\nウォーカーごとの受理率:\")\nfor i, rate in enumerate(ensemble_acceptance_rates):\n    print(f\"Walker {i:2d}: {rate:.3f}\")\n\n# 全ウォーカーのサンプルを結合\nensemble_combined = ensemble_samples.reshape(-1, n_dim)\nprint(f\"\\n結合サンプル数: {len(ensemble_combined)}\")\n\n# 効率指標の計算\nens_autocorr, ens_eff = compute_efficiency_metrics(ensemble_combined)\nprint(f\"平均自己相関時間: {np.mean(ens_autocorr):.2f}\")\nprint(f\"平均有効サンプルサイズ: {np.mean(ens_eff):.1f}\")\nprint(f\"効率: {np.mean(ens_eff)/len(ensemble_combined[300:]):.2%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# アンサンブルサンプラーの可視化\ndef plot_ensemble_results(samples, n_walkers, mu_target, cov_target):\n    \"\"\"\n    アンサンブルサンプラー結果の可視化\n    \"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    burnin = 300\n    samples_clean = samples[burnin:]\n    combined_samples = samples_clean.reshape(-1, 2)\n    \n    # 1. 複数ウォーカーのトレースプロット\n    colors = plt.cm.tab10(np.linspace(0, 1, min(10, n_walkers)))\n    for i in range(min(10, n_walkers)):  # 最大10個のウォーカーを表示\n        axes[0, 0].plot(samples[:1500, i, 0], alpha=0.7, color=colors[i], linewidth=0.5)\n    axes[0, 0].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n    axes[0, 0].set_title('Walker Traces (X1)')\n    axes[0, 0].set_xlabel('Step')\n    axes[0, 0].set_ylabel('X1')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # 2. 散布図と真の分布\n    axes[0, 1].scatter(combined_samples[::20, 0], combined_samples[::20, 1], \n                      alpha=0.6, s=2, c='blue')\n    \n    # 真の分布の等高線\n    x1_range = np.linspace(combined_samples[:, 0].min(), combined_samples[:, 0].max(), 50)\n    x2_range = np.linspace(combined_samples[:, 1].min(), combined_samples[:, 1].max(), 50)\n    X1, X2 = np.meshgrid(x1_range, x2_range)\n    pos = np.dstack((X1, X2))\n    rv = stats.multivariate_normal(mu_target, cov_target)\n    axes[0, 1].contour(X1, X2, rv.pdf(pos), colors='red', alpha=0.8, linewidths=2)\n    axes[0, 1].set_title('Combined Samples')\n    axes[0, 1].set_xlabel('X1')\n    axes[0, 1].set_ylabel('X2')\n    axes[0, 1].set_aspect('equal')\n    \n    # 3. ウォーカーの軌跡（2次元）\n    for i in range(min(5, n_walkers)):\n        walker_traj = samples[:200, i, :]\n        axes[0, 2].plot(walker_traj[:, 0], walker_traj[:, 1], \n                       alpha=0.7, linewidth=1, label=f'Walker {i}')\n        axes[0, 2].plot(walker_traj[0, 0], walker_traj[0, 1], 'o', markersize=4)\n    axes[0, 2].set_title('Walker Trajectories (first 200 steps)')\n    axes[0, 2].set_xlabel('X1')\n    axes[0, 2].set_ylabel('X2')\n    axes[0, 2].legend()\n    axes[0, 2].set_aspect('equal')\n    \n    # 4. マージナル分布\n    axes[1, 0].hist(combined_samples[:, 0], bins=50, density=True, alpha=0.7, \n                   color='lightblue', label='X1 samples')\n    x1_theory = np.linspace(combined_samples[:, 0].min(), combined_samples[:, 0].max(), 100)\n    axes[1, 0].plot(x1_theory, stats.norm.pdf(x1_theory, mu_target[0], np.sqrt(cov_target[0, 0])), \n                   'r-', linewidth=2, label='X1 true')\n    axes[1, 0].set_title('Marginal Distribution X1')\n    axes[1, 0].set_xlabel('X1')\n    axes[1, 0].set_ylabel('Density')\n    axes[1, 0].legend()\n    \n    # 5. 自己相関関数\n    from statsmodels.tsa.stattools import acf\n    lags = min(100, len(combined_samples) // 10)\n    autocorr_x1 = acf(combined_samples[:, 0], nlags=lags, fft=True)\n    autocorr_x2 = acf(combined_samples[:, 1], nlags=lags, fft=True)\n    \n    axes[1, 1].plot(autocorr_x1, label='X1', alpha=0.8)\n    axes[1, 1].plot(autocorr_x2, label='X2', alpha=0.8)\n    axes[1, 1].axhline(0, color='k', linestyle='--', alpha=0.5)\n    axes[1, 1].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n    axes[1, 1].set_title('Autocorrelation Functions')\n    axes[1, 1].set_xlabel('Lag')\n    axes[1, 1].set_ylabel('ACF')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    # 6. 統計サマリー\n    axes[1, 2].axis('off')\n    sample_mean = np.mean(combined_samples, axis=0)\n    sample_cov = np.cov(combined_samples.T)\n    \n    summary_text = f\"Ensemble Sampler Summary\\n\\n\"\n    summary_text += f\"Number of walkers: {n_walkers}\\n\"\n    summary_text += f\"Total samples: {len(combined_samples)}\\n\\n\"\n    summary_text += f\"True mean: [{mu_target[0]:.3f}, {mu_target[1]:.3f}]\\n\"\n    summary_text += f\"Sample mean: [{sample_mean[0]:.3f}, {sample_mean[1]:.3f}]\\n\\n\"\n    summary_text += f\"True covariance:\\n[[{cov_target[0,0]:.3f}, {cov_target[0,1]:.3f}],\\n\"\n    summary_text += f\" [{cov_target[1,0]:.3f}, {cov_target[1,1]:.3f}]]\\n\\n\"\n    summary_text += f\"Sample covariance:\\n[[{sample_cov[0,0]:.3f}, {sample_cov[0,1]:.3f}],\\n\"\n    summary_text += f\" [{sample_cov[1,0]:.3f}, {sample_cov[1,1]:.3f}]]\"\n    \n    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes, \n                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n    \n    plt.suptitle('Ensemble Sampler Results', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# 結果の可視化\nplot_ensemble_results(ensemble_samples, n_walkers, mu_target, cov_target)\n\n# ランダムウォークMHとの比較\ndef random_walk_mh(log_prob_fn, initial_value, n_samples, step_size=0.5):\n    \"\"\"\n    ランダムウォーク・メトロポリス・ヘイスティングス法（比較用）\n    \"\"\"\n    dim = len(initial_value)\n    samples = np.zeros((n_samples, dim))\n    current = initial_value.copy()\n    current_log_prob = log_prob_fn(current)\n    n_accepted = 0\n    \n    proposal_cov = step_size**2 * np.eye(dim)\n    \n    for i in range(n_samples):\n        # 提案\n        proposed = np.random.multivariate_normal(current, proposal_cov)\n        proposed_log_prob = log_prob_fn(proposed)\n        \n        # 受理確率\n        log_alpha = proposed_log_prob - current_log_prob\n        alpha = min(1.0, np.exp(log_alpha))\n        \n        # 受理/棄却\n        if np.random.rand() < alpha:\n            current = proposed\n            current_log_prob = proposed_log_prob\n            n_accepted += 1\n        \n        samples[i] = current\n    \n    acceptance_rate = n_accepted / n_samples\n    return samples, acceptance_rate\n\n# 比較のためのランダムウォークMH\nprint(\"\\n比較用ランダムウォークMH実行中...\")\nrwmh_samples, rwmh_acceptance_rate = random_walk_mh(\n    log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target),\n    initial_value=np.array([0.0, 0.0]),\n    n_samples=len(ensemble_combined),\n    step_size=0.8\n)\n\nrwmh_autocorr, rwmh_eff = compute_efficiency_metrics(rwmh_samples)\n\nprint(f\"\\n=== 手法比較 ===\")\nprint(f\"{'Method':<20} {'Acceptance':<12} {'Mean ESS':<12} {'Mean τ':<12} {'Efficiency':<12}\")\nprint(\"-\" * 80)\n\nmethods = ['Random Walk MH', 'Adaptive Metropolis', 'Ensemble Sampler']\nall_samples = [rwmh_samples, am_samples, ensemble_combined]\nall_acceptance = [rwmh_acceptance_rate, am_acceptance_rate, np.mean(ensemble_acceptance_rates)]\nall_eff = [rwmh_eff, am_eff, ens_eff]\nall_autocorr = [rwmh_autocorr, am_autocorr, ens_autocorr]\n\nfor method, acc_rate, eff, autocorr, samples in zip(methods, all_acceptance, all_eff, all_autocorr, all_samples):\n    efficiency = np.mean(eff) / len(samples[300:])\n    print(f\"{method:<20} {acc_rate:<12.3f} {np.mean(eff):<12.1f} {np.mean(autocorr):<12.2f} {efficiency:<12.3f}\")\n\nprint(f\"\\n=== 相対性能（Random Walk MH基準） ===\")\nbase_eff = np.mean(rwmh_eff)\nfor method, eff in zip(methods, all_eff):\n    eff_ratio = np.mean(eff) / base_eff\n    print(f\"{method}: ESS improvement = {eff_ratio:.2f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.3 並列・分散MCMC\n\n大規模データやパラメータ数が多い問題では、並列・分散MCMCが有効です。複数のチェーンを並列実行し、効率的にサンプリングを行います。\n\n### 主要なアプローチ\n- **独立並列チェーン**：複数の独立なチェーンを並列実行\n- **分散データMCMC**：データを分割して各チェーンで処理\n- **Consensus Monte Carlo**：各チェーンの結果を統合\n- **Embarrassingly Parallel MCMC**：問題を分割して並列処理"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from concurrent.futures import ProcessPoolExecutor\nimport multiprocessing as mp\n\ndef single_chain_runner(args):\n    \"\"\"\n    単一チェーン実行のためのワーカー関数\n    \"\"\"\n    chain_id, log_prob_fn_params, initial_value, n_samples, step_size, seed = args\n    \n    # 各プロセスで独立した乱数シードを設定\n    np.random.seed(seed)\n    \n    # 対数確率関数の再構築（プロセス間通信のため）\n    mu, cov = log_prob_fn_params\n    log_prob_fn = lambda x: multivariate_normal_log_prob(x, mu, cov)\n    \n    # ランダムウォークMHの実行\n    samples, acceptance_rate = random_walk_mh(log_prob_fn, initial_value, n_samples, step_size)\n    \n    return chain_id, samples, acceptance_rate\n\ndef parallel_mcmc_chains(log_prob_fn_params, n_chains=4, n_samples=2000, step_size=0.8):\n    \"\"\"\n    複数チェーンの並列実行\n    \n    Parameters:\n    - log_prob_fn_params: 対数確率関数のパラメータ（プロセス間通信用）\n    - n_chains: チェーン数\n    - n_samples: チェーンあたりのサンプル数\n    - step_size: ステップサイズ\n    \n    Returns:\n    - all_samples: 全チェーンのサンプル\n    - acceptance_rates: 各チェーンの受理率\n    \"\"\"\n    \n    # 各チェーンの初期値（互いに異なる）\n    np.random.seed(42)\n    dim = 2\n    initial_values = [np.random.normal(0, 2, dim) for _ in range(n_chains)]\n    \n    # 各チェーンの引数準備\n    chain_args = []\n    for i in range(n_chains):\n        seed = 42 + i * 1000  # 各チェーンで異なるシード\n        chain_args.append((i, log_prob_fn_params, initial_values[i], n_samples, step_size, seed))\n    \n    print(f\"並列MCMC実行中... ({n_chains} chains, {n_samples} samples each)\")\n    \n    # 並列実行\n    with ProcessPoolExecutor(max_workers=min(n_chains, mp.cpu_count())) as executor:\n        results = list(executor.map(single_chain_runner, chain_args))\n    \n    # 結果の整理\n    all_samples = []\n    acceptance_rates = []\n    \n    for chain_id, samples, acc_rate in sorted(results):\n        all_samples.append(samples)\n        acceptance_rates.append(acc_rate)\n        print(f\"Chain {chain_id}: Acceptance rate = {acc_rate:.3f}\")\n    \n    return np.array(all_samples), np.array(acceptance_rates)\n\ndef gelman_rubin_diagnostic(chains):\n    \"\"\"\n    Gelman-Rubin診断（R-hat統計量）の計算\n    \n    Parameters:\n    - chains: shape (n_chains, n_samples, n_params)\n    \n    Returns:\n    - r_hat: 各パラメータのR-hat値\n    \"\"\"\n    n_chains, n_samples, n_params = chains.shape\n    \n    # 各チェーンから後半のサンプルを使用\n    burnin = n_samples // 2\n    chains_clean = chains[:, burnin:, :]\n    n_samples_clean = chains_clean.shape[1]\n    \n    r_hat_values = []\n    \n    for param in range(n_params):\n        # 各チェーンの平均と分散\n        chain_means = np.mean(chains_clean[:, :, param], axis=1)\n        chain_vars = np.var(chains_clean[:, :, param], axis=1, ddof=1)\n        \n        # 全体平均\n        overall_mean = np.mean(chain_means)\n        \n        # Between-chain variance\n        B = n_samples_clean * np.var(chain_means, ddof=1)\n        \n        # Within-chain variance\n        W = np.mean(chain_vars)\n        \n        # Marginal posterior variance\n        var_hat = ((n_samples_clean - 1) * W + B) / n_samples_clean\n        \n        # R-hat統計量\n        if W > 0:\n            r_hat = np.sqrt(var_hat / W)\n        else:\n            r_hat = np.inf\n            \n        r_hat_values.append(r_hat)\n    \n    return np.array(r_hat_values)\n\n# 並列MCMCの実行\nprint(\"=== 並列MCMC実験 ===\")\nlog_prob_params = (mu_target, cov_target)  # プロセス間通信用のパラメータ\n\nparallel_samples, parallel_acceptance_rates = parallel_mcmc_chains(\n    log_prob_fn_params=log_prob_params,\n    n_chains=4,\n    n_samples=5000,\n    step_size=0.8\n)\n\nprint(f\"\\n平均受理率: {np.mean(parallel_acceptance_rates):.3f}\")\nprint(f\"受理率の標準偏差: {np.std(parallel_acceptance_rates):.3f}\")\n\n# Gelman-Rubin診断\nr_hat = gelman_rubin_diagnostic(parallel_samples)\nprint(f\"\\nGelman-Rubin診断 (R-hat):\")\nfor i, r in enumerate(r_hat):\n    status = \"✓\" if r < 1.1 else \"⚠\" if r < 1.2 else \"✗\"\n    print(f\"  Parameter {i+1}: {r:.4f} {status}\")\n\nconvergence_status = \"収束\" if np.all(r_hat < 1.1) else \"未収束\" if np.any(r_hat > 1.2) else \"境界\"\nprint(f\"収束判定: {convergence_status}\")\n\n# 全チェーンの結合\ncombined_parallel_samples = parallel_samples.reshape(-1, 2)\nprint(f\"\\n結合サンプル数: {len(combined_parallel_samples)}\")\n\n# 効率計算\nparallel_autocorr, parallel_eff = compute_efficiency_metrics(combined_parallel_samples)\nprint(f\"平均自己相関時間: {np.mean(parallel_autocorr):.2f}\")\nprint(f\"平均有効サンプルサイズ: {np.mean(parallel_eff):.1f}\")\nprint(f\"効率: {np.mean(parallel_eff)/len(combined_parallel_samples[500:]):.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.4 変分推論との比較\n\n変分推論（Variational Inference, VI）は、MCMCの代替手法として注目されています。近似的ですが高速な推論が可能です。\n\n### 変分推論の基本概念\n- **変分族**：近似事後分布の候補集合\n- **ELBO**：Evidence Lower Bound（最大化目標）\n- **KLダイバージェンス**：真の事後分布との距離\n- **平均場近似**：独立性仮定による簡単化"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleVariationalInference:\n    \"\"\"\n    簡易版変分推論（平均場近似）\n    \n    仮定：近似事後分布は多変量正規分布\n    \"\"\"\n    \n    def __init__(self, log_prob_fn, grad_log_prob_fn):\n        self.log_prob_fn = log_prob_fn\n        self.grad_log_prob_fn = grad_log_prob_fn\n    \n    def fit(self, initial_mu, initial_log_sigma, n_iterations=2000, learning_rate=0.01, n_samples=100):\n        \"\"\"\n        変分パラメータの最適化\n        \n        Parameters:\n        - initial_mu: 平均の初期値\n        - initial_log_sigma: 対数標準偏差の初期値\n        - n_iterations: 最適化イテレーション数\n        - learning_rate: 学習率\n        - n_samples: モンテカルロサンプル数（ELBO推定用）\n        \n        Returns:\n        - mu: 最適化された平均\n        - sigma: 最適化された標準偏差\n        - elbo_history: ELBO値の履歴\n        \"\"\"\n        \n        # 変分パラメータ\n        mu = initial_mu.copy()\n        log_sigma = initial_log_sigma.copy()\n        \n        elbo_history = []\n        \n        for iteration in range(n_iterations):\n            # 現在のパラメータから ELBO とその勾配を計算\n            elbo_val, grad_mu, grad_log_sigma = self._compute_elbo_and_gradients(\n                mu, log_sigma, n_samples\n            )\n            \n            # パラメータ更新（勾配上昇法）\n            mu += learning_rate * grad_mu\n            log_sigma += learning_rate * grad_log_sigma\n            \n            elbo_history.append(elbo_val)\n            \n            if (iteration + 1) % 500 == 0:\n                print(f\"Iteration {iteration+1}/{n_iterations}, ELBO: {elbo_val:.4f}\")\n        \n        sigma = np.exp(log_sigma)\n        return mu, sigma, np.array(elbo_history)\n    \n    def _compute_elbo_and_gradients(self, mu, log_sigma, n_samples):\n        \"\"\"\n        ELBO とその勾配のモンテカルロ推定\n        \"\"\"\n        sigma = np.exp(log_sigma)\n        \n        # q(θ) からのサンプリング\n        epsilon = np.random.normal(0, 1, (n_samples, len(mu)))\n        theta_samples = mu + sigma * epsilon\n        \n        # 各項の計算\n        log_p_values = np.array([self.log_prob_fn(theta) for theta in theta_samples])\n        log_q_values = -0.5 * np.sum(epsilon**2, axis=1) - 0.5 * len(mu) * np.log(2 * np.pi) - np.sum(log_sigma)\n        \n        # ELBO = E_q[log p(θ)] - E_q[log q(θ)]\n        elbo = np.mean(log_p_values - log_q_values)\n        \n        # 勾配の計算（再パラメータ化勾配）\n        grad_mu = np.mean([self.grad_log_prob_fn(theta) for theta in theta_samples], axis=0)\n        \n        # log_sigma に対する勾配\n        grad_log_sigma = np.zeros_like(log_sigma)\n        for i in range(len(log_sigma)):\n            # ∂ELBO/∂log_σ_i = E[∂log p(θ)/∂θ_i * ε_i] + 1\n            grad_contribution = np.mean([\n                self.grad_log_prob_fn(theta)[i] * epsilon[j, i] \n                for j, theta in enumerate(theta_samples)\n            ])\n            grad_log_sigma[i] = grad_contribution + 1  # エントロピー項の寄与\n        \n        return elbo, grad_mu, grad_log_sigma\n    \n    def sample(self, mu, sigma, n_samples):\n        \"\"\"\n        近似事後分布からのサンプリング\n        \"\"\"\n        return np.random.multivariate_normal(mu, np.diag(sigma**2), n_samples)\n\n# 変分推論の実行\nprint(\"=== 変分推論実験 ===\")\nvi = SimpleVariationalInference(\n    log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target),\n    grad_log_prob_fn=lambda x: multivariate_normal_grad_log_prob(x, mu_target, cov_target)\n)\n\n# 初期値の設定\ninitial_mu_vi = np.array([1.0, 1.0])  # 真の値から少しずらす\ninitial_log_sigma_vi = np.array([0.0, 0.0])  # log(1) = 0\n\nprint(\"変分推論最適化実行中...\")\nvi_mu, vi_sigma, elbo_history = vi.fit(\n    initial_mu=initial_mu_vi,\n    initial_log_sigma=initial_log_sigma_vi,\n    n_iterations=2000,\n    learning_rate=0.01,\n    n_samples=100\n)\n\nprint(f\"\\n変分推論結果:\")\nprint(f\"推定平均: [{vi_mu[0]:.3f}, {vi_mu[1]:.3f}]\")\nprint(f\"推定標準偏差: [{vi_sigma[0]:.3f}, {vi_sigma[1]:.3f}]\")\nprint(f\"真の平均: [{mu_target[0]:.3f}, {mu_target[1]:.3f}]\")\nprint(f\"真の標準偏差: [{np.sqrt(cov_target[0,0]):.3f}, {np.sqrt(cov_target[1,1]):.3f}]\")\n\n# 変分推論からのサンプリング\nvi_samples = vi.sample(vi_mu, vi_sigma, 5000)\nprint(f\"変分推論サンプル数: {len(vi_samples)}\")\n\n# 効率の評価（変分推論では相関がないため簡単）\nvi_sample_mean = np.mean(vi_samples, axis=0)\nvi_sample_cov = np.cov(vi_samples.T)\n\nprint(f\"\\nサンプル統計:\")\nprint(f\"サンプル平均: [{vi_sample_mean[0]:.3f}, {vi_sample_mean[1]:.3f}]\")\nprint(f\"サンプル共分散: [{vi_sample_cov[0,0]:.3f}, {vi_sample_cov[0,1]:.3f}]\")\nprint(f\"                [{vi_sample_cov[1,0]:.3f}, {vi_sample_cov[1,1]:.3f}]\")\n\n# 真の共分散との比較\nprint(f\"\\n真の共分散との差:\")\ncov_error = np.abs(vi_sample_cov - cov_target)\nprint(f\"共分散誤差: [{cov_error[0,0]:.3f}, {cov_error[0,1]:.3f}]\")\nprint(f\"            [{cov_error[1,0]:.3f}, {cov_error[1,1]:.3f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MCMC手法と変分推論の包括的比較\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# 準備：各手法のサンプルデータ\nmethods = ['Random Walk MH', 'Adaptive Metropolis', 'Ensemble Sampler', 'Parallel MCMC', 'Variational Inference']\nall_samples_comparison = [rwmh_samples, am_samples, ensemble_combined, combined_parallel_samples, vi_samples]\ncolors = ['red', 'orange', 'blue', 'green', 'purple']\n\n# 1. トレースプロット比較（MCMC手法のみ）\nmcmc_methods = methods[:4]\nmcmc_samples = all_samples_comparison[:4]\nmcmc_colors = colors[:4]\n\nfor i, (method, samples, color) in enumerate(zip(mcmc_methods, mcmc_samples, mcmc_colors)):\n    axes[0, 0].plot(samples[:1500, 0], alpha=0.7, label=method, color=color, linewidth=0.8)\naxes[0, 0].axvline(300, color='gray', linestyle='--', alpha=0.7, label='Burn-in')\naxes[0, 0].set_title('Trace Plot Comparison (X1)')\naxes[0, 0].set_xlabel('Iteration')\naxes[0, 0].set_ylabel('X1')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. 散布図比較（全手法）\nfor i, (method, samples, color) in enumerate(zip(methods, all_samples_comparison, colors)):\n    burnin = 300 if method != 'Variational Inference' else 0\n    clean_samples = samples[burnin:]\n    axes[0, 1].scatter(clean_samples[::20, 0], clean_samples[::20, 1], \n                      alpha=0.6, s=5, label=method, color=color)\n\n# 真の分布の等高線\nx1_range = np.linspace(-4, 4, 50)\nx2_range = np.linspace(-4, 4, 50)\nX1, X2 = np.meshgrid(x1_range, x2_range)\npos = np.dstack((X1, X2))\nrv = stats.multivariate_normal(mu_target, cov_target)\naxes[0, 1].contour(X1, X2, rv.pdf(pos), colors='black', alpha=0.8, linewidths=2)\naxes[0, 1].set_title('Sample Scatter Plot Comparison')\naxes[0, 1].set_xlabel('X1')\naxes[0, 1].set_ylabel('X2')\naxes[0, 1].legend()\naxes[0, 1].set_aspect('equal')\n\n# 3. ELBO収束プロット（変分推論）\naxes[0, 2].plot(elbo_history, color='purple', alpha=0.8)\naxes[0, 2].set_title('Variational Inference: ELBO Convergence')\naxes[0, 2].set_xlabel('Iteration')\naxes[0, 2].set_ylabel('ELBO')\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. 自己相関関数比較（MCMC手法のみ）\nfor i, (method, samples, color) in enumerate(zip(mcmc_methods, mcmc_samples, mcmc_colors)):\n    burnin = 300\n    clean_samples = samples[burnin:]\n    if len(clean_samples) > 100:\n        lags = min(50, len(clean_samples) // 10)\n        autocorr = acf(clean_samples[:, 0], nlags=lags, fft=True)\n        axes[1, 0].plot(autocorr, label=method, alpha=0.8, color=color)\n\naxes[1, 0].axhline(0, color='k', linestyle='--', alpha=0.5)\naxes[1, 0].axhline(0.05, color='gray', linestyle='--', alpha=0.5)\naxes[1, 0].set_title('Autocorrelation Comparison (X1)')\naxes[1, 0].set_xlabel('Lag')\naxes[1, 0].set_ylabel('ACF')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. マージナル分布比較\nx1_range_plot = np.linspace(-4, 4, 100)\ntrue_marginal = stats.norm.pdf(x1_range_plot, mu_target[0], np.sqrt(cov_target[0, 0]))\naxes[1, 1].plot(x1_range_plot, true_marginal, 'k-', linewidth=3, label='True', alpha=0.8)\n\nfor i, (method, samples, color) in enumerate(zip(methods, all_samples_comparison, colors)):\n    burnin = 300 if method != 'Variational Inference' else 0\n    clean_samples = samples[burnin:]\n    axes[1, 1].hist(clean_samples[:, 0], bins=30, density=True, alpha=0.5, \n                   color=color, label=method)\n\naxes[1, 1].set_title('Marginal Distribution X1')\naxes[1, 1].set_xlabel('X1')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\n# 6. 効率指標比較\nmethod_names_short = ['RWMH', 'AM', 'Ensemble', 'Parallel', 'VI']\nall_eff_comparison = []\n\nfor i, samples in enumerate(all_samples_comparison):\n    if i < 4:  # MCMC手法\n        autocorr, eff = compute_efficiency_metrics(samples)\n        all_eff_comparison.append(np.mean(eff))\n    else:  # 変分推論\n        all_eff_comparison.append(len(samples))  # 全サンプルが独立\n\naxes[1, 2].bar(method_names_short, all_eff_comparison, alpha=0.7, color=colors)\naxes[1, 2].set_title('Effective Sample Size Comparison')\naxes[1, 2].set_ylabel('Effective Sample Size')\naxes[1, 2].tick_params(axis='x', rotation=45)\naxes[1, 2].grid(True, alpha=0.3)\n\n# 7. 計算時間の概念的比較（実際の測定ではなく理論的）\n# 注：実際の実装では時間計測を行う\ncomputational_cost = [1.0, 1.2, 2.5, 0.8, 0.1]  # 相対的コスト（概念的）\nefficiency_per_cost = [eff/cost for eff, cost in zip(all_eff_comparison, computational_cost)]\n\naxes[2, 0].bar(method_names_short, computational_cost, alpha=0.7, color=colors)\naxes[2, 0].set_title('Relative Computational Cost')\naxes[2, 0].set_ylabel('Relative Cost')\naxes[2, 0].tick_params(axis='x', rotation=45)\naxes[2, 0].grid(True, alpha=0.3)\n\n# 8. 効率・コスト比\naxes[2, 1].bar(method_names_short, efficiency_per_cost, alpha=0.7, color=colors)\naxes[2, 1].set_title('Efficiency per Unit Cost')\naxes[2, 1].set_ylabel('ESS per Unit Cost')\naxes[2, 1].tick_params(axis='x', rotation=45)\naxes[2, 1].grid(True, alpha=0.3)\n\n# 9. 統計サマリー\naxes[2, 2].axis('off')\nsummary_text = \"Method Comparison Summary\\n\\n\"\n\n# 統計の計算\nfor i, (method, samples) in enumerate(zip(methods, all_samples_comparison)):\n    burnin = 300 if method != 'Variational Inference' else 0\n    clean_samples = samples[burnin:]\n    sample_mean = np.mean(clean_samples, axis=0)\n    \n    if i < 4:  # MCMC手法\n        autocorr, eff = compute_efficiency_metrics(samples)\n        mean_eff = np.mean(eff)\n        mean_tau = np.mean(autocorr)\n        summary_text += f\"{method}:\\n\"\n        summary_text += f\"  Mean: [{sample_mean[0]:.3f}, {sample_mean[1]:.3f}]\\n\"\n        summary_text += f\"  ESS: {mean_eff:.1f}, τ: {mean_tau:.2f}\\n\\n\"\n    else:  # 変分推論\n        summary_text += f\"{method}:\\n\"\n        summary_text += f\"  Mean: [{sample_mean[0]:.3f}, {sample_mean[1]:.3f}]\\n\"\n        summary_text += f\"  Independent samples: {len(clean_samples)}\\n\\n\"\n\naxes[2, 2].text(0.1, 0.9, summary_text, transform=axes[2, 2].transAxes, \n               fontsize=9, verticalalignment='top', fontfamily='monospace')\n\nplt.tight_layout()\nplt.show()\n\n# 定量的比較表\nprint(f\"\\n=== 全手法の定量的比較 ===\")\nprint(f\"{'Method':<20} {'Sample Mean X1':<15} {'Sample Mean X2':<15} {'Mean ESS':<12} {'Notes':<20}\")\nprint(\"-\" * 90)\n\nfor i, (method, samples) in enumerate(zip(methods, all_samples_comparison)):\n    burnin = 300 if method != 'Variational Inference' else 0\n    clean_samples = samples[burnin:]\n    sample_mean = np.mean(clean_samples, axis=0)\n    \n    if i < 4:  # MCMC手法\n        autocorr, eff = compute_efficiency_metrics(samples)\n        mean_eff = np.mean(eff)\n        notes = f\"τ={np.mean(autocorr):.2f}\"\n    else:  # 変分推論\n        mean_eff = len(clean_samples)\n        notes = \"Independent samples\"\n    \n    print(f\"{method:<20} {sample_mean[0]:<15.3f} {sample_mean[1]:<15.3f} {mean_eff:<12.1f} {notes:<20}\")\n\nprint(f\"\\n真の平均: [{mu_target[0]:.3f}, {mu_target[1]:.3f}]\")\n\n# 手法選択の指針\nprint(f\"\\n=== 手法選択の指針 ===\")\nselection_guide = {\n    \"高精度が必要\": \"Adaptive Metropolis, Ensemble Sampler - 真の分布により近い近似\",\n    \"高速計算が重要\": \"Variational Inference - 近似的だが非常に高速\",\n    \"並列処理可能\": \"Parallel MCMC, Ensemble Sampler - マルチコア活用\",\n    \"実装の簡単さ\": \"Random Walk MH - 理解・実装が容易\",\n    \"高次元問題\": \"Ensemble Sampler, Adaptive methods - スケーラビリティ\",\n    \"事後分布の探索\": \"MCMC methods - 分布の詳細な構造を探索\",\n    \"点推定で十分\": \"Variational Inference - 平均・分散の高速推定\"\n}\n\nfor criteria, recommendation in selection_guide.items():\n    print(f\"{criteria}: {recommendation}\")\n\nprint(f\"\\n=== MCMC vs 変分推論 ===\")\ncomparison_table = {\n    \"精度\": \"MCMC > VI (近似誤差なし vs 近似誤差あり)\",\n    \"速度\": \"VI >> MCMC (最適化 vs サンプリング)\",\n    \"メモリ使用量\": \"VI < MCMC (パラメータのみ vs 全サンプル)\",\n    \"診断の容易さ\": \"MCMC > VI (収束診断豊富 vs ELBO監視)\",\n    \"不確実性定量化\": \"MCMC > VI (完全な分布 vs パラメトリック近似)\",\n    \"実装の複雑さ\": \"MCMC ≈ VI (どちらも非自明)\"\n}\n\nfor aspect, comparison in comparison_table.items():\n    print(f\"{aspect}: {comparison}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.5 実用的なMCMCライブラリの紹介\n\n実際の研究・開発では、専用ライブラリを使用することが一般的です。主要なライブラリを紹介します。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 主要なMCMCライブラリの概要と使用例\nprint(\"=== 主要なMCMCライブラリ ===\")\nprint()\n\nlibraries_info = {\n    \"PyMC\": {\n        \"description\": \"Pythonで最も人気のあるベイズ統計ライブラリ\",\n        \"features\": [\n            \"直感的なモデル記述\",\n            \"自動微分によるHMC/NUTS\",\n            \"豊富な分布ライブラリ\",\n            \"変分推論サポート\",\n            \"優れた可視化機能\"\n        ],\n        \"use_cases\": \"階層モデル、時系列分析、機械学習\"\n    },\n    \"Stan (PyStan)\": {\n        \"description\": \"高性能なベイズ推論プラットフォーム\",\n        \"features\": [\n            \"専用言語による高速実行\",\n            \"最先端のNUTS実装\",\n            \"自動微分と最適化\",\n            \"R, Python, Julia等の多言語サポート\",\n            \"詳細な診断機能\"\n        ],\n        \"use_cases\": \"複雑なモデル、大規模データ、研究用途\"\n    },\n    \"TensorFlow Probability\": {\n        \"description\": \"TensorFlowベースの確率的プログラミング\",\n        \"features\": [\n            \"GPU加速サポート\",\n            \"深層学習との統合\",\n            \"変分推論と正規化フロー\",\n            \"大規模データ対応\",\n            \"分散計算サポート\"\n        ],\n        \"use_cases\": \"ベイズ深層学習、大規模推論\"\n    },\n    \"PyTorch/Pyro\": {\n        \"description\": \"PyTorchベースの確率的プログラミング\",\n        \"features\": [\n            \"動的計算グラフ\",\n            \"HMC/NUTSサポート\",\n            \"変分推論\",\n            \"ガイド付き推論\",\n            \"研究向け柔軟性\"\n        ],\n        \"use_cases\": \"研究開発、カスタムモデル\"\n    },\n    \"emcee\": {\n        \"description\": \"アンサンブルサンプラー専門ライブラリ\",\n        \"features\": [\n            \"Affine Invariant Ensemble Sampler\",\n            \"高次元問題に効果的\",\n            \"並列化サポート\",\n            \"シンプルなAPI\",\n            \"天体物理学で人気\"\n        ],\n        \"use_cases\": \"高次元パラメータ推定、物理学応用\"\n    },\n    \"scikit-learn\": {\n        \"description\": \"機械学習ライブラリ（MCMC機能も含む）\",\n        \"features\": [\n            \"ベイズ推論手法\",\n            \"ガウス過程\",\n            \"混合モデル\",\n            \"機械学習との統合\",\n            \"豊富なドキュメント\"\n        ],\n        \"use_cases\": \"機械学習、データサイエンス\"\n    }\n}\n\nfor lib_name, info in libraries_info.items():\n    print(f\"【{lib_name}】\")\n    print(f\"概要: {info['description']}\")\n    print(\"主な機能:\")\n    for feature in info['features']:\n        print(f\"  • {feature}\")\n    print(f\"適用分野: {info['use_cases']}\")\n    print()\n\n# ライブラリ選択の指針\nprint(\"=== ライブラリ選択の指針 ===\")\nprint()\n\nselection_guide = {\n    \"初学者・一般的な用途\": \"PyMC - 直感的で豊富なドキュメント\",\n    \"高性能・複雑なモデル\": \"Stan - 最適化された実装と豊富な診断\",\n    \"深層学習との統合\": \"TensorFlow Probability, Pyro - GPU活用と大規模データ\",\n    \"研究・カスタム実装\": \"Pyro, 自作実装 - 柔軟性と制御\",\n    \"高次元パラメータ推定\": \"emcee - 効率的なアンサンブル手法\",\n    \"機械学習応用\": \"scikit-learn - MLパイプラインとの統合\",\n    \"教育・理解\": \"自作実装 - アルゴリズムの詳細理解\"\n}\n\nfor use_case, recommendation in selection_guide.items():\n    print(f\"{use_case}: {recommendation}\")\n\nprint()\nprint(\"=== パフォーマンス比較の目安 ===\")\nprint()\n\nperformance_comparison = {\n    \"実行速度\": \"Stan > TFP ≈ Pyro > PyMC > emcee > scikit-learn > 自作実装\",\n    \"学習コスト\": \"PyMC < scikit-learn < emcee < TFP ≈ Pyro < Stan < 自作実装\",\n    \"柔軟性\": \"自作実装 > Pyro ≈ TFP > Stan ≈ PyMC > emcee > scikit-learn\",\n    \"診断機能\": \"Stan > PyMC > TFP ≈ Pyro > emcee > scikit-learn > 自作実装\",\n    \"コミュニティ\": \"PyMC ≈ Stan > scikit-learn > TFP > Pyro > emcee > 自作実装\"\n}\n\nfor aspect, ranking in performance_comparison.items():\n    print(f\"{aspect}: {ranking}\")\n\n# 実装例のテンプレート（疑似コード）\nprint()\nprint(\"=== 各ライブラリの実装例（疑似コード） ===\")\nprint()\n\nprint(\"【PyMC例】\")\npymc_example = \"\"\"\nimport pymc as pm\n\nwith pm.Model() as model:\n    # 事前分布\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=p)\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    \n    # 尤度\n    mu = pm.math.dot(X, beta)\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n    \n    # サンプリング\n    trace = pm.sample(2000, tune=1000, cores=4)\n\"\"\"\nprint(pymc_example)\n\nprint(\"【emcee例】\")\nemcee_example = \"\"\"\nimport emcee\n\ndef log_prob(theta, x, y, yerr):\n    # 対数確率密度の計算\n    model = theta[0] * x + theta[1]\n    return -0.5 * np.sum(((y - model) / yerr) ** 2)\n\n# 初期位置の設定\nndim, nwalkers = 2, 50\npos = np.random.randn(nwalkers, ndim)\n\n# サンプラーの作成と実行\nsampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(x, y, yerr))\nsampler.run_mcmc(pos, 5000, progress=True)\n\"\"\"\nprint(emcee_example)\n\nprint(\"【scikit-learn例（ベイズ推論）】\")\nsklearn_example = \"\"\"\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\n# ベイズ混合モデル\nbgm = BayesianGaussianMixture(n_components=3, random_state=42)\nbgm.fit(X)\nlabels = bgm.predict(X)\n\n# ガウス過程\ngpr = GaussianProcessRegressor(random_state=42)\ngpr.fit(X_train, y_train)\ny_pred, y_std = gpr.predict(X_test, return_std=True)\n\"\"\"\nprint(sklearn_example)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.6 演習問題\n\n### 問題1：適応的MCMCの改良\n遅延棄却適応的メトロポリス（DRAM）を実装してください。棄却時に縮小した提案を試行する機能を追加します。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 問題1: DRAM（遅延棄却適応的メトロポリス）の実装\nclass DelayedRejectionAdaptiveMetropolis:\n    \"\"\"\n    遅延棄却適応的メトロポリス法（DRAM）\n    \n    棄却時に縮小した提案を追加で試行する改良版適応的メトロポリス法\n    \n    実装のヒント：\n    1. 通常の適応的メトロポリス法をベースとする\n    2. 棄却時に、縮小ファクター（例：0.1）を適用した提案を試行\n    3. 2段階の受理確率を計算\n    4. 統計を適切に記録\n    \"\"\"\n    \n    def __init__(self, log_prob_fn, initial_cov_scale=0.1, adaptation_start=100, shrink_factor=0.1):\n        self.log_prob_fn = log_prob_fn\n        self.initial_cov_scale = initial_cov_scale\n        self.adaptation_start = adaptation_start\n        self.shrink_factor = shrink_factor  # 縮小ファクター\n        \n    def sample(self, initial_value, n_samples):\n        \"\"\"\n        DRAMサンプリングの実行\n        \n        Returns:\n        - samples: サンプル配列\n        - acceptance_stats: {'first_stage': rate, 'second_stage': rate, 'total': rate}\n        - final_cov: 最終共分散行列\n        \"\"\"\n        # ここに実装してください\n        # \n        # 実装手順：\n        # 1. 適応的メトロポリス法の基本構造を作成\n        # 2. 第1段階の提案が棄却された場合の処理を追加\n        # 3. 第2段階（縮小提案）の受理確率計算を実装\n        # 4. 統計の記録を適切に行う\n        \n        pass  # 学習者が実装\n\n# テスト実行用のコード\n# dram = DelayedRejectionAdaptiveMetropolis(\n#     log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target)\n# )\n# \n# dram_samples, dram_stats, dram_cov = dram.sample(\n#     initial_value=np.array([0.0, 0.0]),\n#     n_samples=5000\n# )\n# \n# print(f\"DRAM統計:\")\n# print(f\"第1段階受理率: {dram_stats['first_stage']:.3f}\")\n# print(f\"第2段階受理率: {dram_stats['second_stage']:.3f}\") \n# print(f\"総受理率: {dram_stats['total']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 問題2：アンサンブルサンプラーの拡張\nより多くのウォーカーを用いたアンサンブルサンプラーを実装し、並列化による高速化を実現してください。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 問題2: 並列化アンサンブルサンプラーの実装\nfrom concurrent.futures import ThreadPoolExecutor\nimport threading\n\nclass ParallelEnsembleSampler:\n    \"\"\"\n    並列化されたアフィン不変アンサンブルサンプラー\n    \n    実装のヒント：\n    1. ウォーカーのグループを作成し、各グループを並列処理\n    2. ThreadPoolExecutorまたはProcessPoolExecutorを使用\n    3. ウォーカー間の同期を適切に管理\n    4. メモリ効率を考慮した実装\n    \"\"\"\n    \n    def __init__(self, log_prob_fn, n_walkers, n_dim, n_threads=None):\n        self.log_prob_fn = log_prob_fn\n        self.n_walkers = n_walkers\n        self.n_dim = n_dim\n        self.n_threads = n_threads or min(4, n_walkers)\n        \n        if n_walkers < 2 * n_dim:\n            raise ValueError(f\"n_walkers ({n_walkers}) must be >= 2 * n_dim ({2 * n_dim})\")\n    \n    def sample_parallel(self, initial_positions, n_steps):\n        \"\"\"\n        並列アンサンブルサンプリングの実行\n        \n        実装課題：\n        1. ウォーカーを複数のグループに分割\n        2. 各グループを並列処理\n        3. ステップごとの同期処理\n        4. パフォーマンス測定\n        \n        Returns:\n        - samples: サンプル配列 (n_steps, n_walkers, n_dim)\n        - acceptance_rates: 各ウォーカーの受理率\n        - timing_info: 実行時間情報\n        \"\"\"\n        # ここに実装してください\n        pass  # 学習者が実装\n    \n    def _worker_group_step(self, walker_indices, current_positions, current_log_probs):\n        \"\"\"\n        ウォーカーグループの1ステップ実行（並列処理用）\n        \"\"\"\n        # 並列処理用のワーカー関数を実装\n        pass  # 学習者が実装\n\n# 使用例（疑似コード）\n# parallel_ensemble = ParallelEnsembleSampler(\n#     log_prob_fn=lambda x: multivariate_normal_log_prob(x, mu_target, cov_target),\n#     n_walkers=100,  # より多くのウォーカー\n#     n_dim=2,\n#     n_threads=4\n# )\n# \n# initial_pos = np.random.normal(0, 1, (100, 2))\n# samples, rates, timing = parallel_ensemble.sample_parallel(initial_pos, 2000)\n# \n# print(f\"並列化効果:\")\n# print(f\"実行時間: {timing['total_time']:.2f}秒\")\n# print(f\"スレッド効率: {timing['thread_efficiency']:.2%}\")\n# print(f\"平均受理率: {np.mean(rates):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## まとめ\n\nこの章では、基本的なMCMC手法を超えた高度な手法について学習しました：\n\n### 学習した手法\n\n1. **適応的MCMC**：\n   - サンプリング中のパラメータ自動調整\n   - 経験共分散による提案分布最適化\n   - 固定パラメータの限界を克服\n   - ユーザー介入の削減\n\n2. **アンサンブルサンプラー**：\n   - 複数ウォーカーの協調サンプリング\n   - アフィン不変性による頑健性\n   - 高次元問題での優位性\n   - 並列化による高速化\n\n3. **並列・分散MCMC**：\n   - 複数チェーンの並列実行\n   - Gelman-Rubin診断による収束確認\n   - スケーラビリティの向上\n   - 計算資源の効率活用\n\n4. **変分推論**：\n   - 近似的だが高速な推論\n   - ELBO最大化による最適化\n   - MCMCの補完的手法\n   - 大規模データへの対応\n\n### 性能比較の結果\n\n| 手法 | 精度 | 速度 | 並列性 | 実装難易度 | 適用範囲 |\n|------|------|------|--------|------------|----------|\n| Random Walk MH | 中 | 低 | 中 | 易 | 汎用 |\n| Adaptive Metropolis | 高 | 中 | 中 | 中 | 汎用 |\n| Ensemble Sampler | 高 | 中 | 高 | 中 | 高次元 |\n| Parallel MCMC | 高 | 高 | 最高 | 難 | 大規模 |\n| Variational Inference | 中 | 最高 | 高 | 難 | 近似OK |\n\n### 実用的な指針\n\n**手法選択の基準**：\n- **高精度重視** → Adaptive Metropolis, Ensemble Sampler\n- **高速計算重視** → Variational Inference\n- **並列処理活用** → Parallel MCMC, Ensemble Sampler\n- **高次元問題** → Ensemble Sampler, Adaptive methods\n- **実装簡単さ** → Random Walk MH\n- **大規模データ** → Variational Inference, Parallel MCMC\n\n**ライブラリ活用**：\n- 実際の応用では専用ライブラリの使用を強く推奨\n- PyMC, Stan, emcee, TensorFlow Probabilityが主要選択肢\n- 手法の理解のための自作実装も重要\n\n### MCMCの発展と展望\n\n**現在のトレンド**：\n- **GPU加速**：TensorFlow Probability, PyTorchによる高速化\n- **自動調整**：パラメータチューニングの自動化\n- **深層学習統合**：ベイズニューラルネットワーク\n- **近似手法**：変分推論や正規化フローとの融合\n\n**将来の方向性**：\n- **量子MCMC**：量子コンピュータを活用した新手法\n- **適応的アーキテクチャ**：問題に応じた手法自動選択\n- **分散システム**：クラウド環境での大規模並列処理\n- **リアルタイム推論**：ストリーミングデータへの対応\n\n### 学習のまとめ\n\nMCMCは統計学・機械学習の基盤技術として：\n\n1. **理論的基礎**：マルコフ連鎖理論と詳細釣り合い条件\n2. **基本手法**：Metropolis-Hastings, Gibbs sampling\n3. **高度な手法**：適応的MCMC, アンサンブルサンプラー, 並列MCMC\n4. **実用技術**：収束診断, 効率評価, ライブラリ活用\n5. **発展的手法**：変分推論との比較・統合\n\nこれらの知識を基に、実際の問題に適した手法を選択し、効率的なベイズ推論を実行できるようになることが目標です。\n\n**次のステップ**：\n- 実際のデータを用いた応用問題への挑戦\n- 専門ライブラリの習得（PyMC, Stan等）\n- 最新研究論文の追跡\n- 自身の研究・業務での実践適用\n\nMCMCは現在も活発に発展している分野であり、継続的な学習と実践を通じて、より深い理解と応用力を身につけることが重要です。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}