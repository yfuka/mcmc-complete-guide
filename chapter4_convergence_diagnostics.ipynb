{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 4: 収束診断と性能評価 - 実践的診断ガイド\n\n## 学習目標\n- MCMCの収束診断の重要性を理解し、適切な判断ができるようになる\n- 視覚的診断手法を習得し、問題の早期発見ができる\n- 数値的診断統計量を計算・解釈し、定量的評価ができる\n- 有効サンプルサイズと自己相関時間を理解し、効率を評価できる\n- 複数チェーンを用いた収束診断を実践し、信頼性を高められる\n- 実践的な診断手順を身につけ、自動化された診断システムを構築できる\n\n## なぜ収束診断が重要なのか？\n\nMCMCは「十分長く走らせれば必ず収束する」理論的保証がありますが、**現実の計算時間は有限**です。\n\n### 収束診断の失敗による深刻な結果\n\n1. **間違った推論**: 収束していないサンプルによる誤った結論\n2. **再現性の欠如**: 異なる実行で異なる結果\n3. **信頼性の失墜**: 研究や意思決定の信頼性低下\n\n### 本章のアプローチ\n\n理論だけでなく、**実際に遭遇する問題**と**実践的な解決策**に焦点を当てます：\n\n- 収束の失敗パターンの理解\n- 自動診断システムの構築\n- 問題発見から改善までの完全ワークフロー"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 収束診断の重要性\n",
    "\n",
    "MCMCサンプリングにおいて、以下の点を確認する必要があります：\n",
    "\n",
    "1. **収束**：マルコフ連鎖が定常分布に達したか？\n",
    "2. **混合**：状態空間を十分に探索しているか？\n",
    "3. **効率**：自己相関が十分に小さいか？\n",
    "\n",
    "### 収束の失敗例\n",
    "まず、収束しない場合の例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 収束が困難な分布の例：多峰性分布\n",
    "def multimodal_log_pdf(x):\n",
    "    \"\"\"多峰性分布（3つのピーク）\"\"\"\n",
    "    component1 = stats.norm.logpdf(x, -4, 0.5)\n",
    "    component2 = stats.norm.logpdf(x, 0, 0.5)\n",
    "    component3 = stats.norm.logpdf(x, 4, 0.5)\n",
    "    \n",
    "    # log(exp(c1) + exp(c2) + exp(c3))\n",
    "    max_comp = np.maximum(np.maximum(component1, component2), component3)\n",
    "    return max_comp + np.log(\n",
    "        np.exp(component1 - max_comp) + \n",
    "        np.exp(component2 - max_comp) + \n",
    "        np.exp(component3 - max_comp)\n",
    "    )\n",
    "\n",
    "def metropolis_hastings_simple(target_log_pdf, initial_value, n_samples, step_size=0.5):\n",
    "    \"\"\"シンプルなメトロポリス・ヘイスティングス実装\"\"\"\n",
    "    samples = np.zeros(n_samples)\n",
    "    current = initial_value\n",
    "    current_log_prob = target_log_pdf(current)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 提案\n",
    "        proposed = current + np.random.normal(0, step_size)\n",
    "        proposed_log_prob = target_log_pdf(proposed)\n",
    "        \n",
    "        # 受理確率\n",
    "        log_alpha = proposed_log_prob - current_log_prob\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # 受理/棄却\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposed\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = current\n",
    "    \n",
    "    return samples, n_accepted / n_samples\n",
    "\n",
    "# 異なる初期値と異なるステップサイズでサンプリング\n",
    "initial_values = [-4, 0, 4]\n",
    "step_sizes = [0.1, 1.0, 3.0]\n",
    "n_samples = 5000\n",
    "\n",
    "# 結果の保存\n",
    "sampling_results = {}\n",
    "\n",
    "for init_val in initial_values:\n",
    "    for step_size in step_sizes:\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            multimodal_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        sampling_results[key] = {\n",
    "            'samples': samples,\n",
    "            'acceptance_rate': acc_rate,\n",
    "            'initial_value': init_val,\n",
    "            'step_size': step_size\n",
    "        }\n",
    "\n",
    "print(\"収束の問題を示すサンプリング結果:\")\n",
    "for key, result in sampling_results.items():\n",
    "    mean_sample = np.mean(result['samples'][1000:])\n",
    "    print(f\"{key}: 受理率={result['acceptance_rate']:.3f}, 平均={mean_sample:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 収束問題の可視化\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "# 真の分布を計算\n",
    "x_range = np.linspace(-6, 6, 1000)\n",
    "true_density = np.exp(multimodal_log_pdf(x_range))\n",
    "\n",
    "for i, init_val in enumerate(initial_values):\n",
    "    for j, step_size in enumerate(step_sizes):\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples']\n",
    "        \n",
    "        # トレースプロット\n",
    "        axes[i, j].plot(samples[:2000], alpha=0.7, linewidth=0.8)\n",
    "        axes[i, j].axhline(init_val, color='red', linestyle='--', alpha=0.7, label='Initial')\n",
    "        axes[i, j].set_title(f'Init={init_val}, Step={step_size}\\nAcc={sampling_results[key][\"acceptance_rate\"]:.3f}')\n",
    "        axes[i, j].set_xlabel('Iteration')\n",
    "        axes[i, j].set_ylabel('Value')\n",
    "        axes[i, j].legend()\n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Convergence Issues in Multimodal Distribution', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ヒストグラム比較\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, step_size in enumerate(step_sizes):\n",
    "    for init_val in initial_values:\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples'][1000:]\n",
    "        axes[i].hist(samples, bins=50, alpha=0.5, density=True, \n",
    "                    label=f'Init={init_val}')\n",
    "    \n",
    "    axes[i].plot(x_range, true_density, 'k-', linewidth=2, label='True')\n",
    "    axes[i].set_title(f'Step Size = {step_size}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 視覚的診断手法\n",
    "\n",
    "### 4.2.1 トレースプロット\n",
    "パラメータの時系列変化を可視化し、収束と混合を視覚的に確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace_diagnostics(samples, parameter_names=None, title=\"Trace Diagnostics\"):\n",
    "    \"\"\"\n",
    "    包括的なトレース診断プロット\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 4, figsize=(16, 4*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # 1. 全トレースプロット\n",
    "        axes[i, 0].plot(param_samples, alpha=0.7, linewidth=0.8)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - Full Trace')\n",
    "        axes[i, 0].set_xlabel('Iteration')\n",
    "        axes[i, 0].set_ylabel('Value')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 前半・後半の比較\n",
    "        mid_point = len(param_samples) // 2\n",
    "        axes[i, 1].plot(param_samples[:mid_point], alpha=0.7, label='First half', color='blue')\n",
    "        axes[i, 1].plot(range(mid_point, len(param_samples)), \n",
    "                       param_samples[mid_point:], alpha=0.7, label='Second half', color='red')\n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - First vs Second Half')\n",
    "        axes[i, 1].set_xlabel('Iteration')\n",
    "        axes[i, 1].set_ylabel('Value')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ランニング平均\n",
    "        running_mean = np.cumsum(param_samples) / np.arange(1, len(param_samples) + 1)\n",
    "        axes[i, 2].plot(running_mean)\n",
    "        # 信頼区間も表示\n",
    "        running_var = np.cumsum((param_samples - running_mean)**2) / np.arange(1, len(param_samples) + 1)\n",
    "        running_se = np.sqrt(running_var / np.arange(1, len(param_samples) + 1))\n",
    "        axes[i, 2].fill_between(range(len(running_mean)), \n",
    "                               running_mean - 1.96*running_se,\n",
    "                               running_mean + 1.96*running_se, alpha=0.3)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Running Mean')\n",
    "        axes[i, 2].set_xlabel('Iteration')\n",
    "        axes[i, 2].set_ylabel('Running Mean')\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. 密度プロット（時間窓別）\n",
    "        n_windows = 4\n",
    "        window_size = len(param_samples) // n_windows\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_windows))\n",
    "        \n",
    "        for w in range(n_windows):\n",
    "            start_idx = w * window_size\n",
    "            end_idx = (w + 1) * window_size if w < n_windows - 1 else len(param_samples)\n",
    "            window_samples = param_samples[start_idx:end_idx]\n",
    "            \n",
    "            if len(window_samples) > 10:  # 十分なサンプルがある場合のみ\n",
    "                axes[i, 3].hist(window_samples, bins=30, alpha=0.5, density=True,\n",
    "                               color=colors[w], label=f'Window {w+1}')\n",
    "        \n",
    "        axes[i, 3].set_title(f'{parameter_names[i]} - Density by Time Window')\n",
    "        axes[i, 3].set_xlabel('Value')\n",
    "        axes[i, 3].set_ylabel('Density')\n",
    "        axes[i, 3].legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 良好な収束例（正規分布）\n",
    "good_samples, _ = metropolis_hastings_simple(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), 0, 5000, 1.0\n",
    ")\n",
    "\n",
    "plot_trace_diagnostics(good_samples, ['Normal Distribution'], \"Good Convergence Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 自己相関関数の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_analysis(samples, max_lags=200, parameter_names=None):\n",
    "    \"\"\"\n",
    "    自己相関関数の詳細分析\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 3, figsize=(15, 5*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    autocorr_results = {}\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # 自己相関の計算\n",
    "        lags = min(max_lags, len(param_samples) // 4)\n",
    "        autocorr = acf(param_samples, nlags=lags, fft=True)\n",
    "        \n",
    "        # 統合自己相関時間の計算\n",
    "        # τ_int = 1 + 2 * Σ(ρ(k)) for k where ρ(k) > 0\n",
    "        tau_int = 1.0\n",
    "        for k in range(1, len(autocorr)):\n",
    "            if autocorr[k] > 0.01:  # 閾値以上の相関がある間は加算\n",
    "                tau_int += 2 * autocorr[k]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # 有効サンプルサイズ\n",
    "        n_eff = len(param_samples) / (2 * tau_int + 1)\n",
    "        \n",
    "        autocorr_results[parameter_names[i]] = {\n",
    "            'tau_int': tau_int,\n",
    "            'n_eff': n_eff,\n",
    "            'autocorr': autocorr\n",
    "        }\n",
    "        \n",
    "        # 1. 自己相関関数プロット\n",
    "        axes[i, 0].plot(autocorr, 'b-', alpha=0.8)\n",
    "        axes[i, 0].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        axes[i, 0].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - ACF\\nτ_int = {tau_int:.1f}, N_eff = {n_eff:.0f}')\n",
    "        axes[i, 0].set_xlabel('Lag')\n",
    "        axes[i, 0].set_ylabel('Autocorrelation')\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 対数スケールでの自己相関\n",
    "        positive_autocorr = np.maximum(autocorr, 1e-10)\n",
    "        axes[i, 1].semilogy(positive_autocorr, 'b-', alpha=0.8)\n",
    "        \n",
    "        # 指数的減衰のフィッティング\n",
    "        try:\n",
    "            # 最初の数点を使って指数的減衰をフィット\n",
    "            fit_range = min(50, len(autocorr) // 2)\n",
    "            x_fit = np.arange(fit_range)\n",
    "            y_fit = autocorr[:fit_range]\n",
    "            \n",
    "            # 線形回帰で指数的減衰の係数を推定\n",
    "            mask = y_fit > 0.01\n",
    "            if np.sum(mask) > 5:\n",
    "                coeffs = np.polyfit(x_fit[mask], np.log(y_fit[mask]), 1)\n",
    "                exp_fit = np.exp(coeffs[1] + coeffs[0] * x_fit)\n",
    "                axes[i, 1].plot(x_fit, exp_fit, 'r--', alpha=0.7, \n",
    "                               label=f'Exp fit: τ = {-1/coeffs[0]:.1f}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - ACF (Log Scale)')\n",
    "        axes[i, 1].set_xlabel('Lag')\n",
    "        axes[i, 1].set_ylabel('Log Autocorrelation')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 薄化（thinning）の効果\n",
    "        thin_factors = [1, 2, 5, 10, 20]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(thin_factors)))\n",
    "        \n",
    "        for j, thin in enumerate(thin_factors):\n",
    "            if thin < len(param_samples) // 10:  # 十分なサンプルが残る場合のみ\n",
    "                thinned_samples = param_samples[::thin]\n",
    "                if len(thinned_samples) > 100:\n",
    "                    thin_lags = min(50, len(thinned_samples) // 4)\n",
    "                    thin_autocorr = acf(thinned_samples, nlags=thin_lags, fft=True)\n",
    "                    axes[i, 2].plot(thin_autocorr, color=colors[j], alpha=0.7, \n",
    "                                   label=f'Thin={thin}')\n",
    "        \n",
    "        axes[i, 2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Effect of Thinning')\n",
    "        axes[i, 2].set_xlabel('Lag')\n",
    "        axes[i, 2].set_ylabel('Autocorrelation')\n",
    "        axes[i, 2].legend()\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return autocorr_results\n",
    "\n",
    "# 自己相関分析の実行\n",
    "autocorr_results = plot_autocorrelation_analysis(good_samples, parameter_names=['Normal Distribution'])\n",
    "\n",
    "print(\"自己相関分析結果:\")\n",
    "for param, results in autocorr_results.items():\n",
    "    print(f\"{param}:\")\n",
    "    print(f\"  統合自己相関時間: {results['tau_int']:.2f}\")\n",
    "    print(f\"  有効サンプルサイズ: {results['n_eff']:.0f}\")\n",
    "    print(f\"  効率: {results['n_eff']/len(good_samples):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 数値的診断統計量\n",
    "\n",
    "### 4.3.1 Gelman-Rubin統計量（$\\hat{R}$）\n",
    "\n",
    "複数のチェーンを使って収束を診断する最も重要な統計量です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_rubin_diagnostic(chains, split_chains=True):\n",
    "    \"\"\"\n",
    "    Gelman-Rubin収束診断統計量の計算\n",
    "    \n",
    "    Parameters:\n",
    "    - chains: shape (n_chains, n_samples) または (n_chains, n_samples, n_params)\n",
    "    - split_chains: 各チェーンを前半・後半に分割するか\n",
    "    \n",
    "    Returns:\n",
    "    - R_hat: R-hat統計量\n",
    "    - n_eff: 有効サンプルサイズ\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    \n",
    "    if chains.ndim == 2:\n",
    "        # 単一パラメータの場合\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if split_chains:\n",
    "        # 各チェーンを前半・後半に分割\n",
    "        mid_point = n_samples // 2\n",
    "        first_half = chains[:, :mid_point, :]\n",
    "        second_half = chains[:, mid_point:, :]\n",
    "        chains_split = np.concatenate([first_half, second_half], axis=0)\n",
    "        n_chains *= 2\n",
    "        n_samples = mid_point\n",
    "        chains = chains_split\n",
    "    \n",
    "    R_hat = np.zeros(n_params)\n",
    "    n_eff = np.zeros(n_params)\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        # 各チェーンの平均と分散\n",
    "        chain_means = np.mean(chains[:, :, p], axis=1)\n",
    "        chain_vars = np.var(chains[:, :, p], axis=1, ddof=1)\n",
    "        \n",
    "        # 全体平均\n",
    "        overall_mean = np.mean(chain_means)\n",
    "        \n",
    "        # チェーン間分散 B\n",
    "        B = n_samples * np.var(chain_means, ddof=1)\n",
    "        \n",
    "        # チェーン内分散 W\n",
    "        W = np.mean(chain_vars)\n",
    "        \n",
    "        # 分散の推定値\n",
    "        var_plus = ((n_samples - 1) * W + B) / n_samples\n",
    "        \n",
    "        # R-hat統計量\n",
    "        R_hat[p] = np.sqrt(var_plus / W) if W > 0 else np.inf\n",
    "        \n",
    "        # 有効サンプルサイズの計算\n",
    "        # 各チェーンの自己相関を考慮\n",
    "        all_samples = chains[:, :, p].flatten()\n",
    "        \n",
    "        # 自己相関時間の推定\n",
    "        try:\n",
    "            autocorr = acf(all_samples, nlags=min(200, len(all_samples)//4), fft=True)\n",
    "            tau_int = 1.0\n",
    "            for k in range(1, len(autocorr)):\n",
    "                if autocorr[k] > 0.01:\n",
    "                    tau_int += 2 * autocorr[k]\n",
    "                else:\n",
    "                    break\n",
    "            n_eff[p] = len(all_samples) / (2 * tau_int + 1)\n",
    "        except:\n",
    "            n_eff[p] = len(all_samples) / 10  # 保守的な推定\n",
    "    \n",
    "    if n_params == 1:\n",
    "        return R_hat[0], n_eff[0]\n",
    "    else:\n",
    "        return R_hat, n_eff\n",
    "\n",
    "def run_multiple_chains(target_log_pdf, initial_values, n_samples, step_size=1.0):\n",
    "    \"\"\"\n",
    "    複数のチェーンを並列実行\n",
    "    \"\"\"\n",
    "    n_chains = len(initial_values)\n",
    "    chains = np.zeros((n_chains, n_samples))\n",
    "    acceptance_rates = np.zeros(n_chains)\n",
    "    \n",
    "    for i, init_val in enumerate(initial_values):\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            target_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        chains[i] = samples\n",
    "        acceptance_rates[i] = acc_rate\n",
    "    \n",
    "    return chains, acceptance_rates\n",
    "\n",
    "# 複数チェーンでの収束診断\n",
    "print(\"複数チェーンでのサンプリング実行中...\")\n",
    "\n",
    "# 正規分布での例\n",
    "initial_values_normal = [-2, -1, 0, 1, 2]\n",
    "chains_normal, acc_rates_normal = run_multiple_chains(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), \n",
    "    initial_values_normal, \n",
    "    3000, \n",
    "    step_size=1.0\n",
    ")\n",
    "\n",
    "# 多峰性分布での例\n",
    "initial_values_multimodal = [-4, -2, 0, 2, 4]\n",
    "chains_multimodal, acc_rates_multimodal = run_multiple_chains(\n",
    "    multimodal_log_pdf,\n",
    "    initial_values_multimodal,\n",
    "    3000,\n",
    "    step_size=2.0\n",
    ")\n",
    "\n",
    "# Gelman-Rubin診断の実行\n",
    "R_hat_normal, n_eff_normal = gelman_rubin_diagnostic(chains_normal)\n",
    "R_hat_multimodal, n_eff_multimodal = gelman_rubin_diagnostic(chains_multimodal)\n",
    "\n",
    "print(f\"\\n=== Gelman-Rubin診断結果 ===\")\n",
    "print(f\"正規分布:\")\n",
    "print(f\"  R-hat: {R_hat_normal:.4f}\")\n",
    "print(f\"  有効サンプルサイズ: {n_eff_normal:.0f}\")\n",
    "print(f\"  平均受理率: {np.mean(acc_rates_normal):.3f}\")\n",
    "\n",
    "print(f\"\\n多峰性分布:\")\n",
    "print(f\"  R-hat: {R_hat_multimodal:.4f}\")\n",
    "print(f\"  有効サンプルサイズ: {n_eff_multimodal:.0f}\")\n",
    "print(f\"  平均受理率: {np.mean(acc_rates_multimodal):.3f}\")\n",
    "\n",
    "print(f\"\\n判定基準:\")\n",
    "print(f\"  R-hat < 1.01: 収束良好\")\n",
    "print(f\"  1.01 ≤ R-hat < 1.1: 注意が必要\")\n",
    "print(f\"  R-hat ≥ 1.1: 収束不良\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数チェーンの可視化\n",
    "def plot_multiple_chains(chains, title=\"Multiple Chains Analysis\", parameter_name=\"Parameter\", true_mean=None):\n",
    "    \"\"\"\n",
    "    複数チェーンの詳細分析プロット\n",
    "    \"\"\"\n",
    "    n_chains, n_samples = chains.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_chains))\n",
    "    \n",
    "    # 1. 全チェーンのトレースプロット\n",
    "    for i in range(n_chains):\n",
    "        axes[0, 0].plot(chains[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 0].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 0].set_title('All Chains - Trace Plot')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. チェーン別のヒストグラム\n",
    "    for i in range(n_chains):\n",
    "        burnin = n_samples // 4\n",
    "        axes[0, 1].hist(chains[i, burnin:], bins=30, alpha=0.6, \n",
    "                       color=colors[i], density=True, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 1].axvline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 1].set_title('Distribution by Chain')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. ランニング平均の収束\n",
    "    for i in range(n_chains):\n",
    "        running_mean = np.cumsum(chains[i]) / np.arange(1, n_samples + 1)\n",
    "        axes[0, 2].plot(running_mean, alpha=0.7, color=colors[i], \n",
    "                       linewidth=1, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 2].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 2].set_title('Running Mean Convergence')\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Running Mean')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. チェーン間・チェーン内分散の変化\n",
    "    window_size = max(100, n_samples // 20)\n",
    "    n_windows = n_samples // window_size\n",
    "    \n",
    "    between_var = []\n",
    "    within_var = []\n",
    "    r_hat_evolution = []\n",
    "    \n",
    "    for w in range(1, n_windows + 1):\n",
    "        end_idx = w * window_size\n",
    "        window_chains = chains[:, :end_idx]\n",
    "        \n",
    "        if end_idx >= 200:  # 十分なサンプルがある場合のみ\n",
    "            try:\n",
    "                r_hat_w, _ = gelman_rubin_diagnostic(window_chains, split_chains=False)\n",
    "                r_hat_evolution.append(r_hat_w)\n",
    "                \n",
    "                # チェーン間・内分散の計算\n",
    "                chain_means = np.mean(window_chains, axis=1)\n",
    "                chain_vars = np.var(window_chains, axis=1, ddof=1)\n",
    "                B = end_idx * np.var(chain_means, ddof=1)\n",
    "                W = np.mean(chain_vars)\n",
    "                between_var.append(B)\n",
    "                within_var.append(W)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if len(r_hat_evolution) > 0:\n",
    "        axes[1, 0].plot(range(1, len(r_hat_evolution) + 1), r_hat_evolution, 'b-', linewidth=2)\n",
    "        axes[1, 0].axhline(1.0, color='green', linestyle='--', label='Perfect convergence')\n",
    "        axes[1, 0].axhline(1.01, color='orange', linestyle='--', label='Good convergence')\n",
    "        axes[1, 0].axhline(1.1, color='red', linestyle='--', label='Poor convergence')\n",
    "        axes[1, 0].set_title('R-hat Evolution')\n",
    "        axes[1, 0].set_xlabel('Window')\n",
    "        axes[1, 0].set_ylabel('R-hat')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. 自己相関比較\n",
    "    for i in range(min(3, n_chains)):  # 最初の3チェーンのみ表示\n",
    "        burnin = n_samples // 4\n",
    "        chain_data = chains[i, burnin:]\n",
    "        if len(chain_data) > 100:\n",
    "            lags = min(100, len(chain_data) // 4)\n",
    "            autocorr = acf(chain_data, nlags=lags, fft=True)\n",
    "            axes[1, 1].plot(autocorr, alpha=0.7, color=colors[i], \n",
    "                           label=f'Chain {i+1}')\n",
    "    axes[1, 1].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "    axes[1, 1].set_title('Autocorrelation Comparison')\n",
    "    axes[1, 1].set_xlabel('Lag')\n",
    "    axes[1, 1].set_ylabel('Autocorrelation')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Rank plot (chains の混合の確認)\n",
    "    all_samples = chains.flatten()\n",
    "    ranks = stats.rankdata(all_samples)\n",
    "    ranks = ranks.reshape(chains.shape)\n",
    "    \n",
    "    for i in range(n_chains):\n",
    "        axes[1, 2].plot(ranks[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    axes[1, 2].set_title('Rank Plot (Mixing Assessment)')\n",
    "    axes[1, 2].set_xlabel('Iteration')\n",
    "    axes[1, 2].set_ylabel('Rank')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 複数チェーンの分析\n",
    "print(\"正規分布の複数チェーン分析:\")\n",
    "plot_multiple_chains(chains_normal, \"Normal Distribution - Multiple Chains\", \"Value\", 0.0)\n",
    "\n",
    "print(\"多峰性分布の複数チェーン分析:\")\n",
    "plot_multiple_chains(chains_multimodal, \"Multimodal Distribution - Multiple Chains\", \"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 その他の診断統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_diagnostics(chains, parameter_names=None):\n",
    "    \"\"\"\n",
    "    包括的な診断統計量の計算\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    if chains.ndim == 2:\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        param_name = parameter_names[p]\n",
    "        param_chains = chains[:, :, p]\n",
    "        \n",
    "        # 基本統計\n",
    "        all_samples = param_chains.flatten()\n",
    "        burnin = n_samples // 4\n",
    "        clean_samples = param_chains[:, burnin:].flatten()\n",
    "        \n",
    "        # 1. Gelman-Rubin統計量\n",
    "        R_hat, n_eff = gelman_rubin_diagnostic(param_chains)\n",
    "        \n",
    "        # 2. Monte Carlo Standard Error (MCSE)\n",
    "        # MCSE = σ / sqrt(N_eff)\n",
    "        mcse = np.std(clean_samples, ddof=1) / np.sqrt(n_eff)\n",
    "        \n",
    "        # 3. 分位点のMCSE\n",
    "        def mcse_quantile(samples, q):\n",
    "            \"\"\"分位点のMonte Carlo標準誤差\"\"\"\n",
    "            n = len(samples)\n",
    "            p = q\n",
    "            # 正規近似を使用\n",
    "            return np.sqrt(p * (1 - p) / n) / stats.norm.pdf(stats.norm.ppf(p))\n",
    "        \n",
    "        mcse_q025 = mcse_quantile(clean_samples, 0.025)\n",
    "        mcse_q975 = mcse_quantile(clean_samples, 0.975)\n",
    "        \n",
    "        # 4. Geweke診断（前半と後半の比較）\n",
    "        first_10pct = int(0.1 * n_samples)\n",
    "        last_50pct = int(0.5 * n_samples)\n",
    "        \n",
    "        geweke_scores = []\n",
    "        for chain in range(n_chains):\n",
    "            first_part = param_chains[chain, :first_10pct]\n",
    "            last_part = param_chains[chain, -last_50pct:]\n",
    "            \n",
    "            if len(first_part) > 10 and len(last_part) > 10:\n",
    "                mean_diff = np.mean(first_part) - np.mean(last_part)\n",
    "                \n",
    "                # スペクトル密度による分散推定（簡易版）\n",
    "                var_first = np.var(first_part, ddof=1) / len(first_part)\n",
    "                var_last = np.var(last_part, ddof=1) / len(last_part)\n",
    "                \n",
    "                geweke_score = mean_diff / np.sqrt(var_first + var_last)\n",
    "                geweke_scores.append(geweke_score)\n",
    "        \n",
    "        geweke_pvalue = 2 * (1 - stats.norm.cdf(np.abs(np.mean(geweke_scores))))\n",
    "        \n",
    "        # 5. Heidelberger-Welch 検定（簡易版）\n",
    "        # 定常性の検定\n",
    "        hw_pvalues = []\n",
    "        for chain in range(n_chains):\n",
    "            chain_data = param_chains[chain]\n",
    "            \n",
    "            # チェーンを複数の窓に分割して平均の違いを検定\n",
    "            n_windows = 5\n",
    "            window_size = len(chain_data) // n_windows\n",
    "            window_means = []\n",
    "            \n",
    "            for w in range(n_windows):\n",
    "                start_idx = w * window_size\n",
    "                end_idx = (w + 1) * window_size\n",
    "                if end_idx <= len(chain_data):\n",
    "                    window_means.append(np.mean(chain_data[start_idx:end_idx]))\n",
    "            \n",
    "            if len(window_means) > 2:\n",
    "                # 一元配置分散分析\n",
    "                _, p_val = stats.f_oneway(*[chain_data[i*window_size:(i+1)*window_size] \n",
    "                                           for i in range(len(window_means))])\n",
    "                hw_pvalues.append(p_val)\n",
    "        \n",
    "        hw_pvalue = np.mean(hw_pvalues) if hw_pvalues else np.nan\n",
    "        \n",
    "        # 6. ESS (Effective Sample Size) の詳細計算\n",
    "        # バッチ法による推定も追加\n",
    "        batch_sizes = [10, 20, 50, 100]\n",
    "        batch_ess = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            if len(clean_samples) > batch_size * 10:\n",
    "                n_batches = len(clean_samples) // batch_size\n",
    "                batches = clean_samples[:n_batches * batch_size].reshape(n_batches, batch_size)\n",
    "                batch_means = np.mean(batches, axis=1)\n",
    "                \n",
    "                # バッチ平均の分散\n",
    "                batch_var = np.var(batch_means, ddof=1)\n",
    "                total_var = np.var(clean_samples, ddof=1)\n",
    "                \n",
    "                if batch_var > 0:\n",
    "                    ess_batch = len(clean_samples) * total_var / (batch_size * batch_var)\n",
    "                    batch_ess.append(ess_batch)\n",
    "        \n",
    "        avg_batch_ess = np.mean(batch_ess) if batch_ess else n_eff\n",
    "        \n",
    "        results[param_name] = {\n",
    "            'mean': np.mean(clean_samples),\n",
    "            'std': np.std(clean_samples, ddof=1),\n",
    "            'q025': np.percentile(clean_samples, 2.5),\n",
    "            'q975': np.percentile(clean_samples, 97.5),\n",
    "            'R_hat': R_hat,\n",
    "            'n_eff': n_eff,\n",
    "            'n_eff_batch': avg_batch_ess,\n",
    "            'mcse': mcse,\n",
    "            'mcse_q025': mcse_q025,\n",
    "            'mcse_q975': mcse_q975,\n",
    "            'geweke_score': np.mean(geweke_scores) if geweke_scores else np.nan,\n",
    "            'geweke_pvalue': geweke_pvalue,\n",
    "            'hw_pvalue': hw_pvalue,\n",
    "            'n_samples_total': len(all_samples),\n",
    "            'n_samples_clean': len(clean_samples),\n",
    "            'efficiency': n_eff / len(clean_samples)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_diagnostics_table(diagnostics):\n",
    "    \"\"\"\n",
    "    診断結果の表形式出力\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MCMC診断統計量サマリー\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # ヘッダー\n",
    "    header = f\"{'Parameter':<15} {'Mean':<8} {'Std':<8} {'R-hat':<8} {'N_eff':<8} {'MCSE':<8} {'Geweke':<8} {'Status':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for param_name, stats in diagnostics.items():\n",
    "        # 収束ステータスの判定\n",
    "        status = \"Good\"\n",
    "        if stats['R_hat'] > 1.1:\n",
    "            status = \"Poor\"\n",
    "        elif stats['R_hat'] > 1.01:\n",
    "            status = \"Caution\"\n",
    "        \n",
    "        if stats['n_eff'] < 100:\n",
    "            status += \"/Low ESS\"\n",
    "        \n",
    "        row = (f\"{param_name:<15} {stats['mean']:<8.3f} {stats['std']:<8.3f} \"\n",
    "               f\"{stats['R_hat']:<8.4f} {stats['n_eff']:<8.0f} {stats['mcse']:<8.4f} \"\n",
    "               f\"{stats['geweke_pvalue']:<8.3f} {status:<12}\")\n",
    "        print(row)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"判定基準:\")\n",
    "    print(\"  R-hat < 1.01: Good, 1.01-1.1: Caution, > 1.1: Poor\")\n",
    "    print(\"  Geweke: p-value > 0.05 で定常性仮説を棄却しない\")\n",
    "    print(\"  N_eff: 有効サンプルサイズ (目安: > 100)\")\n",
    "\n",
    "# 診断統計量の計算と表示\n",
    "print(\"診断統計量の計算中...\")\n",
    "\n",
    "# 正規分布の診断\n",
    "diagnostics_normal = comprehensive_diagnostics(chains_normal, ['Normal'])\n",
    "print(\"\\n正規分布の診断結果:\")\n",
    "print_diagnostics_table(diagnostics_normal)\n",
    "\n",
    "# 多峰性分布の診断\n",
    "diagnostics_multimodal = comprehensive_diagnostics(chains_multimodal, ['Multimodal'])\n",
    "print(\"\\n多峰性分布の診断結果:\")\n",
    "print_diagnostics_table(diagnostics_multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 実践的な診断手順\n",
    "\n",
    "実際のMCMC分析で推奨される診断手順をまとめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def mcmc_diagnostic_workflow(target_log_pdf, initial_values, n_samples, \n                           step_size=1.0, parameter_names=None, \n                           true_values=None, verbose=True):\n    \"\"\"\n    MCMC診断の標準ワークフロー - プロフェッショナル版\n    \n    この関数は実際の研究や業務で使用できるレベルの\n    包括的な診断システムを提供します。\n    \n    Returns:\n    - diagnostics: 診断結果の辞書\n    - recommendations: 推奨事項のリスト\n    - quality_score: 総合品質スコア (0-100)\n    \"\"\"\n    if verbose:\n        print(\"🔧 MCMC診断ワークフロー開始...\")\n        print(f\"   チェーン数: {len(initial_values)}\")\n        print(f\"   サンプル数: {n_samples}\")\n        print(f\"   ステップサイズ: {step_size}\")\n    \n    # Step 1: 複数チェーンの実行\n    if verbose:\n        print(\"\\n📊 Step 1: 複数チェーンでのサンプリング実行中...\")\n    \n    chains, acceptance_rates = run_multiple_chains(\n        target_log_pdf, initial_values, n_samples, step_size\n    )\n    \n    # Step 2: 基本統計の確認\n    avg_acceptance = np.mean(acceptance_rates)\n    if verbose:\n        print(f\"\\n📈 Step 2: 基本統計の確認\")\n        print(f\"   平均受理率: {avg_acceptance:.3f}\")\n        print(f\"   受理率範囲: {np.min(acceptance_rates):.3f} - {np.max(acceptance_rates):.3f}\")\n        \n        # 受理率の評価\n        if 0.4 <= avg_acceptance <= 0.6:\n            print(\"   ✅ 受理率は適切な範囲内です\")\n        elif 0.2 <= avg_acceptance < 0.4 or 0.6 < avg_acceptance <= 0.8:\n            print(\"   ⚠️  受理率がやや範囲外です\")\n        else:\n            print(\"   ❌ 受理率が推奨範囲外です\")\n    \n    # Step 3: 視覚的診断\n    if verbose:\n        print(\"\\n👁️  Step 3: 視覚的診断\")\n    \n    plot_multiple_chains(chains, \"Diagnostic Workflow - Visual Inspection\", \n                        parameter_name=\"Parameter\", \n                        true_mean=true_values[0] if true_values else None)\n    \n    # Step 4: 数値診断\n    if verbose:\n        print(\"\\n🔢 Step 4: 数値診断統計量の計算\")\n    \n    diagnostics = comprehensive_diagnostics(chains, parameter_names)\n    print_diagnostics_table(diagnostics)\n    \n    # Step 5: 品質スコアの計算\n    quality_scores = []\n    for param_name, stats in diagnostics.items():\n        param_score = 100  # 満点から減点方式\n        \n        # R-hat評価 (40点満点)\n        if stats['R_hat'] <= 1.01:\n            rhat_score = 40\n        elif stats['R_hat'] <= 1.05:\n            rhat_score = 30\n        elif stats['R_hat'] <= 1.1:\n            rhat_score = 15\n        else:\n            rhat_score = 0\n        \n        # ESS評価 (30点満点)\n        if stats['n_eff'] >= 400:\n            ess_score = 30\n        elif stats['n_eff'] >= 200:\n            ess_score = 20\n        elif stats['n_eff'] >= 100:\n            ess_score = 10\n        else:\n            ess_score = 0\n        \n        # 受理率評価 (20点満点)\n        if 0.4 <= avg_acceptance <= 0.6:\n            acc_score = 20\n        elif 0.2 <= avg_acceptance <= 0.8:\n            acc_score = 15\n        else:\n            acc_score = 5\n        \n        # MCSE評価 (10点満点)\n        relative_mcse = stats['mcse'] / stats['std']\n        if relative_mcse <= 0.05:\n            mcse_score = 10\n        elif relative_mcse <= 0.1:\n            mcse_score = 7\n        elif relative_mcse <= 0.2:\n            mcse_score = 3\n        else:\n            mcse_score = 0\n        \n        param_score = rhat_score + ess_score + acc_score + mcse_score\n        quality_scores.append(param_score)\n    \n    overall_quality = np.mean(quality_scores)\n    \n    # Step 6: 推奨事項の生成（詳細版）\n    recommendations = []\n    critical_issues = []\n    warnings = []\n    suggestions = []\n    \n    for param_name, stats in diagnostics.items():\n        # 致命的な問題\n        if stats['R_hat'] > 1.1:\n            critical_issues.append(f\"{param_name}: 収束していません (R-hat = {stats['R_hat']:.4f})\")\n        \n        if stats['n_eff'] < 50:\n            critical_issues.append(f\"{param_name}: 有効サンプルサイズが極端に小さいです ({stats['n_eff']:.0f})\")\n        \n        # 警告レベル\n        if 1.01 < stats['R_hat'] <= 1.1:\n            warnings.append(f\"{param_name}: R-hat がやや高いです ({stats['R_hat']:.4f})\")\n        \n        if 50 <= stats['n_eff'] < 100:\n            warnings.append(f\"{param_name}: 有効サンプルサイズが小さいです ({stats['n_eff']:.0f})\")\n        \n        relative_mcse = stats['mcse'] / stats['std']\n        if relative_mcse > 0.1:\n            warnings.append(f\"{param_name}: Monte Carlo誤差が大きいです ({relative_mcse:.3f})\")\n        \n        # 改善提案\n        if stats['efficiency'] < 0.1:\n            suggestions.append(f\"{param_name}: 効率が低いです。より良いパラメタリゼーションを検討してください\")\n        \n        if not np.isnan(stats['geweke_pvalue']) and stats['geweke_pvalue'] < 0.05:\n            suggestions.append(f\"{param_name}: 非定常性が検出されました。バーンイン期間を延長してください\")\n    \n    # 受理率に関する推奨事項\n    if avg_acceptance < 0.2:\n        critical_issues.append(f\"受理率が極端に低いです ({avg_acceptance:.3f}). ステップサイズを大幅に縮小してください\")\n    elif avg_acceptance < 0.4:\n        warnings.append(f\"受理率が低いです ({avg_acceptance:.3f}). ステップサイズを縮小することを検討してください\")\n    elif avg_acceptance > 0.8:\n        warnings.append(f\"受理率が高すぎます ({avg_acceptance:.3f}). ステップサイズを拡大してください\")\n    \n    # 総合的な推奨事項\n    total_samples = len(initial_values) * n_samples\n    total_eff = sum([stats['n_eff'] for stats in diagnostics.values()])\n    overall_efficiency = total_eff / total_samples\n    \n    if overall_efficiency < 0.05:\n        critical_issues.append(f\"全体的な効率が極端に低いです ({overall_efficiency:.3f}). アルゴリズムの根本的な見直しが必要です\")\n    elif overall_efficiency < 0.1:\n        warnings.append(f\"全体的な効率が低いです ({overall_efficiency:.3f}). パラメタリゼーションの改善を検討してください\")\n    \n    # 推奨事項の統合\n    recommendations = critical_issues + warnings + suggestions\n    \n    # Step 7: 結果サマリーの表示\n    if verbose:\n        print(f\"\\n🎯 Step 7: 総合評価\")\n        print(f\"   品質スコア: {overall_quality:.1f}/100\")\n        \n        if overall_quality >= 80:\n            print(\"   ✅ 優秀: 診断結果は非常に良好です\")\n        elif overall_quality >= 60:\n            print(\"   ⚠️  良好: 軽微な改善の余地があります\")\n        elif overall_quality >= 40:\n            print(\"   ⚠️  注意: いくつかの問題が検出されました\")\n        else:\n            print(\"   ❌ 問題: 重大な問題が検出されました\")\n        \n        print(f\"   全体効率: {overall_efficiency:.3f}\")\n        \n        if recommendations:\n            print(f\"\\n📝 改善提案 ({len(recommendations)}件):\")\n            for i, rec in enumerate(recommendations, 1):\n                if rec in critical_issues:\n                    print(f\"   🔴 {i}. {rec}\")\n                elif rec in warnings:\n                    print(f\"   🟡 {i}. {rec}\")\n                else:\n                    print(f\"   🔵 {i}. {rec}\")\n        else:\n            print(\"\\n🎉 素晴らしい! 改善提案はありません。\")\n    \n    return {\n        'chains': chains,\n        'acceptance_rates': acceptance_rates,\n        'diagnostics': diagnostics,\n        'recommendations': recommendations,\n        'critical_issues': critical_issues,\n        'warnings': warnings,\n        'suggestions': suggestions,\n        'overall_efficiency': overall_efficiency,\n        'quality_score': overall_quality,\n        'quality_breakdown': {\n            'r_hat_component': np.mean([40 if stats['R_hat'] <= 1.01 else 0 for stats in diagnostics.values()]),\n            'ess_component': np.mean([30 if stats['n_eff'] >= 400 else 0 for stats in diagnostics.values()]),\n            'acceptance_component': 20 if 0.4 <= avg_acceptance <= 0.6 else 0,\n            'mcse_component': np.mean([10 if stats['mcse']/stats['std'] <= 0.05 else 0 for stats in diagnostics.values()])\n        }\n    }\n\n# プロフェッショナル診断ワークフローの実行例\nprint(\"=\" * 80)\nprint(\"🚀 プロフェッショナル診断ワークフロー実行例\")\nprint(\"=\" * 80)\n\nprint(\"\\n=== 正規分布での高品質診断 ===\")\nresults_normal_pro = mcmc_diagnostic_workflow(\n    target_log_pdf=lambda x: stats.norm.logpdf(x, 0, 1),\n    initial_values=[-2, -1, 0, 1, 2],\n    n_samples=3000,\n    step_size=1.0,\n    parameter_names=['Normal'],\n    true_values=[0.0]\n)\n\nprint(f\"\\n📊 品質内訳:\")\nfor component, score in results_normal_pro['quality_breakdown'].items():\n    print(f\"   {component}: {score:.1f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"=== 多峰性分布での診断（チャレンジングケース） ===\")\nresults_multimodal_pro = mcmc_diagnostic_workflow(\n    target_log_pdf=multimodal_log_pdf,\n    initial_values=[-4, -2, 0, 2, 4],\n    n_samples=3000,\n    step_size=1.5,\n    parameter_names=['Multimodal']\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 演習問題\n",
    "\n",
    "### 問題1：収束診断の実践\n",
    "以下の困難な分布に対してMCMCを実行し、収束診断を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題1: 困難な分布からのサンプリング\n",
    "def challenging_log_pdf(x):\n",
    "    \"\"\"\n",
    "    挑戦的な分布：高い相関を持つ2変量分布\n",
    "    \"\"\"\n",
    "    if len(x) != 2:\n",
    "        return -np.inf\n",
    "    \n",
    "    # 非常に細長い分布（高相関）\n",
    "    mu = np.array([0, 0])\n",
    "    cov = np.array([[1, 0.99], [0.99, 1]])\n",
    "    \n",
    "    diff = x - mu\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        log_det = 2 * np.sum(np.log(np.diag(chol)))\n",
    "        solve = np.linalg.solve(chol, diff)\n",
    "        mahalanobis_sq = np.sum(solve**2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf\n",
    "    \n",
    "    return -0.5 * (2 * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n",
    "\n",
    "# ここで2変量MHサンプリングを実装し、診断してください\n",
    "# ヒント：\n",
    "# 1. 複数の初期値から開始\n",
    "# 2. 異なるステップサイズを試す\n",
    "# 3. 収束診断を実行\n",
    "# 4. 問題点を特定し、改善策を提案\n",
    "\n",
    "pass  # 学習者が実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題2：適応的診断\n",
    "リアルタイムで収束を監視し、自動的に停止条件を判定するシステムを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題2: 適応的収束診断\n",
    "def adaptive_mcmc_with_diagnostics(target_log_pdf, initial_values, \n",
    "                                 max_samples=10000, check_interval=500,\n",
    "                                 r_hat_threshold=1.01, min_eff_samples=1000):\n",
    "    \"\"\"\n",
    "    適応的収束診断付きMCMC\n",
    "    \n",
    "    Parameters:\n",
    "    - target_log_pdf: 目標分布\n",
    "    - initial_values: 初期値のリスト\n",
    "    - max_samples: 最大サンプル数\n",
    "    - check_interval: 診断をチェックする間隔\n",
    "    - r_hat_threshold: R-hat の収束閾値\n",
    "    - min_eff_samples: 最小有効サンプル数\n",
    "    \n",
    "    実装のヒント:\n",
    "    1. check_interval ごとに R-hat と有効サンプルサイズを計算\n",
    "    2. 収束条件を満たしたら早期停止\n",
    "    3. 収束の履歴をプロット\n",
    "    \"\"\"\n",
    "    # ここに実装してください\n",
    "    pass  # 学習者が実装\n",
    "\n",
    "# テスト\n",
    "# result = adaptive_mcmc_with_diagnostics(\n",
    "#     lambda x: stats.norm.logpdf(x, 0, 1),\n",
    "#     [-2, -1, 0, 1, 2]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## まとめ：実践的MCMC診断のマスターガイド\n\nこの章では、MCMCの収束診断と性能評価について包括的に学習しました：\n\n### 🔍 重要な診断手法\n\n1. **視覚的診断**：\n   - **トレースプロット**：混合と収束の即座の確認\n   - **ランニング平均**：収束の安定性を時系列で追跡\n   - **密度プロット**：時間窓別の分布変化を検出\n\n2. **数値診断**：\n   - **Gelman-Rubin統計量（R-hat）**：< 1.01 で良好、< 1.1で許容範囲\n   - **有効サンプルサイズ（ESS）**：> 400 が理想、最低 > 100\n   - **Monte Carlo標準誤差（MCSE）**：推定精度の定量的指標\n   - **Geweke診断**：定常性の統計的検定\n\n3. **自己相関分析**：\n   - **統合自己相関時間**：アルゴリズム効率の根本指標\n   - **薄化（thinning）**の効果検証\n   - **指数的減衰**フィッティングによる時定数推定\n\n### 📋 実践的な診断手順（チェックリスト）\n\n#### フェーズ1: サンプリング設計\n- [ ] **複数チェーン準備**：最低3チェーン、理想的には8-10チェーン\n- [ ] **初期値分散**：目標分布の異なる領域から開始\n- [ ] **十分なサンプル数**：最低2000、複雑な問題では10000+\n\n#### フェーズ2: リアルタイム監視\n- [ ] **定期的チェック**：500-1000サンプルごとに診断実行\n- [ ] **早期警告**：R-hat > 1.2で即座にアラート\n- [ ] **トレンド監視**：収束指標の時間変化を追跡\n\n#### フェーズ3: 総合診断\n- [ ] **視覚的検査**：全トレースプロットを目視確認\n- [ ] **数値診断**：R-hat < 1.01、ESS > 400を確認\n- [ ] **自己相関分析**：τ_int < 10（目安）\n- [ ] **分布比較**：理論値または他手法との一致確認\n\n#### フェーズ4: 品質保証\n- [ ] **感度分析**：異なる初期値・パラメータで結果の一貫性確認\n- [ ] **再現性テスト**：同じ設定で複数回実行し結果比較\n- [ ] **文書化**：診断結果と判断根拠を記録\n\n### ⚠️ 一般的な問題と対策マトリックス\n\n| 症状 | 原因 | 対策 | 緊急度 |\n|------|------|------|--------|\n| R-hat > 1.1 | 収束不良 | サンプル数増加、初期値変更 | 🔴 高 |\n| ESS < 100 | 効率低下 | ステップサイズ調整、アルゴリズム変更 | 🟡 中 |\n| 高い自己相関 | 混合不良 | 薄化、ブロック更新 | 🟡 中 |\n| モード間移動なし | 多峰性 | 並列焼きなまし、長いチェーン | 🔴 高 |\n| 受理率 < 20% | ステップ過大 | ステップサイズ縮小 | 🟢 低 |\n| 受理率 > 70% | ステップ過小 | ステップサイズ拡大 | 🟢 低 |\n\n### 🎯 ベストプラクティス\n\n#### 設計段階\n- **保守的設定**：問題の複雑さを過小評価しない\n- **スケーラビリティ**：パラメータ数増加に備えた設計\n- **自動化**：人手による判断ミスを防ぐシステム化\n\n#### 実行段階\n- **段階的増加**：短いテストから始めて段階的に長くする\n- **並列実行**：計算資源を最大活用\n- **中間保存**：長時間計算の中断リスク対策\n\n#### 評価段階\n- **複数指標**：単一指標に依存しない総合判断\n- **可視化重視**：数値だけでなく必ず視覚的確認\n- **外部検証**：理論値や他手法との比較\n\n### 🚀 次のステップ\n\n収束診断をマスターしたあなたは、MCMCを**安全で信頼性の高いツール**として使えるようになりました。\n\n次の章では、これまで学んだ全てのMCMC手法を**実際のベイズ推論問題**に適用し、データサイエンスの実践的課題を解決していきます：\n\n- 実データでの階層ベイズモデル\n- 欠損データのあるモデリング\n- 予測と不確実性の定量化\n- モデル選択と比較\n\n**重要**：完璧な診断は存在しません。しかし、**体系的で一貫した診断手順**により、MCMCの信頼性を大幅に向上させることができます。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}