{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 4: åæŸè¨ºæ–­ã¨æ€§èƒ½è©•ä¾¡ - å®Ÿè·µçš„è¨ºæ–­ã‚¬ã‚¤ãƒ‰\n\n## å­¦ç¿’ç›®æ¨™\n- MCMCã®åæŸè¨ºæ–­ã®é‡è¦æ€§ã‚’ç†è§£ã—ã€é©åˆ‡ãªåˆ¤æ–­ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹\n- è¦–è¦šçš„è¨ºæ–­æ‰‹æ³•ã‚’ç¿’å¾—ã—ã€å•é¡Œã®æ—©æœŸç™ºè¦‹ãŒã§ãã‚‹\n- æ•°å€¤çš„è¨ºæ–­çµ±è¨ˆé‡ã‚’è¨ˆç®—ãƒ»è§£é‡ˆã—ã€å®šé‡çš„è©•ä¾¡ãŒã§ãã‚‹\n- æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¨è‡ªå·±ç›¸é–¢æ™‚é–“ã‚’ç†è§£ã—ã€åŠ¹ç‡ã‚’è©•ä¾¡ã§ãã‚‹\n- è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã‚’ç”¨ã„ãŸåæŸè¨ºæ–­ã‚’å®Ÿè·µã—ã€ä¿¡é ¼æ€§ã‚’é«˜ã‚ã‚‰ã‚Œã‚‹\n- å®Ÿè·µçš„ãªè¨ºæ–­æ‰‹é †ã‚’èº«ã«ã¤ã‘ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹\n\n## ãªãœåæŸè¨ºæ–­ãŒé‡è¦ãªã®ã‹ï¼Ÿ\n\nMCMCã¯ã€Œååˆ†é•·ãèµ°ã‚‰ã›ã‚Œã°å¿…ãšåæŸã™ã‚‹ã€ç†è«–çš„ä¿è¨¼ãŒã‚ã‚Šã¾ã™ãŒã€**ç¾å®Ÿã®è¨ˆç®—æ™‚é–“ã¯æœ‰é™**ã§ã™ã€‚\n\n### åæŸè¨ºæ–­ã®å¤±æ•—ã«ã‚ˆã‚‹æ·±åˆ»ãªçµæœ\n\n1. **é–“é•ã£ãŸæ¨è«–**: åæŸã—ã¦ã„ãªã„ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ˆã‚‹èª¤ã£ãŸçµè«–\n2. **å†ç¾æ€§ã®æ¬ å¦‚**: ç•°ãªã‚‹å®Ÿè¡Œã§ç•°ãªã‚‹çµæœ\n3. **ä¿¡é ¼æ€§ã®å¤±å¢œ**: ç ”ç©¶ã‚„æ„æ€æ±ºå®šã®ä¿¡é ¼æ€§ä½ä¸‹\n\n### æœ¬ç« ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n\nç†è«–ã ã‘ã§ãªãã€**å®Ÿéš›ã«é­é‡ã™ã‚‹å•é¡Œ**ã¨**å®Ÿè·µçš„ãªè§£æ±ºç­–**ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ï¼š\n\n- åæŸã®å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£\n- è‡ªå‹•è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰\n- å•é¡Œç™ºè¦‹ã‹ã‚‰æ”¹å–„ã¾ã§ã®å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 åæŸè¨ºæ–­ã®é‡è¦æ€§\n",
    "\n",
    "MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "\n",
    "1. **åæŸ**ï¼šãƒãƒ«ã‚³ãƒ•é€£é–ãŒå®šå¸¸åˆ†å¸ƒã«é”ã—ãŸã‹ï¼Ÿ\n",
    "2. **æ··åˆ**ï¼šçŠ¶æ…‹ç©ºé–“ã‚’ååˆ†ã«æ¢ç´¢ã—ã¦ã„ã‚‹ã‹ï¼Ÿ\n",
    "3. **åŠ¹ç‡**ï¼šè‡ªå·±ç›¸é–¢ãŒååˆ†ã«å°ã•ã„ã‹ï¼Ÿ\n",
    "\n",
    "### åæŸã®å¤±æ•—ä¾‹\n",
    "ã¾ãšã€åæŸã—ãªã„å ´åˆã®ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åæŸãŒå›°é›£ãªåˆ†å¸ƒã®ä¾‹ï¼šå¤šå³°æ€§åˆ†å¸ƒ\n",
    "def multimodal_log_pdf(x):\n",
    "    \"\"\"å¤šå³°æ€§åˆ†å¸ƒï¼ˆ3ã¤ã®ãƒ”ãƒ¼ã‚¯ï¼‰\"\"\"\n",
    "    component1 = stats.norm.logpdf(x, -4, 0.5)\n",
    "    component2 = stats.norm.logpdf(x, 0, 0.5)\n",
    "    component3 = stats.norm.logpdf(x, 4, 0.5)\n",
    "    \n",
    "    # log(exp(c1) + exp(c2) + exp(c3))\n",
    "    max_comp = np.maximum(np.maximum(component1, component2), component3)\n",
    "    return max_comp + np.log(\n",
    "        np.exp(component1 - max_comp) + \n",
    "        np.exp(component2 - max_comp) + \n",
    "        np.exp(component3 - max_comp)\n",
    "    )\n",
    "\n",
    "def metropolis_hastings_simple(target_log_pdf, initial_value, n_samples, step_size=0.5):\n",
    "    \"\"\"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹å®Ÿè£…\"\"\"\n",
    "    samples = np.zeros(n_samples)\n",
    "    current = initial_value\n",
    "    current_log_prob = target_log_pdf(current)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # ææ¡ˆ\n",
    "        proposed = current + np.random.normal(0, step_size)\n",
    "        proposed_log_prob = target_log_pdf(proposed)\n",
    "        \n",
    "        # å—ç†ç¢ºç‡\n",
    "        log_alpha = proposed_log_prob - current_log_prob\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # å—ç†/æ£„å´\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposed\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = current\n",
    "    \n",
    "    return samples, n_accepted / n_samples\n",
    "\n",
    "# ç•°ãªã‚‹åˆæœŸå€¤ã¨ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "initial_values = [-4, 0, 4]\n",
    "step_sizes = [0.1, 1.0, 3.0]\n",
    "n_samples = 5000\n",
    "\n",
    "# çµæœã®ä¿å­˜\n",
    "sampling_results = {}\n",
    "\n",
    "for init_val in initial_values:\n",
    "    for step_size in step_sizes:\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            multimodal_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        sampling_results[key] = {\n",
    "            'samples': samples,\n",
    "            'acceptance_rate': acc_rate,\n",
    "            'initial_value': init_val,\n",
    "            'step_size': step_size\n",
    "        }\n",
    "\n",
    "print(\"åæŸã®å•é¡Œã‚’ç¤ºã™ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœ:\")\n",
    "for key, result in sampling_results.items():\n",
    "    mean_sample = np.mean(result['samples'][1000:])\n",
    "    print(f\"{key}: å—ç†ç‡={result['acceptance_rate']:.3f}, å¹³å‡={mean_sample:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åæŸå•é¡Œã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "# çœŸã®åˆ†å¸ƒã‚’è¨ˆç®—\n",
    "x_range = np.linspace(-6, 6, 1000)\n",
    "true_density = np.exp(multimodal_log_pdf(x_range))\n",
    "\n",
    "for i, init_val in enumerate(initial_values):\n",
    "    for j, step_size in enumerate(step_sizes):\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples']\n",
    "        \n",
    "        # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        axes[i, j].plot(samples[:2000], alpha=0.7, linewidth=0.8)\n",
    "        axes[i, j].axhline(init_val, color='red', linestyle='--', alpha=0.7, label='Initial')\n",
    "        axes[i, j].set_title(f'Init={init_val}, Step={step_size}\\nAcc={sampling_results[key][\"acceptance_rate\"]:.3f}')\n",
    "        axes[i, j].set_xlabel('Iteration')\n",
    "        axes[i, j].set_ylabel('Value')\n",
    "        axes[i, j].legend()\n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Convergence Issues in Multimodal Distribution', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, step_size in enumerate(step_sizes):\n",
    "    for init_val in initial_values:\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples'][1000:]\n",
    "        axes[i].hist(samples, bins=50, alpha=0.5, density=True, \n",
    "                    label=f'Init={init_val}')\n",
    "    \n",
    "    axes[i].plot(x_range, true_density, 'k-', linewidth=2, label='True')\n",
    "    axes[i].set_title(f'Step Size = {step_size}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 è¦–è¦šçš„è¨ºæ–­æ‰‹æ³•\n",
    "\n",
    "### 4.2.1 ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’å¯è¦–åŒ–ã—ã€åæŸã¨æ··åˆã‚’è¦–è¦šçš„ã«ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace_diagnostics(samples, parameter_names=None, title=\"Trace Diagnostics\"):\n",
    "    \"\"\"\n",
    "    åŒ…æ‹¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 4, figsize=(16, 4*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # 1. å…¨ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        axes[i, 0].plot(param_samples, alpha=0.7, linewidth=0.8)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - Full Trace')\n",
    "        axes[i, 0].set_xlabel('Iteration')\n",
    "        axes[i, 0].set_ylabel('Value')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. å‰åŠãƒ»å¾ŒåŠã®æ¯”è¼ƒ\n",
    "        mid_point = len(param_samples) // 2\n",
    "        axes[i, 1].plot(param_samples[:mid_point], alpha=0.7, label='First half', color='blue')\n",
    "        axes[i, 1].plot(range(mid_point, len(param_samples)), \n",
    "                       param_samples[mid_point:], alpha=0.7, label='Second half', color='red')\n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - First vs Second Half')\n",
    "        axes[i, 1].set_xlabel('Iteration')\n",
    "        axes[i, 1].set_ylabel('Value')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å¹³å‡\n",
    "        running_mean = np.cumsum(param_samples) / np.arange(1, len(param_samples) + 1)\n",
    "        axes[i, 2].plot(running_mean)\n",
    "        # ä¿¡é ¼åŒºé–“ã‚‚è¡¨ç¤º\n",
    "        running_var = np.cumsum((param_samples - running_mean)**2) / np.arange(1, len(param_samples) + 1)\n",
    "        running_se = np.sqrt(running_var / np.arange(1, len(param_samples) + 1))\n",
    "        axes[i, 2].fill_between(range(len(running_mean)), \n",
    "                               running_mean - 1.96*running_se,\n",
    "                               running_mean + 1.96*running_se, alpha=0.3)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Running Mean')\n",
    "        axes[i, 2].set_xlabel('Iteration')\n",
    "        axes[i, 2].set_ylabel('Running Mean')\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. å¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ™‚é–“çª“åˆ¥ï¼‰\n",
    "        n_windows = 4\n",
    "        window_size = len(param_samples) // n_windows\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_windows))\n",
    "        \n",
    "        for w in range(n_windows):\n",
    "            start_idx = w * window_size\n",
    "            end_idx = (w + 1) * window_size if w < n_windows - 1 else len(param_samples)\n",
    "            window_samples = param_samples[start_idx:end_idx]\n",
    "            \n",
    "            if len(window_samples) > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿\n",
    "                axes[i, 3].hist(window_samples, bins=30, alpha=0.5, density=True,\n",
    "                               color=colors[w], label=f'Window {w+1}')\n",
    "        \n",
    "        axes[i, 3].set_title(f'{parameter_names[i]} - Density by Time Window')\n",
    "        axes[i, 3].set_xlabel('Value')\n",
    "        axes[i, 3].set_ylabel('Density')\n",
    "        axes[i, 3].legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è‰¯å¥½ãªåæŸä¾‹ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰\n",
    "good_samples, _ = metropolis_hastings_simple(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), 0, 5000, 1.0\n",
    ")\n",
    "\n",
    "plot_trace_diagnostics(good_samples, ['Normal Distribution'], \"Good Convergence Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 è‡ªå·±ç›¸é–¢é–¢æ•°ã®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_analysis(samples, max_lags=200, parameter_names=None):\n",
    "    \"\"\"\n",
    "    è‡ªå·±ç›¸é–¢é–¢æ•°ã®è©³ç´°åˆ†æ\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 3, figsize=(15, 5*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    autocorr_results = {}\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # è‡ªå·±ç›¸é–¢ã®è¨ˆç®—\n",
    "        lags = min(max_lags, len(param_samples) // 4)\n",
    "        autocorr = acf(param_samples, nlags=lags, fft=True)\n",
    "        \n",
    "        # çµ±åˆè‡ªå·±ç›¸é–¢æ™‚é–“ã®è¨ˆç®—\n",
    "        # Ï„_int = 1 + 2 * Î£(Ï(k)) for k where Ï(k) > 0\n",
    "        tau_int = 1.0\n",
    "        for k in range(1, len(autocorr)):\n",
    "            if autocorr[k] > 0.01:  # é–¾å€¤ä»¥ä¸Šã®ç›¸é–¢ãŒã‚ã‚‹é–“ã¯åŠ ç®—\n",
    "                tau_int += 2 * autocorr[k]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º\n",
    "        n_eff = len(param_samples) / (2 * tau_int + 1)\n",
    "        \n",
    "        autocorr_results[parameter_names[i]] = {\n",
    "            'tau_int': tau_int,\n",
    "            'n_eff': n_eff,\n",
    "            'autocorr': autocorr\n",
    "        }\n",
    "        \n",
    "        # 1. è‡ªå·±ç›¸é–¢é–¢æ•°ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        axes[i, 0].plot(autocorr, 'b-', alpha=0.8)\n",
    "        axes[i, 0].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        axes[i, 0].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - ACF\\nÏ„_int = {tau_int:.1f}, N_eff = {n_eff:.0f}')\n",
    "        axes[i, 0].set_xlabel('Lag')\n",
    "        axes[i, 0].set_ylabel('Autocorrelation')\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è‡ªå·±ç›¸é–¢\n",
    "        positive_autocorr = np.maximum(autocorr, 1e-10)\n",
    "        axes[i, 1].semilogy(positive_autocorr, 'b-', alpha=0.8)\n",
    "        \n",
    "        # æŒ‡æ•°çš„æ¸›è¡°ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°\n",
    "        try:\n",
    "            # æœ€åˆã®æ•°ç‚¹ã‚’ä½¿ã£ã¦æŒ‡æ•°çš„æ¸›è¡°ã‚’ãƒ•ã‚£ãƒƒãƒˆ\n",
    "            fit_range = min(50, len(autocorr) // 2)\n",
    "            x_fit = np.arange(fit_range)\n",
    "            y_fit = autocorr[:fit_range]\n",
    "            \n",
    "            # ç·šå½¢å›å¸°ã§æŒ‡æ•°çš„æ¸›è¡°ã®ä¿‚æ•°ã‚’æ¨å®š\n",
    "            mask = y_fit > 0.01\n",
    "            if np.sum(mask) > 5:\n",
    "                coeffs = np.polyfit(x_fit[mask], np.log(y_fit[mask]), 1)\n",
    "                exp_fit = np.exp(coeffs[1] + coeffs[0] * x_fit)\n",
    "                axes[i, 1].plot(x_fit, exp_fit, 'r--', alpha=0.7, \n",
    "                               label=f'Exp fit: Ï„ = {-1/coeffs[0]:.1f}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - ACF (Log Scale)')\n",
    "        axes[i, 1].set_xlabel('Lag')\n",
    "        axes[i, 1].set_ylabel('Log Autocorrelation')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. è–„åŒ–ï¼ˆthinningï¼‰ã®åŠ¹æœ\n",
    "        thin_factors = [1, 2, 5, 10, 20]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(thin_factors)))\n",
    "        \n",
    "        for j, thin in enumerate(thin_factors):\n",
    "            if thin < len(param_samples) // 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒæ®‹ã‚‹å ´åˆã®ã¿\n",
    "                thinned_samples = param_samples[::thin]\n",
    "                if len(thinned_samples) > 100:\n",
    "                    thin_lags = min(50, len(thinned_samples) // 4)\n",
    "                    thin_autocorr = acf(thinned_samples, nlags=thin_lags, fft=True)\n",
    "                    axes[i, 2].plot(thin_autocorr, color=colors[j], alpha=0.7, \n",
    "                                   label=f'Thin={thin}')\n",
    "        \n",
    "        axes[i, 2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Effect of Thinning')\n",
    "        axes[i, 2].set_xlabel('Lag')\n",
    "        axes[i, 2].set_ylabel('Autocorrelation')\n",
    "        axes[i, 2].legend()\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return autocorr_results\n",
    "\n",
    "# è‡ªå·±ç›¸é–¢åˆ†æã®å®Ÿè¡Œ\n",
    "autocorr_results = plot_autocorrelation_analysis(good_samples, parameter_names=['Normal Distribution'])\n",
    "\n",
    "print(\"è‡ªå·±ç›¸é–¢åˆ†æçµæœ:\")\n",
    "for param, results in autocorr_results.items():\n",
    "    print(f\"{param}:\")\n",
    "    print(f\"  çµ±åˆè‡ªå·±ç›¸é–¢æ™‚é–“: {results['tau_int']:.2f}\")\n",
    "    print(f\"  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {results['n_eff']:.0f}\")\n",
    "    print(f\"  åŠ¹ç‡: {results['n_eff']/len(good_samples):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 æ•°å€¤çš„è¨ºæ–­çµ±è¨ˆé‡\n",
    "\n",
    "### 4.3.1 Gelman-Rubinçµ±è¨ˆé‡ï¼ˆ$\\hat{R}$ï¼‰\n",
    "\n",
    "è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’ä½¿ã£ã¦åæŸã‚’è¨ºæ–­ã™ã‚‹æœ€ã‚‚é‡è¦ãªçµ±è¨ˆé‡ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_rubin_diagnostic(chains, split_chains=True):\n",
    "    \"\"\"\n",
    "    Gelman-RubinåæŸè¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—\n",
    "    \n",
    "    Parameters:\n",
    "    - chains: shape (n_chains, n_samples) ã¾ãŸã¯ (n_chains, n_samples, n_params)\n",
    "    - split_chains: å„ãƒã‚§ãƒ¼ãƒ³ã‚’å‰åŠãƒ»å¾ŒåŠã«åˆ†å‰²ã™ã‚‹ã‹\n",
    "    \n",
    "    Returns:\n",
    "    - R_hat: R-hatçµ±è¨ˆé‡\n",
    "    - n_eff: æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    \n",
    "    if chains.ndim == 2:\n",
    "        # å˜ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´åˆ\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if split_chains:\n",
    "        # å„ãƒã‚§ãƒ¼ãƒ³ã‚’å‰åŠãƒ»å¾ŒåŠã«åˆ†å‰²\n",
    "        mid_point = n_samples // 2\n",
    "        first_half = chains[:, :mid_point, :]\n",
    "        second_half = chains[:, mid_point:, :]\n",
    "        chains_split = np.concatenate([first_half, second_half], axis=0)\n",
    "        n_chains *= 2\n",
    "        n_samples = mid_point\n",
    "        chains = chains_split\n",
    "    \n",
    "    R_hat = np.zeros(n_params)\n",
    "    n_eff = np.zeros(n_params)\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        # å„ãƒã‚§ãƒ¼ãƒ³ã®å¹³å‡ã¨åˆ†æ•£\n",
    "        chain_means = np.mean(chains[:, :, p], axis=1)\n",
    "        chain_vars = np.var(chains[:, :, p], axis=1, ddof=1)\n",
    "        \n",
    "        # å…¨ä½“å¹³å‡\n",
    "        overall_mean = np.mean(chain_means)\n",
    "        \n",
    "        # ãƒã‚§ãƒ¼ãƒ³é–“åˆ†æ•£ B\n",
    "        B = n_samples * np.var(chain_means, ddof=1)\n",
    "        \n",
    "        # ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ W\n",
    "        W = np.mean(chain_vars)\n",
    "        \n",
    "        # åˆ†æ•£ã®æ¨å®šå€¤\n",
    "        var_plus = ((n_samples - 1) * W + B) / n_samples\n",
    "        \n",
    "        # R-hatçµ±è¨ˆé‡\n",
    "        R_hat[p] = np.sqrt(var_plus / W) if W > 0 else np.inf\n",
    "        \n",
    "        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—\n",
    "        # å„ãƒã‚§ãƒ¼ãƒ³ã®è‡ªå·±ç›¸é–¢ã‚’è€ƒæ…®\n",
    "        all_samples = chains[:, :, p].flatten()\n",
    "        \n",
    "        # è‡ªå·±ç›¸é–¢æ™‚é–“ã®æ¨å®š\n",
    "        try:\n",
    "            autocorr = acf(all_samples, nlags=min(200, len(all_samples)//4), fft=True)\n",
    "            tau_int = 1.0\n",
    "            for k in range(1, len(autocorr)):\n",
    "                if autocorr[k] > 0.01:\n",
    "                    tau_int += 2 * autocorr[k]\n",
    "                else:\n",
    "                    break\n",
    "            n_eff[p] = len(all_samples) / (2 * tau_int + 1)\n",
    "        except:\n",
    "            n_eff[p] = len(all_samples) / 10  # ä¿å®ˆçš„ãªæ¨å®š\n",
    "    \n",
    "    if n_params == 1:\n",
    "        return R_hat[0], n_eff[0]\n",
    "    else:\n",
    "        return R_hat, n_eff\n",
    "\n",
    "def run_multiple_chains(target_log_pdf, initial_values, n_samples, step_size=1.0):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’ä¸¦åˆ—å®Ÿè¡Œ\n",
    "    \"\"\"\n",
    "    n_chains = len(initial_values)\n",
    "    chains = np.zeros((n_chains, n_samples))\n",
    "    acceptance_rates = np.zeros(n_chains)\n",
    "    \n",
    "    for i, init_val in enumerate(initial_values):\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            target_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        chains[i] = samples\n",
    "        acceptance_rates[i] = acc_rate\n",
    "    \n",
    "    return chains, acceptance_rates\n",
    "\n",
    "# è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã§ã®åæŸè¨ºæ–­\n",
    "print(\"è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "\n",
    "# æ­£è¦åˆ†å¸ƒã§ã®ä¾‹\n",
    "initial_values_normal = [-2, -1, 0, 1, 2]\n",
    "chains_normal, acc_rates_normal = run_multiple_chains(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), \n",
    "    initial_values_normal, \n",
    "    3000, \n",
    "    step_size=1.0\n",
    ")\n",
    "\n",
    "# å¤šå³°æ€§åˆ†å¸ƒã§ã®ä¾‹\n",
    "initial_values_multimodal = [-4, -2, 0, 2, 4]\n",
    "chains_multimodal, acc_rates_multimodal = run_multiple_chains(\n",
    "    multimodal_log_pdf,\n",
    "    initial_values_multimodal,\n",
    "    3000,\n",
    "    step_size=2.0\n",
    ")\n",
    "\n",
    "# Gelman-Rubinè¨ºæ–­ã®å®Ÿè¡Œ\n",
    "R_hat_normal, n_eff_normal = gelman_rubin_diagnostic(chains_normal)\n",
    "R_hat_multimodal, n_eff_multimodal = gelman_rubin_diagnostic(chains_multimodal)\n",
    "\n",
    "print(f\"\\n=== Gelman-Rubinè¨ºæ–­çµæœ ===\")\n",
    "print(f\"æ­£è¦åˆ†å¸ƒ:\")\n",
    "print(f\"  R-hat: {R_hat_normal:.4f}\")\n",
    "print(f\"  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {n_eff_normal:.0f}\")\n",
    "print(f\"  å¹³å‡å—ç†ç‡: {np.mean(acc_rates_normal):.3f}\")\n",
    "\n",
    "print(f\"\\nå¤šå³°æ€§åˆ†å¸ƒ:\")\n",
    "print(f\"  R-hat: {R_hat_multimodal:.4f}\")\n",
    "print(f\"  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {n_eff_multimodal:.0f}\")\n",
    "print(f\"  å¹³å‡å—ç†ç‡: {np.mean(acc_rates_multimodal):.3f}\")\n",
    "\n",
    "print(f\"\\nåˆ¤å®šåŸºæº–:\")\n",
    "print(f\"  R-hat < 1.01: åæŸè‰¯å¥½\")\n",
    "print(f\"  1.01 â‰¤ R-hat < 1.1: æ³¨æ„ãŒå¿…è¦\")\n",
    "print(f\"  R-hat â‰¥ 1.1: åæŸä¸è‰¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®å¯è¦–åŒ–\n",
    "def plot_multiple_chains(chains, title=\"Multiple Chains Analysis\", parameter_name=\"Parameter\", true_mean=None):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®è©³ç´°åˆ†æãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    n_chains, n_samples = chains.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_chains))\n",
    "    \n",
    "    # 1. å…¨ãƒã‚§ãƒ¼ãƒ³ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    for i in range(n_chains):\n",
    "        axes[0, 0].plot(chains[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 0].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 0].set_title('All Chains - Trace Plot')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ãƒã‚§ãƒ¼ãƒ³åˆ¥ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n",
    "    for i in range(n_chains):\n",
    "        burnin = n_samples // 4\n",
    "        axes[0, 1].hist(chains[i, burnin:], bins=30, alpha=0.6, \n",
    "                       color=colors[i], density=True, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 1].axvline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 1].set_title('Distribution by Chain')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å¹³å‡ã®åæŸ\n",
    "    for i in range(n_chains):\n",
    "        running_mean = np.cumsum(chains[i]) / np.arange(1, n_samples + 1)\n",
    "        axes[0, 2].plot(running_mean, alpha=0.7, color=colors[i], \n",
    "                       linewidth=1, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 2].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 2].set_title('Running Mean Convergence')\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Running Mean')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ãƒã‚§ãƒ¼ãƒ³é–“ãƒ»ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¤‰åŒ–\n",
    "    window_size = max(100, n_samples // 20)\n",
    "    n_windows = n_samples // window_size\n",
    "    \n",
    "    between_var = []\n",
    "    within_var = []\n",
    "    r_hat_evolution = []\n",
    "    \n",
    "    for w in range(1, n_windows + 1):\n",
    "        end_idx = w * window_size\n",
    "        window_chains = chains[:, :end_idx]\n",
    "        \n",
    "        if end_idx >= 200:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿\n",
    "            try:\n",
    "                r_hat_w, _ = gelman_rubin_diagnostic(window_chains, split_chains=False)\n",
    "                r_hat_evolution.append(r_hat_w)\n",
    "                \n",
    "                # ãƒã‚§ãƒ¼ãƒ³é–“ãƒ»å†…åˆ†æ•£ã®è¨ˆç®—\n",
    "                chain_means = np.mean(window_chains, axis=1)\n",
    "                chain_vars = np.var(window_chains, axis=1, ddof=1)\n",
    "                B = end_idx * np.var(chain_means, ddof=1)\n",
    "                W = np.mean(chain_vars)\n",
    "                between_var.append(B)\n",
    "                within_var.append(W)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if len(r_hat_evolution) > 0:\n",
    "        axes[1, 0].plot(range(1, len(r_hat_evolution) + 1), r_hat_evolution, 'b-', linewidth=2)\n",
    "        axes[1, 0].axhline(1.0, color='green', linestyle='--', label='Perfect convergence')\n",
    "        axes[1, 0].axhline(1.01, color='orange', linestyle='--', label='Good convergence')\n",
    "        axes[1, 0].axhline(1.1, color='red', linestyle='--', label='Poor convergence')\n",
    "        axes[1, 0].set_title('R-hat Evolution')\n",
    "        axes[1, 0].set_xlabel('Window')\n",
    "        axes[1, 0].set_ylabel('R-hat')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. è‡ªå·±ç›¸é–¢æ¯”è¼ƒ\n",
    "    for i in range(min(3, n_chains)):  # æœ€åˆã®3ãƒã‚§ãƒ¼ãƒ³ã®ã¿è¡¨ç¤º\n",
    "        burnin = n_samples // 4\n",
    "        chain_data = chains[i, burnin:]\n",
    "        if len(chain_data) > 100:\n",
    "            lags = min(100, len(chain_data) // 4)\n",
    "            autocorr = acf(chain_data, nlags=lags, fft=True)\n",
    "            axes[1, 1].plot(autocorr, alpha=0.7, color=colors[i], \n",
    "                           label=f'Chain {i+1}')\n",
    "    axes[1, 1].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "    axes[1, 1].set_title('Autocorrelation Comparison')\n",
    "    axes[1, 1].set_xlabel('Lag')\n",
    "    axes[1, 1].set_ylabel('Autocorrelation')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Rank plot (chains ã®æ··åˆã®ç¢ºèª)\n",
    "    all_samples = chains.flatten()\n",
    "    ranks = stats.rankdata(all_samples)\n",
    "    ranks = ranks.reshape(chains.shape)\n",
    "    \n",
    "    for i in range(n_chains):\n",
    "        axes[1, 2].plot(ranks[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    axes[1, 2].set_title('Rank Plot (Mixing Assessment)')\n",
    "    axes[1, 2].set_xlabel('Iteration')\n",
    "    axes[1, 2].set_ylabel('Rank')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®åˆ†æ\n",
    "print(\"æ­£è¦åˆ†å¸ƒã®è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³åˆ†æ:\")\n",
    "plot_multiple_chains(chains_normal, \"Normal Distribution - Multiple Chains\", \"Value\", 0.0)\n",
    "\n",
    "print(\"å¤šå³°æ€§åˆ†å¸ƒã®è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³åˆ†æ:\")\n",
    "plot_multiple_chains(chains_multimodal, \"Multimodal Distribution - Multiple Chains\", \"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 ãã®ä»–ã®è¨ºæ–­çµ±è¨ˆé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_diagnostics(chains, parameter_names=None):\n",
    "    \"\"\"\n",
    "    åŒ…æ‹¬çš„ãªè¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    if chains.ndim == 2:\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        param_name = parameter_names[p]\n",
    "        param_chains = chains[:, :, p]\n",
    "        \n",
    "        # åŸºæœ¬çµ±è¨ˆ\n",
    "        all_samples = param_chains.flatten()\n",
    "        burnin = n_samples // 4\n",
    "        clean_samples = param_chains[:, burnin:].flatten()\n",
    "        \n",
    "        # 1. Gelman-Rubinçµ±è¨ˆé‡\n",
    "        R_hat, n_eff = gelman_rubin_diagnostic(param_chains)\n",
    "        \n",
    "        # 2. Monte Carlo Standard Error (MCSE)\n",
    "        # MCSE = Ïƒ / sqrt(N_eff)\n",
    "        mcse = np.std(clean_samples, ddof=1) / np.sqrt(n_eff)\n",
    "        \n",
    "        # 3. åˆ†ä½ç‚¹ã®MCSE\n",
    "        def mcse_quantile(samples, q):\n",
    "            \"\"\"åˆ†ä½ç‚¹ã®Monte Carloæ¨™æº–èª¤å·®\"\"\"\n",
    "            n = len(samples)\n",
    "            p = q\n",
    "            # æ­£è¦è¿‘ä¼¼ã‚’ä½¿ç”¨\n",
    "            return np.sqrt(p * (1 - p) / n) / stats.norm.pdf(stats.norm.ppf(p))\n",
    "        \n",
    "        mcse_q025 = mcse_quantile(clean_samples, 0.025)\n",
    "        mcse_q975 = mcse_quantile(clean_samples, 0.975)\n",
    "        \n",
    "        # 4. Gewekeè¨ºæ–­ï¼ˆå‰åŠã¨å¾ŒåŠã®æ¯”è¼ƒï¼‰\n",
    "        first_10pct = int(0.1 * n_samples)\n",
    "        last_50pct = int(0.5 * n_samples)\n",
    "        \n",
    "        geweke_scores = []\n",
    "        for chain in range(n_chains):\n",
    "            first_part = param_chains[chain, :first_10pct]\n",
    "            last_part = param_chains[chain, -last_50pct:]\n",
    "            \n",
    "            if len(first_part) > 10 and len(last_part) > 10:\n",
    "                mean_diff = np.mean(first_part) - np.mean(last_part)\n",
    "                \n",
    "                # ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦ã«ã‚ˆã‚‹åˆ†æ•£æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "                var_first = np.var(first_part, ddof=1) / len(first_part)\n",
    "                var_last = np.var(last_part, ddof=1) / len(last_part)\n",
    "                \n",
    "                geweke_score = mean_diff / np.sqrt(var_first + var_last)\n",
    "                geweke_scores.append(geweke_score)\n",
    "        \n",
    "        geweke_pvalue = 2 * (1 - stats.norm.cdf(np.abs(np.mean(geweke_scores))))\n",
    "        \n",
    "        # 5. Heidelberger-Welch æ¤œå®šï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "        # å®šå¸¸æ€§ã®æ¤œå®š\n",
    "        hw_pvalues = []\n",
    "        for chain in range(n_chains):\n",
    "            chain_data = param_chains[chain]\n",
    "            \n",
    "            # ãƒã‚§ãƒ¼ãƒ³ã‚’è¤‡æ•°ã®çª“ã«åˆ†å‰²ã—ã¦å¹³å‡ã®é•ã„ã‚’æ¤œå®š\n",
    "            n_windows = 5\n",
    "            window_size = len(chain_data) // n_windows\n",
    "            window_means = []\n",
    "            \n",
    "            for w in range(n_windows):\n",
    "                start_idx = w * window_size\n",
    "                end_idx = (w + 1) * window_size\n",
    "                if end_idx <= len(chain_data):\n",
    "                    window_means.append(np.mean(chain_data[start_idx:end_idx]))\n",
    "            \n",
    "            if len(window_means) > 2:\n",
    "                # ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æ\n",
    "                _, p_val = stats.f_oneway(*[chain_data[i*window_size:(i+1)*window_size] \n",
    "                                           for i in range(len(window_means))])\n",
    "                hw_pvalues.append(p_val)\n",
    "        \n",
    "        hw_pvalue = np.mean(hw_pvalues) if hw_pvalues else np.nan\n",
    "        \n",
    "        # 6. ESS (Effective Sample Size) ã®è©³ç´°è¨ˆç®—\n",
    "        # ãƒãƒƒãƒæ³•ã«ã‚ˆã‚‹æ¨å®šã‚‚è¿½åŠ \n",
    "        batch_sizes = [10, 20, 50, 100]\n",
    "        batch_ess = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            if len(clean_samples) > batch_size * 10:\n",
    "                n_batches = len(clean_samples) // batch_size\n",
    "                batches = clean_samples[:n_batches * batch_size].reshape(n_batches, batch_size)\n",
    "                batch_means = np.mean(batches, axis=1)\n",
    "                \n",
    "                # ãƒãƒƒãƒå¹³å‡ã®åˆ†æ•£\n",
    "                batch_var = np.var(batch_means, ddof=1)\n",
    "                total_var = np.var(clean_samples, ddof=1)\n",
    "                \n",
    "                if batch_var > 0:\n",
    "                    ess_batch = len(clean_samples) * total_var / (batch_size * batch_var)\n",
    "                    batch_ess.append(ess_batch)\n",
    "        \n",
    "        avg_batch_ess = np.mean(batch_ess) if batch_ess else n_eff\n",
    "        \n",
    "        results[param_name] = {\n",
    "            'mean': np.mean(clean_samples),\n",
    "            'std': np.std(clean_samples, ddof=1),\n",
    "            'q025': np.percentile(clean_samples, 2.5),\n",
    "            'q975': np.percentile(clean_samples, 97.5),\n",
    "            'R_hat': R_hat,\n",
    "            'n_eff': n_eff,\n",
    "            'n_eff_batch': avg_batch_ess,\n",
    "            'mcse': mcse,\n",
    "            'mcse_q025': mcse_q025,\n",
    "            'mcse_q975': mcse_q975,\n",
    "            'geweke_score': np.mean(geweke_scores) if geweke_scores else np.nan,\n",
    "            'geweke_pvalue': geweke_pvalue,\n",
    "            'hw_pvalue': hw_pvalue,\n",
    "            'n_samples_total': len(all_samples),\n",
    "            'n_samples_clean': len(clean_samples),\n",
    "            'efficiency': n_eff / len(clean_samples)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_diagnostics_table(diagnostics):\n",
    "    \"\"\"\n",
    "    è¨ºæ–­çµæœã®è¡¨å½¢å¼å‡ºåŠ›\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MCMCè¨ºæ–­çµ±è¨ˆé‡ã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # ãƒ˜ãƒƒãƒ€ãƒ¼\n",
    "    header = f\"{'Parameter':<15} {'Mean':<8} {'Std':<8} {'R-hat':<8} {'N_eff':<8} {'MCSE':<8} {'Geweke':<8} {'Status':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for param_name, stats in diagnostics.items():\n",
    "        # åæŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®åˆ¤å®š\n",
    "        status = \"Good\"\n",
    "        if stats['R_hat'] > 1.1:\n",
    "            status = \"Poor\"\n",
    "        elif stats['R_hat'] > 1.01:\n",
    "            status = \"Caution\"\n",
    "        \n",
    "        if stats['n_eff'] < 100:\n",
    "            status += \"/Low ESS\"\n",
    "        \n",
    "        row = (f\"{param_name:<15} {stats['mean']:<8.3f} {stats['std']:<8.3f} \"\n",
    "               f\"{stats['R_hat']:<8.4f} {stats['n_eff']:<8.0f} {stats['mcse']:<8.4f} \"\n",
    "               f\"{stats['geweke_pvalue']:<8.3f} {status:<12}\")\n",
    "        print(row)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"åˆ¤å®šåŸºæº–:\")\n",
    "    print(\"  R-hat < 1.01: Good, 1.01-1.1: Caution, > 1.1: Poor\")\n",
    "    print(\"  Geweke: p-value > 0.05 ã§å®šå¸¸æ€§ä»®èª¬ã‚’æ£„å´ã—ãªã„\")\n",
    "    print(\"  N_eff: æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º (ç›®å®‰: > 100)\")\n",
    "\n",
    "# è¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
    "print(\"è¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—ä¸­...\")\n",
    "\n",
    "# æ­£è¦åˆ†å¸ƒã®è¨ºæ–­\n",
    "diagnostics_normal = comprehensive_diagnostics(chains_normal, ['Normal'])\n",
    "print(\"\\næ­£è¦åˆ†å¸ƒã®è¨ºæ–­çµæœ:\")\n",
    "print_diagnostics_table(diagnostics_normal)\n",
    "\n",
    "# å¤šå³°æ€§åˆ†å¸ƒã®è¨ºæ–­\n",
    "diagnostics_multimodal = comprehensive_diagnostics(chains_multimodal, ['Multimodal'])\n",
    "print(\"\\nå¤šå³°æ€§åˆ†å¸ƒã®è¨ºæ–­çµæœ:\")\n",
    "print_diagnostics_table(diagnostics_multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 å®Ÿè·µçš„ãªè¨ºæ–­æ‰‹é †\n",
    "\n",
    "å®Ÿéš›ã®MCMCåˆ†æã§æ¨å¥¨ã•ã‚Œã‚‹è¨ºæ–­æ‰‹é †ã‚’ã¾ã¨ã‚ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def mcmc_diagnostic_workflow(target_log_pdf, initial_values, n_samples, \n                           step_size=1.0, parameter_names=None, \n                           true_values=None, verbose=True):\n    \"\"\"\n    MCMCè¨ºæ–­ã®æ¨™æº–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ - ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆ\n    \n    ã“ã®é–¢æ•°ã¯å®Ÿéš›ã®ç ”ç©¶ã‚„æ¥­å‹™ã§ä½¿ç”¨ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã®\n    åŒ…æ‹¬çš„ãªè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›ã—ã¾ã™ã€‚\n    \n    Returns:\n    - diagnostics: è¨ºæ–­çµæœã®è¾æ›¸\n    - recommendations: æ¨å¥¨äº‹é …ã®ãƒªã‚¹ãƒˆ\n    - quality_score: ç·åˆå“è³ªã‚¹ã‚³ã‚¢ (0-100)\n    \"\"\"\n    if verbose:\n        print(\"ğŸ”§ MCMCè¨ºæ–­ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹...\")\n        print(f\"   ãƒã‚§ãƒ¼ãƒ³æ•°: {len(initial_values)}\")\n        print(f\"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {n_samples}\")\n        print(f\"   ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º: {step_size}\")\n    \n    # Step 1: è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ\n    if verbose:\n        print(\"\\nğŸ“Š Step 1: è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n    \n    chains, acceptance_rates = run_multiple_chains(\n        target_log_pdf, initial_values, n_samples, step_size\n    )\n    \n    # Step 2: åŸºæœ¬çµ±è¨ˆã®ç¢ºèª\n    avg_acceptance = np.mean(acceptance_rates)\n    if verbose:\n        print(f\"\\nğŸ“ˆ Step 2: åŸºæœ¬çµ±è¨ˆã®ç¢ºèª\")\n        print(f\"   å¹³å‡å—ç†ç‡: {avg_acceptance:.3f}\")\n        print(f\"   å—ç†ç‡ç¯„å›²: {np.min(acceptance_rates):.3f} - {np.max(acceptance_rates):.3f}\")\n        \n        # å—ç†ç‡ã®è©•ä¾¡\n        if 0.4 <= avg_acceptance <= 0.6:\n            print(\"   âœ… å—ç†ç‡ã¯é©åˆ‡ãªç¯„å›²å†…ã§ã™\")\n        elif 0.2 <= avg_acceptance < 0.4 or 0.6 < avg_acceptance <= 0.8:\n            print(\"   âš ï¸  å—ç†ç‡ãŒã‚„ã‚„ç¯„å›²å¤–ã§ã™\")\n        else:\n            print(\"   âŒ å—ç†ç‡ãŒæ¨å¥¨ç¯„å›²å¤–ã§ã™\")\n    \n    # Step 3: è¦–è¦šçš„è¨ºæ–­\n    if verbose:\n        print(\"\\nğŸ‘ï¸  Step 3: è¦–è¦šçš„è¨ºæ–­\")\n    \n    plot_multiple_chains(chains, \"Diagnostic Workflow - Visual Inspection\", \n                        parameter_name=\"Parameter\", \n                        true_mean=true_values[0] if true_values else None)\n    \n    # Step 4: æ•°å€¤è¨ºæ–­\n    if verbose:\n        print(\"\\nğŸ”¢ Step 4: æ•°å€¤è¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—\")\n    \n    diagnostics = comprehensive_diagnostics(chains, parameter_names)\n    print_diagnostics_table(diagnostics)\n    \n    # Step 5: å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n    quality_scores = []\n    for param_name, stats in diagnostics.items():\n        param_score = 100  # æº€ç‚¹ã‹ã‚‰æ¸›ç‚¹æ–¹å¼\n        \n        # R-hatè©•ä¾¡ (40ç‚¹æº€ç‚¹)\n        if stats['R_hat'] <= 1.01:\n            rhat_score = 40\n        elif stats['R_hat'] <= 1.05:\n            rhat_score = 30\n        elif stats['R_hat'] <= 1.1:\n            rhat_score = 15\n        else:\n            rhat_score = 0\n        \n        # ESSè©•ä¾¡ (30ç‚¹æº€ç‚¹)\n        if stats['n_eff'] >= 400:\n            ess_score = 30\n        elif stats['n_eff'] >= 200:\n            ess_score = 20\n        elif stats['n_eff'] >= 100:\n            ess_score = 10\n        else:\n            ess_score = 0\n        \n        # å—ç†ç‡è©•ä¾¡ (20ç‚¹æº€ç‚¹)\n        if 0.4 <= avg_acceptance <= 0.6:\n            acc_score = 20\n        elif 0.2 <= avg_acceptance <= 0.8:\n            acc_score = 15\n        else:\n            acc_score = 5\n        \n        # MCSEè©•ä¾¡ (10ç‚¹æº€ç‚¹)\n        relative_mcse = stats['mcse'] / stats['std']\n        if relative_mcse <= 0.05:\n            mcse_score = 10\n        elif relative_mcse <= 0.1:\n            mcse_score = 7\n        elif relative_mcse <= 0.2:\n            mcse_score = 3\n        else:\n            mcse_score = 0\n        \n        param_score = rhat_score + ess_score + acc_score + mcse_score\n        quality_scores.append(param_score)\n    \n    overall_quality = np.mean(quality_scores)\n    \n    # Step 6: æ¨å¥¨äº‹é …ã®ç”Ÿæˆï¼ˆè©³ç´°ç‰ˆï¼‰\n    recommendations = []\n    critical_issues = []\n    warnings = []\n    suggestions = []\n    \n    for param_name, stats in diagnostics.items():\n        # è‡´å‘½çš„ãªå•é¡Œ\n        if stats['R_hat'] > 1.1:\n            critical_issues.append(f\"{param_name}: åæŸã—ã¦ã„ã¾ã›ã‚“ (R-hat = {stats['R_hat']:.4f})\")\n        \n        if stats['n_eff'] < 50:\n            critical_issues.append(f\"{param_name}: æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒæ¥µç«¯ã«å°ã•ã„ã§ã™ ({stats['n_eff']:.0f})\")\n        \n        # è­¦å‘Šãƒ¬ãƒ™ãƒ«\n        if 1.01 < stats['R_hat'] <= 1.1:\n            warnings.append(f\"{param_name}: R-hat ãŒã‚„ã‚„é«˜ã„ã§ã™ ({stats['R_hat']:.4f})\")\n        \n        if 50 <= stats['n_eff'] < 100:\n            warnings.append(f\"{param_name}: æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„ã§ã™ ({stats['n_eff']:.0f})\")\n        \n        relative_mcse = stats['mcse'] / stats['std']\n        if relative_mcse > 0.1:\n            warnings.append(f\"{param_name}: Monte Carloèª¤å·®ãŒå¤§ãã„ã§ã™ ({relative_mcse:.3f})\")\n        \n        # æ”¹å–„ææ¡ˆ\n        if stats['efficiency'] < 0.1:\n            suggestions.append(f\"{param_name}: åŠ¹ç‡ãŒä½ã„ã§ã™ã€‚ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œè¨ã—ã¦ãã ã•ã„\")\n        \n        if not np.isnan(stats['geweke_pvalue']) and stats['geweke_pvalue'] < 0.05:\n            suggestions.append(f\"{param_name}: éå®šå¸¸æ€§ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒãƒ¼ãƒ³ã‚¤ãƒ³æœŸé–“ã‚’å»¶é•·ã—ã¦ãã ã•ã„\")\n    \n    # å—ç†ç‡ã«é–¢ã™ã‚‹æ¨å¥¨äº‹é …\n    if avg_acceptance < 0.2:\n        critical_issues.append(f\"å—ç†ç‡ãŒæ¥µç«¯ã«ä½ã„ã§ã™ ({avg_acceptance:.3f}). ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’å¤§å¹…ã«ç¸®å°ã—ã¦ãã ã•ã„\")\n    elif avg_acceptance < 0.4:\n        warnings.append(f\"å—ç†ç‡ãŒä½ã„ã§ã™ ({avg_acceptance:.3f}). ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’ç¸®å°ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„\")\n    elif avg_acceptance > 0.8:\n        warnings.append(f\"å—ç†ç‡ãŒé«˜ã™ãã¾ã™ ({avg_acceptance:.3f}). ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ã—ã¦ãã ã•ã„\")\n    \n    # ç·åˆçš„ãªæ¨å¥¨äº‹é …\n    total_samples = len(initial_values) * n_samples\n    total_eff = sum([stats['n_eff'] for stats in diagnostics.values()])\n    overall_efficiency = total_eff / total_samples\n    \n    if overall_efficiency < 0.05:\n        critical_issues.append(f\"å…¨ä½“çš„ãªåŠ¹ç‡ãŒæ¥µç«¯ã«ä½ã„ã§ã™ ({overall_efficiency:.3f}). ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ ¹æœ¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦ã§ã™\")\n    elif overall_efficiency < 0.1:\n        warnings.append(f\"å…¨ä½“çš„ãªåŠ¹ç‡ãŒä½ã„ã§ã™ ({overall_efficiency:.3f}). ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„\")\n    \n    # æ¨å¥¨äº‹é …ã®çµ±åˆ\n    recommendations = critical_issues + warnings + suggestions\n    \n    # Step 7: çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º\n    if verbose:\n        print(f\"\\nğŸ¯ Step 7: ç·åˆè©•ä¾¡\")\n        print(f\"   å“è³ªã‚¹ã‚³ã‚¢: {overall_quality:.1f}/100\")\n        \n        if overall_quality >= 80:\n            print(\"   âœ… å„ªç§€: è¨ºæ–­çµæœã¯éå¸¸ã«è‰¯å¥½ã§ã™\")\n        elif overall_quality >= 60:\n            print(\"   âš ï¸  è‰¯å¥½: è»½å¾®ãªæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™\")\n        elif overall_quality >= 40:\n            print(\"   âš ï¸  æ³¨æ„: ã„ãã¤ã‹ã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n        else:\n            print(\"   âŒ å•é¡Œ: é‡å¤§ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n        \n        print(f\"   å…¨ä½“åŠ¹ç‡: {overall_efficiency:.3f}\")\n        \n        if recommendations:\n            print(f\"\\nğŸ“ æ”¹å–„ææ¡ˆ ({len(recommendations)}ä»¶):\")\n            for i, rec in enumerate(recommendations, 1):\n                if rec in critical_issues:\n                    print(f\"   ğŸ”´ {i}. {rec}\")\n                elif rec in warnings:\n                    print(f\"   ğŸŸ¡ {i}. {rec}\")\n                else:\n                    print(f\"   ğŸ”µ {i}. {rec}\")\n        else:\n            print(\"\\nğŸ‰ ç´ æ™´ã‚‰ã—ã„! æ”¹å–„ææ¡ˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\")\n    \n    return {\n        'chains': chains,\n        'acceptance_rates': acceptance_rates,\n        'diagnostics': diagnostics,\n        'recommendations': recommendations,\n        'critical_issues': critical_issues,\n        'warnings': warnings,\n        'suggestions': suggestions,\n        'overall_efficiency': overall_efficiency,\n        'quality_score': overall_quality,\n        'quality_breakdown': {\n            'r_hat_component': np.mean([40 if stats['R_hat'] <= 1.01 else 0 for stats in diagnostics.values()]),\n            'ess_component': np.mean([30 if stats['n_eff'] >= 400 else 0 for stats in diagnostics.values()]),\n            'acceptance_component': 20 if 0.4 <= avg_acceptance <= 0.6 else 0,\n            'mcse_component': np.mean([10 if stats['mcse']/stats['std'] <= 0.05 else 0 for stats in diagnostics.values()])\n        }\n    }\n\n# ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«è¨ºæ–­ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œä¾‹\nprint(\"=\" * 80)\nprint(\"ğŸš€ ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«è¨ºæ–­ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œä¾‹\")\nprint(\"=\" * 80)\n\nprint(\"\\n=== æ­£è¦åˆ†å¸ƒã§ã®é«˜å“è³ªè¨ºæ–­ ===\")\nresults_normal_pro = mcmc_diagnostic_workflow(\n    target_log_pdf=lambda x: stats.norm.logpdf(x, 0, 1),\n    initial_values=[-2, -1, 0, 1, 2],\n    n_samples=3000,\n    step_size=1.0,\n    parameter_names=['Normal'],\n    true_values=[0.0]\n)\n\nprint(f\"\\nğŸ“Š å“è³ªå†…è¨³:\")\nfor component, score in results_normal_pro['quality_breakdown'].items():\n    print(f\"   {component}: {score:.1f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"=== å¤šå³°æ€§åˆ†å¸ƒã§ã®è¨ºæ–­ï¼ˆãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚±ãƒ¼ã‚¹ï¼‰ ===\")\nresults_multimodal_pro = mcmc_diagnostic_workflow(\n    target_log_pdf=multimodal_log_pdf,\n    initial_values=[-4, -2, 0, 2, 4],\n    n_samples=3000,\n    step_size=1.5,\n    parameter_names=['Multimodal']\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 æ¼”ç¿’å•é¡Œ\n",
    "\n",
    "### å•é¡Œ1ï¼šåæŸè¨ºæ–­ã®å®Ÿè·µ\n",
    "ä»¥ä¸‹ã®å›°é›£ãªåˆ†å¸ƒã«å¯¾ã—ã¦MCMCã‚’å®Ÿè¡Œã—ã€åæŸè¨ºæ–­ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ1: å›°é›£ãªåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "def challenging_log_pdf(x):\n",
    "    \"\"\"\n",
    "    æŒ‘æˆ¦çš„ãªåˆ†å¸ƒï¼šé«˜ã„ç›¸é–¢ã‚’æŒã¤2å¤‰é‡åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    if len(x) != 2:\n",
    "        return -np.inf\n",
    "    \n",
    "    # éå¸¸ã«ç´°é•·ã„åˆ†å¸ƒï¼ˆé«˜ç›¸é–¢ï¼‰\n",
    "    mu = np.array([0, 0])\n",
    "    cov = np.array([[1, 0.99], [0.99, 1]])\n",
    "    \n",
    "    diff = x - mu\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        log_det = 2 * np.sum(np.log(np.diag(chol)))\n",
    "        solve = np.linalg.solve(chol, diff)\n",
    "        mahalanobis_sq = np.sum(solve**2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf\n",
    "    \n",
    "    return -0.5 * (2 * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n",
    "\n",
    "# ã“ã“ã§2å¤‰é‡MHã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã€è¨ºæ–­ã—ã¦ãã ã•ã„\n",
    "# ãƒ’ãƒ³ãƒˆï¼š\n",
    "# 1. è¤‡æ•°ã®åˆæœŸå€¤ã‹ã‚‰é–‹å§‹\n",
    "# 2. ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’è©¦ã™\n",
    "# 3. åæŸè¨ºæ–­ã‚’å®Ÿè¡Œ\n",
    "# 4. å•é¡Œç‚¹ã‚’ç‰¹å®šã—ã€æ”¹å–„ç­–ã‚’ææ¡ˆ\n",
    "\n",
    "pass  # å­¦ç¿’è€…ãŒå®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å•é¡Œ2ï¼šé©å¿œçš„è¨ºæ–­\n",
    "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§åæŸã‚’ç›£è¦–ã—ã€è‡ªå‹•çš„ã«åœæ­¢æ¡ä»¶ã‚’åˆ¤å®šã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ2: é©å¿œçš„åæŸè¨ºæ–­\n",
    "def adaptive_mcmc_with_diagnostics(target_log_pdf, initial_values, \n",
    "                                 max_samples=10000, check_interval=500,\n",
    "                                 r_hat_threshold=1.01, min_eff_samples=1000):\n",
    "    \"\"\"\n",
    "    é©å¿œçš„åæŸè¨ºæ–­ä»˜ãMCMC\n",
    "    \n",
    "    Parameters:\n",
    "    - target_log_pdf: ç›®æ¨™åˆ†å¸ƒ\n",
    "    - initial_values: åˆæœŸå€¤ã®ãƒªã‚¹ãƒˆ\n",
    "    - max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    - check_interval: è¨ºæ–­ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–“éš”\n",
    "    - r_hat_threshold: R-hat ã®åæŸé–¾å€¤\n",
    "    - min_eff_samples: æœ€å°æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    \n",
    "    å®Ÿè£…ã®ãƒ’ãƒ³ãƒˆ:\n",
    "    1. check_interval ã”ã¨ã« R-hat ã¨æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "    2. åæŸæ¡ä»¶ã‚’æº€ãŸã—ãŸã‚‰æ—©æœŸåœæ­¢\n",
    "    3. åæŸã®å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    # ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "    pass  # å­¦ç¿’è€…ãŒå®Ÿè£…\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "# result = adaptive_mcmc_with_diagnostics(\n",
    "#     lambda x: stats.norm.logpdf(x, 0, 1),\n",
    "#     [-2, -1, 0, 1, 2]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ã¾ã¨ã‚ï¼šå®Ÿè·µçš„MCMCè¨ºæ–­ã®ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰\n\nã“ã®ç« ã§ã¯ã€MCMCã®åæŸè¨ºæ–­ã¨æ€§èƒ½è©•ä¾¡ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸï¼š\n\n### ğŸ” é‡è¦ãªè¨ºæ–­æ‰‹æ³•\n\n1. **è¦–è¦šçš„è¨ºæ–­**ï¼š\n   - **ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ**ï¼šæ··åˆã¨åæŸã®å³åº§ã®ç¢ºèª\n   - **ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å¹³å‡**ï¼šåæŸã®å®‰å®šæ€§ã‚’æ™‚ç³»åˆ—ã§è¿½è·¡\n   - **å¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆ**ï¼šæ™‚é–“çª“åˆ¥ã®åˆ†å¸ƒå¤‰åŒ–ã‚’æ¤œå‡º\n\n2. **æ•°å€¤è¨ºæ–­**ï¼š\n   - **Gelman-Rubinçµ±è¨ˆé‡ï¼ˆR-hatï¼‰**ï¼š< 1.01 ã§è‰¯å¥½ã€< 1.1ã§è¨±å®¹ç¯„å›²\n   - **æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆESSï¼‰**ï¼š> 400 ãŒç†æƒ³ã€æœ€ä½ > 100\n   - **Monte Carloæ¨™æº–èª¤å·®ï¼ˆMCSEï¼‰**ï¼šæ¨å®šç²¾åº¦ã®å®šé‡çš„æŒ‡æ¨™\n   - **Gewekeè¨ºæ–­**ï¼šå®šå¸¸æ€§ã®çµ±è¨ˆçš„æ¤œå®š\n\n3. **è‡ªå·±ç›¸é–¢åˆ†æ**ï¼š\n   - **çµ±åˆè‡ªå·±ç›¸é–¢æ™‚é–“**ï¼šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ¹ç‡ã®æ ¹æœ¬æŒ‡æ¨™\n   - **è–„åŒ–ï¼ˆthinningï¼‰**ã®åŠ¹æœæ¤œè¨¼\n   - **æŒ‡æ•°çš„æ¸›è¡°**ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹æ™‚å®šæ•°æ¨å®š\n\n### ğŸ“‹ å®Ÿè·µçš„ãªè¨ºæ–­æ‰‹é †ï¼ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼‰\n\n#### ãƒ•ã‚§ãƒ¼ã‚º1: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­è¨ˆ\n- [ ] **è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³æº–å‚™**ï¼šæœ€ä½3ãƒã‚§ãƒ¼ãƒ³ã€ç†æƒ³çš„ã«ã¯8-10ãƒã‚§ãƒ¼ãƒ³\n- [ ] **åˆæœŸå€¤åˆ†æ•£**ï¼šç›®æ¨™åˆ†å¸ƒã®ç•°ãªã‚‹é ˜åŸŸã‹ã‚‰é–‹å§‹\n- [ ] **ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°**ï¼šæœ€ä½2000ã€è¤‡é›‘ãªå•é¡Œã§ã¯10000+\n\n#### ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–\n- [ ] **å®šæœŸçš„ãƒã‚§ãƒƒã‚¯**ï¼š500-1000ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«è¨ºæ–­å®Ÿè¡Œ\n- [ ] **æ—©æœŸè­¦å‘Š**ï¼šR-hat > 1.2ã§å³åº§ã«ã‚¢ãƒ©ãƒ¼ãƒˆ\n- [ ] **ãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–**ï¼šåæŸæŒ‡æ¨™ã®æ™‚é–“å¤‰åŒ–ã‚’è¿½è·¡\n\n#### ãƒ•ã‚§ãƒ¼ã‚º3: ç·åˆè¨ºæ–­\n- [ ] **è¦–è¦šçš„æ¤œæŸ»**ï¼šå…¨ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã‚’ç›®è¦–ç¢ºèª\n- [ ] **æ•°å€¤è¨ºæ–­**ï¼šR-hat < 1.01ã€ESS > 400ã‚’ç¢ºèª\n- [ ] **è‡ªå·±ç›¸é–¢åˆ†æ**ï¼šÏ„_int < 10ï¼ˆç›®å®‰ï¼‰\n- [ ] **åˆ†å¸ƒæ¯”è¼ƒ**ï¼šç†è«–å€¤ã¾ãŸã¯ä»–æ‰‹æ³•ã¨ã®ä¸€è‡´ç¢ºèª\n\n#### ãƒ•ã‚§ãƒ¼ã‚º4: å“è³ªä¿è¨¼\n- [ ] **æ„Ÿåº¦åˆ†æ**ï¼šç•°ãªã‚‹åˆæœŸå€¤ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§çµæœã®ä¸€è²«æ€§ç¢ºèª\n- [ ] **å†ç¾æ€§ãƒ†ã‚¹ãƒˆ**ï¼šåŒã˜è¨­å®šã§è¤‡æ•°å›å®Ÿè¡Œã—çµæœæ¯”è¼ƒ\n- [ ] **æ–‡æ›¸åŒ–**ï¼šè¨ºæ–­çµæœã¨åˆ¤æ–­æ ¹æ‹ ã‚’è¨˜éŒ²\n\n### âš ï¸ ä¸€èˆ¬çš„ãªå•é¡Œã¨å¯¾ç­–ãƒãƒˆãƒªãƒƒã‚¯ã‚¹\n\n| ç—‡çŠ¶ | åŸå›  | å¯¾ç­– | ç·Šæ€¥åº¦ |\n|------|------|------|--------|\n| R-hat > 1.1 | åæŸä¸è‰¯ | ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—åŠ ã€åˆæœŸå€¤å¤‰æ›´ | ğŸ”´ é«˜ |\n| ESS < 100 | åŠ¹ç‡ä½ä¸‹ | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºèª¿æ•´ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å¤‰æ›´ | ğŸŸ¡ ä¸­ |\n| é«˜ã„è‡ªå·±ç›¸é–¢ | æ··åˆä¸è‰¯ | è–„åŒ–ã€ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–° | ğŸŸ¡ ä¸­ |\n| ãƒ¢ãƒ¼ãƒ‰é–“ç§»å‹•ãªã— | å¤šå³°æ€§ | ä¸¦åˆ—ç„¼ããªã¾ã—ã€é•·ã„ãƒã‚§ãƒ¼ãƒ³ | ğŸ”´ é«˜ |\n| å—ç†ç‡ < 20% | ã‚¹ãƒ†ãƒƒãƒ—éå¤§ | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºç¸®å° | ğŸŸ¢ ä½ |\n| å—ç†ç‡ > 70% | ã‚¹ãƒ†ãƒƒãƒ—éå° | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºæ‹¡å¤§ | ğŸŸ¢ ä½ |\n\n### ğŸ¯ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n\n#### è¨­è¨ˆæ®µéš\n- **ä¿å®ˆçš„è¨­å®š**ï¼šå•é¡Œã®è¤‡é›‘ã•ã‚’éå°è©•ä¾¡ã—ãªã„\n- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å¢—åŠ ã«å‚™ãˆãŸè¨­è¨ˆ\n- **è‡ªå‹•åŒ–**ï¼šäººæ‰‹ã«ã‚ˆã‚‹åˆ¤æ–­ãƒŸã‚¹ã‚’é˜²ãã‚·ã‚¹ãƒ†ãƒ åŒ–\n\n#### å®Ÿè¡Œæ®µéš\n- **æ®µéšçš„å¢—åŠ **ï¼šçŸ­ã„ãƒ†ã‚¹ãƒˆã‹ã‚‰å§‹ã‚ã¦æ®µéšçš„ã«é•·ãã™ã‚‹\n- **ä¸¦åˆ—å®Ÿè¡Œ**ï¼šè¨ˆç®—è³‡æºã‚’æœ€å¤§æ´»ç”¨\n- **ä¸­é–“ä¿å­˜**ï¼šé•·æ™‚é–“è¨ˆç®—ã®ä¸­æ–­ãƒªã‚¹ã‚¯å¯¾ç­–\n\n#### è©•ä¾¡æ®µéš\n- **è¤‡æ•°æŒ‡æ¨™**ï¼šå˜ä¸€æŒ‡æ¨™ã«ä¾å­˜ã—ãªã„ç·åˆåˆ¤æ–­\n- **å¯è¦–åŒ–é‡è¦–**ï¼šæ•°å€¤ã ã‘ã§ãªãå¿…ãšè¦–è¦šçš„ç¢ºèª\n- **å¤–éƒ¨æ¤œè¨¼**ï¼šç†è«–å€¤ã‚„ä»–æ‰‹æ³•ã¨ã®æ¯”è¼ƒ\n\n### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n\nåæŸè¨ºæ–­ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã‚ãªãŸã¯ã€MCMCã‚’**å®‰å…¨ã§ä¿¡é ¼æ€§ã®é«˜ã„ãƒ„ãƒ¼ãƒ«**ã¨ã—ã¦ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n\næ¬¡ã®ç« ã§ã¯ã€ã“ã‚Œã¾ã§å­¦ã‚“ã å…¨ã¦ã®MCMCæ‰‹æ³•ã‚’**å®Ÿéš›ã®ãƒ™ã‚¤ã‚ºæ¨è«–å•é¡Œ**ã«é©ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å®Ÿè·µçš„èª²é¡Œã‚’è§£æ±ºã—ã¦ã„ãã¾ã™ï¼š\n\n- å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«\n- æ¬ æãƒ‡ãƒ¼ã‚¿ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n- äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–\n- ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æ¯”è¼ƒ\n\n**é‡è¦**ï¼šå®Œç’§ãªè¨ºæ–­ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã—ã‹ã—ã€**ä½“ç³»çš„ã§ä¸€è²«ã—ãŸè¨ºæ–­æ‰‹é †**ã«ã‚ˆã‚Šã€MCMCã®ä¿¡é ¼æ€§ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}