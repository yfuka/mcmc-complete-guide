{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: 収束診断と性能評価\n",
    "\n",
    "## 学習目標\n",
    "- MCMCの収束診断の重要性を理解する\n",
    "- 視覚的診断手法を習得する\n",
    "- 数値的診断統計量を計算・解釈できる\n",
    "- 有効サンプルサイズと自己相関時間を理解する\n",
    "- 複数チェーンを用いた収束診断を実践する\n",
    "- 実践的な診断手順を身につける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 収束診断の重要性\n",
    "\n",
    "MCMCサンプリングにおいて、以下の点を確認する必要があります：\n",
    "\n",
    "1. **収束**：マルコフ連鎖が定常分布に達したか？\n",
    "2. **混合**：状態空間を十分に探索しているか？\n",
    "3. **効率**：自己相関が十分に小さいか？\n",
    "\n",
    "### 収束の失敗例\n",
    "まず、収束しない場合の例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 収束が困難な分布の例：多峰性分布\n",
    "def multimodal_log_pdf(x):\n",
    "    \"\"\"多峰性分布（3つのピーク）\"\"\"\n",
    "    component1 = stats.norm.logpdf(x, -4, 0.5)\n",
    "    component2 = stats.norm.logpdf(x, 0, 0.5)\n",
    "    component3 = stats.norm.logpdf(x, 4, 0.5)\n",
    "    \n",
    "    # log(exp(c1) + exp(c2) + exp(c3))\n",
    "    max_comp = np.maximum(np.maximum(component1, component2), component3)\n",
    "    return max_comp + np.log(\n",
    "        np.exp(component1 - max_comp) + \n",
    "        np.exp(component2 - max_comp) + \n",
    "        np.exp(component3 - max_comp)\n",
    "    )\n",
    "\n",
    "def metropolis_hastings_simple(target_log_pdf, initial_value, n_samples, step_size=0.5):\n",
    "    \"\"\"シンプルなメトロポリス・ヘイスティングス実装\"\"\"\n",
    "    samples = np.zeros(n_samples)\n",
    "    current = initial_value\n",
    "    current_log_prob = target_log_pdf(current)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 提案\n",
    "        proposed = current + np.random.normal(0, step_size)\n",
    "        proposed_log_prob = target_log_pdf(proposed)\n",
    "        \n",
    "        # 受理確率\n",
    "        log_alpha = proposed_log_prob - current_log_prob\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # 受理/棄却\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposed\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = current\n",
    "    \n",
    "    return samples, n_accepted / n_samples\n",
    "\n",
    "# 異なる初期値と異なるステップサイズでサンプリング\n",
    "initial_values = [-4, 0, 4]\n",
    "step_sizes = [0.1, 1.0, 3.0]\n",
    "n_samples = 5000\n",
    "\n",
    "# 結果の保存\n",
    "sampling_results = {}\n",
    "\n",
    "for init_val in initial_values:\n",
    "    for step_size in step_sizes:\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            multimodal_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        sampling_results[key] = {\n",
    "            'samples': samples,\n",
    "            'acceptance_rate': acc_rate,\n",
    "            'initial_value': init_val,\n",
    "            'step_size': step_size\n",
    "        }\n",
    "\n",
    "print(\"収束の問題を示すサンプリング結果:\")\n",
    "for key, result in sampling_results.items():\n",
    "    mean_sample = np.mean(result['samples'][1000:])\n",
    "    print(f\"{key}: 受理率={result['acceptance_rate']:.3f}, 平均={mean_sample:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 収束問題の可視化\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "# 真の分布を計算\n",
    "x_range = np.linspace(-6, 6, 1000)\n",
    "true_density = np.exp(multimodal_log_pdf(x_range))\n",
    "\n",
    "for i, init_val in enumerate(initial_values):\n",
    "    for j, step_size in enumerate(step_sizes):\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples']\n",
    "        \n",
    "        # トレースプロット\n",
    "        axes[i, j].plot(samples[:2000], alpha=0.7, linewidth=0.8)\n",
    "        axes[i, j].axhline(init_val, color='red', linestyle='--', alpha=0.7, label='Initial')\n",
    "        axes[i, j].set_title(f'Init={init_val}, Step={step_size}\\nAcc={sampling_results[key][\"acceptance_rate\"]:.3f}')\n",
    "        axes[i, j].set_xlabel('Iteration')\n",
    "        axes[i, j].set_ylabel('Value')\n",
    "        axes[i, j].legend()\n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Convergence Issues in Multimodal Distribution', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ヒストグラム比較\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, step_size in enumerate(step_sizes):\n",
    "    for init_val in initial_values:\n",
    "        key = f\"init_{init_val}_step_{step_size}\"\n",
    "        samples = sampling_results[key]['samples'][1000:]\n",
    "        axes[i].hist(samples, bins=50, alpha=0.5, density=True, \n",
    "                    label=f'Init={init_val}')\n",
    "    \n",
    "    axes[i].plot(x_range, true_density, 'k-', linewidth=2, label='True')\n",
    "    axes[i].set_title(f'Step Size = {step_size}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 視覚的診断手法\n",
    "\n",
    "### 4.2.1 トレースプロット\n",
    "パラメータの時系列変化を可視化し、収束と混合を視覚的に確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace_diagnostics(samples, parameter_names=None, title=\"Trace Diagnostics\"):\n",
    "    \"\"\"\n",
    "    包括的なトレース診断プロット\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 4, figsize=(16, 4*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # 1. 全トレースプロット\n",
    "        axes[i, 0].plot(param_samples, alpha=0.7, linewidth=0.8)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - Full Trace')\n",
    "        axes[i, 0].set_xlabel('Iteration')\n",
    "        axes[i, 0].set_ylabel('Value')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 前半・後半の比較\n",
    "        mid_point = len(param_samples) // 2\n",
    "        axes[i, 1].plot(param_samples[:mid_point], alpha=0.7, label='First half', color='blue')\n",
    "        axes[i, 1].plot(range(mid_point, len(param_samples)), \n",
    "                       param_samples[mid_point:], alpha=0.7, label='Second half', color='red')\n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - First vs Second Half')\n",
    "        axes[i, 1].set_xlabel('Iteration')\n",
    "        axes[i, 1].set_ylabel('Value')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ランニング平均\n",
    "        running_mean = np.cumsum(param_samples) / np.arange(1, len(param_samples) + 1)\n",
    "        axes[i, 2].plot(running_mean)\n",
    "        # 信頼区間も表示\n",
    "        running_var = np.cumsum((param_samples - running_mean)**2) / np.arange(1, len(param_samples) + 1)\n",
    "        running_se = np.sqrt(running_var / np.arange(1, len(param_samples) + 1))\n",
    "        axes[i, 2].fill_between(range(len(running_mean)), \n",
    "                               running_mean - 1.96*running_se,\n",
    "                               running_mean + 1.96*running_se, alpha=0.3)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Running Mean')\n",
    "        axes[i, 2].set_xlabel('Iteration')\n",
    "        axes[i, 2].set_ylabel('Running Mean')\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. 密度プロット（時間窓別）\n",
    "        n_windows = 4\n",
    "        window_size = len(param_samples) // n_windows\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_windows))\n",
    "        \n",
    "        for w in range(n_windows):\n",
    "            start_idx = w * window_size\n",
    "            end_idx = (w + 1) * window_size if w < n_windows - 1 else len(param_samples)\n",
    "            window_samples = param_samples[start_idx:end_idx]\n",
    "            \n",
    "            if len(window_samples) > 10:  # 十分なサンプルがある場合のみ\n",
    "                axes[i, 3].hist(window_samples, bins=30, alpha=0.5, density=True,\n",
    "                               color=colors[w], label=f'Window {w+1}')\n",
    "        \n",
    "        axes[i, 3].set_title(f'{parameter_names[i]} - Density by Time Window')\n",
    "        axes[i, 3].set_xlabel('Value')\n",
    "        axes[i, 3].set_ylabel('Density')\n",
    "        axes[i, 3].legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 良好な収束例（正規分布）\n",
    "good_samples, _ = metropolis_hastings_simple(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), 0, 5000, 1.0\n",
    ")\n",
    "\n",
    "plot_trace_diagnostics(good_samples, ['Normal Distribution'], \"Good Convergence Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 自己相関関数の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_analysis(samples, max_lags=200, parameter_names=None):\n",
    "    \"\"\"\n",
    "    自己相関関数の詳細分析\n",
    "    \"\"\"\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.reshape(-1, 1)\n",
    "    \n",
    "    n_params = samples.shape[1]\n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_params, 3, figsize=(15, 5*n_params))\n",
    "    if n_params == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    autocorr_results = {}\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        param_samples = samples[:, i]\n",
    "        \n",
    "        # 自己相関の計算\n",
    "        lags = min(max_lags, len(param_samples) // 4)\n",
    "        autocorr = acf(param_samples, nlags=lags, fft=True)\n",
    "        \n",
    "        # 統合自己相関時間の計算\n",
    "        # τ_int = 1 + 2 * Σ(ρ(k)) for k where ρ(k) > 0\n",
    "        tau_int = 1.0\n",
    "        for k in range(1, len(autocorr)):\n",
    "            if autocorr[k] > 0.01:  # 閾値以上の相関がある間は加算\n",
    "                tau_int += 2 * autocorr[k]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # 有効サンプルサイズ\n",
    "        n_eff = len(param_samples) / (2 * tau_int + 1)\n",
    "        \n",
    "        autocorr_results[parameter_names[i]] = {\n",
    "            'tau_int': tau_int,\n",
    "            'n_eff': n_eff,\n",
    "            'autocorr': autocorr\n",
    "        }\n",
    "        \n",
    "        # 1. 自己相関関数プロット\n",
    "        axes[i, 0].plot(autocorr, 'b-', alpha=0.8)\n",
    "        axes[i, 0].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        axes[i, 0].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 0].set_title(f'{parameter_names[i]} - ACF\\nτ_int = {tau_int:.1f}, N_eff = {n_eff:.0f}')\n",
    "        axes[i, 0].set_xlabel('Lag')\n",
    "        axes[i, 0].set_ylabel('Autocorrelation')\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 対数スケールでの自己相関\n",
    "        positive_autocorr = np.maximum(autocorr, 1e-10)\n",
    "        axes[i, 1].semilogy(positive_autocorr, 'b-', alpha=0.8)\n",
    "        \n",
    "        # 指数的減衰のフィッティング\n",
    "        try:\n",
    "            # 最初の数点を使って指数的減衰をフィット\n",
    "            fit_range = min(50, len(autocorr) // 2)\n",
    "            x_fit = np.arange(fit_range)\n",
    "            y_fit = autocorr[:fit_range]\n",
    "            \n",
    "            # 線形回帰で指数的減衰の係数を推定\n",
    "            mask = y_fit > 0.01\n",
    "            if np.sum(mask) > 5:\n",
    "                coeffs = np.polyfit(x_fit[mask], np.log(y_fit[mask]), 1)\n",
    "                exp_fit = np.exp(coeffs[1] + coeffs[0] * x_fit)\n",
    "                axes[i, 1].plot(x_fit, exp_fit, 'r--', alpha=0.7, \n",
    "                               label=f'Exp fit: τ = {-1/coeffs[0]:.1f}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        axes[i, 1].set_title(f'{parameter_names[i]} - ACF (Log Scale)')\n",
    "        axes[i, 1].set_xlabel('Lag')\n",
    "        axes[i, 1].set_ylabel('Log Autocorrelation')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 薄化（thinning）の効果\n",
    "        thin_factors = [1, 2, 5, 10, 20]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(thin_factors)))\n",
    "        \n",
    "        for j, thin in enumerate(thin_factors):\n",
    "            if thin < len(param_samples) // 10:  # 十分なサンプルが残る場合のみ\n",
    "                thinned_samples = param_samples[::thin]\n",
    "                if len(thinned_samples) > 100:\n",
    "                    thin_lags = min(50, len(thinned_samples) // 4)\n",
    "                    thin_autocorr = acf(thinned_samples, nlags=thin_lags, fft=True)\n",
    "                    axes[i, 2].plot(thin_autocorr, color=colors[j], alpha=0.7, \n",
    "                                   label=f'Thin={thin}')\n",
    "        \n",
    "        axes[i, 2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[i, 2].set_title(f'{parameter_names[i]} - Effect of Thinning')\n",
    "        axes[i, 2].set_xlabel('Lag')\n",
    "        axes[i, 2].set_ylabel('Autocorrelation')\n",
    "        axes[i, 2].legend()\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return autocorr_results\n",
    "\n",
    "# 自己相関分析の実行\n",
    "autocorr_results = plot_autocorrelation_analysis(good_samples, parameter_names=['Normal Distribution'])\n",
    "\n",
    "print(\"自己相関分析結果:\")\n",
    "for param, results in autocorr_results.items():\n",
    "    print(f\"{param}:\")\n",
    "    print(f\"  統合自己相関時間: {results['tau_int']:.2f}\")\n",
    "    print(f\"  有効サンプルサイズ: {results['n_eff']:.0f}\")\n",
    "    print(f\"  効率: {results['n_eff']/len(good_samples):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 数値的診断統計量\n",
    "\n",
    "### 4.3.1 Gelman-Rubin統計量（$\\hat{R}$）\n",
    "\n",
    "複数のチェーンを使って収束を診断する最も重要な統計量です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_rubin_diagnostic(chains, split_chains=True):\n",
    "    \"\"\"\n",
    "    Gelman-Rubin収束診断統計量の計算\n",
    "    \n",
    "    Parameters:\n",
    "    - chains: shape (n_chains, n_samples) または (n_chains, n_samples, n_params)\n",
    "    - split_chains: 各チェーンを前半・後半に分割するか\n",
    "    \n",
    "    Returns:\n",
    "    - R_hat: R-hat統計量\n",
    "    - n_eff: 有効サンプルサイズ\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    \n",
    "    if chains.ndim == 2:\n",
    "        # 単一パラメータの場合\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if split_chains:\n",
    "        # 各チェーンを前半・後半に分割\n",
    "        mid_point = n_samples // 2\n",
    "        first_half = chains[:, :mid_point, :]\n",
    "        second_half = chains[:, mid_point:, :]\n",
    "        chains_split = np.concatenate([first_half, second_half], axis=0)\n",
    "        n_chains *= 2\n",
    "        n_samples = mid_point\n",
    "        chains = chains_split\n",
    "    \n",
    "    R_hat = np.zeros(n_params)\n",
    "    n_eff = np.zeros(n_params)\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        # 各チェーンの平均と分散\n",
    "        chain_means = np.mean(chains[:, :, p], axis=1)\n",
    "        chain_vars = np.var(chains[:, :, p], axis=1, ddof=1)\n",
    "        \n",
    "        # 全体平均\n",
    "        overall_mean = np.mean(chain_means)\n",
    "        \n",
    "        # チェーン間分散 B\n",
    "        B = n_samples * np.var(chain_means, ddof=1)\n",
    "        \n",
    "        # チェーン内分散 W\n",
    "        W = np.mean(chain_vars)\n",
    "        \n",
    "        # 分散の推定値\n",
    "        var_plus = ((n_samples - 1) * W + B) / n_samples\n",
    "        \n",
    "        # R-hat統計量\n",
    "        R_hat[p] = np.sqrt(var_plus / W) if W > 0 else np.inf\n",
    "        \n",
    "        # 有効サンプルサイズの計算\n",
    "        # 各チェーンの自己相関を考慮\n",
    "        all_samples = chains[:, :, p].flatten()\n",
    "        \n",
    "        # 自己相関時間の推定\n",
    "        try:\n",
    "            autocorr = acf(all_samples, nlags=min(200, len(all_samples)//4), fft=True)\n",
    "            tau_int = 1.0\n",
    "            for k in range(1, len(autocorr)):\n",
    "                if autocorr[k] > 0.01:\n",
    "                    tau_int += 2 * autocorr[k]\n",
    "                else:\n",
    "                    break\n",
    "            n_eff[p] = len(all_samples) / (2 * tau_int + 1)\n",
    "        except:\n",
    "            n_eff[p] = len(all_samples) / 10  # 保守的な推定\n",
    "    \n",
    "    if n_params == 1:\n",
    "        return R_hat[0], n_eff[0]\n",
    "    else:\n",
    "        return R_hat, n_eff\n",
    "\n",
    "def run_multiple_chains(target_log_pdf, initial_values, n_samples, step_size=1.0):\n",
    "    \"\"\"\n",
    "    複数のチェーンを並列実行\n",
    "    \"\"\"\n",
    "    n_chains = len(initial_values)\n",
    "    chains = np.zeros((n_chains, n_samples))\n",
    "    acceptance_rates = np.zeros(n_chains)\n",
    "    \n",
    "    for i, init_val in enumerate(initial_values):\n",
    "        samples, acc_rate = metropolis_hastings_simple(\n",
    "            target_log_pdf, init_val, n_samples, step_size\n",
    "        )\n",
    "        chains[i] = samples\n",
    "        acceptance_rates[i] = acc_rate\n",
    "    \n",
    "    return chains, acceptance_rates\n",
    "\n",
    "# 複数チェーンでの収束診断\n",
    "print(\"複数チェーンでのサンプリング実行中...\")\n",
    "\n",
    "# 正規分布での例\n",
    "initial_values_normal = [-2, -1, 0, 1, 2]\n",
    "chains_normal, acc_rates_normal = run_multiple_chains(\n",
    "    lambda x: stats.norm.logpdf(x, 0, 1), \n",
    "    initial_values_normal, \n",
    "    3000, \n",
    "    step_size=1.0\n",
    ")\n",
    "\n",
    "# 多峰性分布での例\n",
    "initial_values_multimodal = [-4, -2, 0, 2, 4]\n",
    "chains_multimodal, acc_rates_multimodal = run_multiple_chains(\n",
    "    multimodal_log_pdf,\n",
    "    initial_values_multimodal,\n",
    "    3000,\n",
    "    step_size=2.0\n",
    ")\n",
    "\n",
    "# Gelman-Rubin診断の実行\n",
    "R_hat_normal, n_eff_normal = gelman_rubin_diagnostic(chains_normal)\n",
    "R_hat_multimodal, n_eff_multimodal = gelman_rubin_diagnostic(chains_multimodal)\n",
    "\n",
    "print(f\"\\n=== Gelman-Rubin診断結果 ===\")\n",
    "print(f\"正規分布:\")\n",
    "print(f\"  R-hat: {R_hat_normal:.4f}\")\n",
    "print(f\"  有効サンプルサイズ: {n_eff_normal:.0f}\")\n",
    "print(f\"  平均受理率: {np.mean(acc_rates_normal):.3f}\")\n",
    "\n",
    "print(f\"\\n多峰性分布:\")\n",
    "print(f\"  R-hat: {R_hat_multimodal:.4f}\")\n",
    "print(f\"  有効サンプルサイズ: {n_eff_multimodal:.0f}\")\n",
    "print(f\"  平均受理率: {np.mean(acc_rates_multimodal):.3f}\")\n",
    "\n",
    "print(f\"\\n判定基準:\")\n",
    "print(f\"  R-hat < 1.01: 収束良好\")\n",
    "print(f\"  1.01 ≤ R-hat < 1.1: 注意が必要\")\n",
    "print(f\"  R-hat ≥ 1.1: 収束不良\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数チェーンの可視化\n",
    "def plot_multiple_chains(chains, title=\"Multiple Chains Analysis\", parameter_name=\"Parameter\", true_mean=None):\n",
    "    \"\"\"\n",
    "    複数チェーンの詳細分析プロット\n",
    "    \"\"\"\n",
    "    n_chains, n_samples = chains.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_chains))\n",
    "    \n",
    "    # 1. 全チェーンのトレースプロット\n",
    "    for i in range(n_chains):\n",
    "        axes[0, 0].plot(chains[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 0].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 0].set_title('All Chains - Trace Plot')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. チェーン別のヒストグラム\n",
    "    for i in range(n_chains):\n",
    "        burnin = n_samples // 4\n",
    "        axes[0, 1].hist(chains[i, burnin:], bins=30, alpha=0.6, \n",
    "                       color=colors[i], density=True, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 1].axvline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 1].set_title('Distribution by Chain')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. ランニング平均の収束\n",
    "    for i in range(n_chains):\n",
    "        running_mean = np.cumsum(chains[i]) / np.arange(1, n_samples + 1)\n",
    "        axes[0, 2].plot(running_mean, alpha=0.7, color=colors[i], \n",
    "                       linewidth=1, label=f'Chain {i+1}')\n",
    "    if true_mean is not None:\n",
    "        axes[0, 2].axhline(true_mean, color='red', linestyle='--', \n",
    "                          linewidth=2, label='True value')\n",
    "    axes[0, 2].set_title('Running Mean Convergence')\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Running Mean')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. チェーン間・チェーン内分散の変化\n",
    "    window_size = max(100, n_samples // 20)\n",
    "    n_windows = n_samples // window_size\n",
    "    \n",
    "    between_var = []\n",
    "    within_var = []\n",
    "    r_hat_evolution = []\n",
    "    \n",
    "    for w in range(1, n_windows + 1):\n",
    "        end_idx = w * window_size\n",
    "        window_chains = chains[:, :end_idx]\n",
    "        \n",
    "        if end_idx >= 200:  # 十分なサンプルがある場合のみ\n",
    "            try:\n",
    "                r_hat_w, _ = gelman_rubin_diagnostic(window_chains, split_chains=False)\n",
    "                r_hat_evolution.append(r_hat_w)\n",
    "                \n",
    "                # チェーン間・内分散の計算\n",
    "                chain_means = np.mean(window_chains, axis=1)\n",
    "                chain_vars = np.var(window_chains, axis=1, ddof=1)\n",
    "                B = end_idx * np.var(chain_means, ddof=1)\n",
    "                W = np.mean(chain_vars)\n",
    "                between_var.append(B)\n",
    "                within_var.append(W)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if len(r_hat_evolution) > 0:\n",
    "        axes[1, 0].plot(range(1, len(r_hat_evolution) + 1), r_hat_evolution, 'b-', linewidth=2)\n",
    "        axes[1, 0].axhline(1.0, color='green', linestyle='--', label='Perfect convergence')\n",
    "        axes[1, 0].axhline(1.01, color='orange', linestyle='--', label='Good convergence')\n",
    "        axes[1, 0].axhline(1.1, color='red', linestyle='--', label='Poor convergence')\n",
    "        axes[1, 0].set_title('R-hat Evolution')\n",
    "        axes[1, 0].set_xlabel('Window')\n",
    "        axes[1, 0].set_ylabel('R-hat')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. 自己相関比較\n",
    "    for i in range(min(3, n_chains)):  # 最初の3チェーンのみ表示\n",
    "        burnin = n_samples // 4\n",
    "        chain_data = chains[i, burnin:]\n",
    "        if len(chain_data) > 100:\n",
    "            lags = min(100, len(chain_data) // 4)\n",
    "            autocorr = acf(chain_data, nlags=lags, fft=True)\n",
    "            axes[1, 1].plot(autocorr, alpha=0.7, color=colors[i], \n",
    "                           label=f'Chain {i+1}')\n",
    "    axes[1, 1].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "    axes[1, 1].set_title('Autocorrelation Comparison')\n",
    "    axes[1, 1].set_xlabel('Lag')\n",
    "    axes[1, 1].set_ylabel('Autocorrelation')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Rank plot (chains の混合の確認)\n",
    "    all_samples = chains.flatten()\n",
    "    ranks = stats.rankdata(all_samples)\n",
    "    ranks = ranks.reshape(chains.shape)\n",
    "    \n",
    "    for i in range(n_chains):\n",
    "        axes[1, 2].plot(ranks[i], alpha=0.7, color=colors[i], \n",
    "                       linewidth=0.8, label=f'Chain {i+1}')\n",
    "    axes[1, 2].set_title('Rank Plot (Mixing Assessment)')\n",
    "    axes[1, 2].set_xlabel('Iteration')\n",
    "    axes[1, 2].set_ylabel('Rank')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 複数チェーンの分析\n",
    "print(\"正規分布の複数チェーン分析:\")\n",
    "plot_multiple_chains(chains_normal, \"Normal Distribution - Multiple Chains\", \"Value\", 0.0)\n",
    "\n",
    "print(\"多峰性分布の複数チェーン分析:\")\n",
    "plot_multiple_chains(chains_multimodal, \"Multimodal Distribution - Multiple Chains\", \"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 その他の診断統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_diagnostics(chains, parameter_names=None):\n",
    "    \"\"\"\n",
    "    包括的な診断統計量の計算\n",
    "    \"\"\"\n",
    "    chains = np.array(chains)\n",
    "    if chains.ndim == 2:\n",
    "        chains = chains[:, :, np.newaxis]\n",
    "    \n",
    "    n_chains, n_samples, n_params = chains.shape\n",
    "    \n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f'Parameter {i+1}' for i in range(n_params)]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for p in range(n_params):\n",
    "        param_name = parameter_names[p]\n",
    "        param_chains = chains[:, :, p]\n",
    "        \n",
    "        # 基本統計\n",
    "        all_samples = param_chains.flatten()\n",
    "        burnin = n_samples // 4\n",
    "        clean_samples = param_chains[:, burnin:].flatten()\n",
    "        \n",
    "        # 1. Gelman-Rubin統計量\n",
    "        R_hat, n_eff = gelman_rubin_diagnostic(param_chains)\n",
    "        \n",
    "        # 2. Monte Carlo Standard Error (MCSE)\n",
    "        # MCSE = σ / sqrt(N_eff)\n",
    "        mcse = np.std(clean_samples, ddof=1) / np.sqrt(n_eff)\n",
    "        \n",
    "        # 3. 分位点のMCSE\n",
    "        def mcse_quantile(samples, q):\n",
    "            \"\"\"分位点のMonte Carlo標準誤差\"\"\"\n",
    "            n = len(samples)\n",
    "            p = q\n",
    "            # 正規近似を使用\n",
    "            return np.sqrt(p * (1 - p) / n) / stats.norm.pdf(stats.norm.ppf(p))\n",
    "        \n",
    "        mcse_q025 = mcse_quantile(clean_samples, 0.025)\n",
    "        mcse_q975 = mcse_quantile(clean_samples, 0.975)\n",
    "        \n",
    "        # 4. Geweke診断（前半と後半の比較）\n",
    "        first_10pct = int(0.1 * n_samples)\n",
    "        last_50pct = int(0.5 * n_samples)\n",
    "        \n",
    "        geweke_scores = []\n",
    "        for chain in range(n_chains):\n",
    "            first_part = param_chains[chain, :first_10pct]\n",
    "            last_part = param_chains[chain, -last_50pct:]\n",
    "            \n",
    "            if len(first_part) > 10 and len(last_part) > 10:\n",
    "                mean_diff = np.mean(first_part) - np.mean(last_part)\n",
    "                \n",
    "                # スペクトル密度による分散推定（簡易版）\n",
    "                var_first = np.var(first_part, ddof=1) / len(first_part)\n",
    "                var_last = np.var(last_part, ddof=1) / len(last_part)\n",
    "                \n",
    "                geweke_score = mean_diff / np.sqrt(var_first + var_last)\n",
    "                geweke_scores.append(geweke_score)\n",
    "        \n",
    "        geweke_pvalue = 2 * (1 - stats.norm.cdf(np.abs(np.mean(geweke_scores))))\n",
    "        \n",
    "        # 5. Heidelberger-Welch 検定（簡易版）\n",
    "        # 定常性の検定\n",
    "        hw_pvalues = []\n",
    "        for chain in range(n_chains):\n",
    "            chain_data = param_chains[chain]\n",
    "            \n",
    "            # チェーンを複数の窓に分割して平均の違いを検定\n",
    "            n_windows = 5\n",
    "            window_size = len(chain_data) // n_windows\n",
    "            window_means = []\n",
    "            \n",
    "            for w in range(n_windows):\n",
    "                start_idx = w * window_size\n",
    "                end_idx = (w + 1) * window_size\n",
    "                if end_idx <= len(chain_data):\n",
    "                    window_means.append(np.mean(chain_data[start_idx:end_idx]))\n",
    "            \n",
    "            if len(window_means) > 2:\n",
    "                # 一元配置分散分析\n",
    "                _, p_val = stats.f_oneway(*[chain_data[i*window_size:(i+1)*window_size] \n",
    "                                           for i in range(len(window_means))])\n",
    "                hw_pvalues.append(p_val)\n",
    "        \n",
    "        hw_pvalue = np.mean(hw_pvalues) if hw_pvalues else np.nan\n",
    "        \n",
    "        # 6. ESS (Effective Sample Size) の詳細計算\n",
    "        # バッチ法による推定も追加\n",
    "        batch_sizes = [10, 20, 50, 100]\n",
    "        batch_ess = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            if len(clean_samples) > batch_size * 10:\n",
    "                n_batches = len(clean_samples) // batch_size\n",
    "                batches = clean_samples[:n_batches * batch_size].reshape(n_batches, batch_size)\n",
    "                batch_means = np.mean(batches, axis=1)\n",
    "                \n",
    "                # バッチ平均の分散\n",
    "                batch_var = np.var(batch_means, ddof=1)\n",
    "                total_var = np.var(clean_samples, ddof=1)\n",
    "                \n",
    "                if batch_var > 0:\n",
    "                    ess_batch = len(clean_samples) * total_var / (batch_size * batch_var)\n",
    "                    batch_ess.append(ess_batch)\n",
    "        \n",
    "        avg_batch_ess = np.mean(batch_ess) if batch_ess else n_eff\n",
    "        \n",
    "        results[param_name] = {\n",
    "            'mean': np.mean(clean_samples),\n",
    "            'std': np.std(clean_samples, ddof=1),\n",
    "            'q025': np.percentile(clean_samples, 2.5),\n",
    "            'q975': np.percentile(clean_samples, 97.5),\n",
    "            'R_hat': R_hat,\n",
    "            'n_eff': n_eff,\n",
    "            'n_eff_batch': avg_batch_ess,\n",
    "            'mcse': mcse,\n",
    "            'mcse_q025': mcse_q025,\n",
    "            'mcse_q975': mcse_q975,\n",
    "            'geweke_score': np.mean(geweke_scores) if geweke_scores else np.nan,\n",
    "            'geweke_pvalue': geweke_pvalue,\n",
    "            'hw_pvalue': hw_pvalue,\n",
    "            'n_samples_total': len(all_samples),\n",
    "            'n_samples_clean': len(clean_samples),\n",
    "            'efficiency': n_eff / len(clean_samples)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_diagnostics_table(diagnostics):\n",
    "    \"\"\"\n",
    "    診断結果の表形式出力\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MCMC診断統計量サマリー\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # ヘッダー\n",
    "    header = f\"{'Parameter':<15} {'Mean':<8} {'Std':<8} {'R-hat':<8} {'N_eff':<8} {'MCSE':<8} {'Geweke':<8} {'Status':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for param_name, stats in diagnostics.items():\n",
    "        # 収束ステータスの判定\n",
    "        status = \"Good\"\n",
    "        if stats['R_hat'] > 1.1:\n",
    "            status = \"Poor\"\n",
    "        elif stats['R_hat'] > 1.01:\n",
    "            status = \"Caution\"\n",
    "        \n",
    "        if stats['n_eff'] < 100:\n",
    "            status += \"/Low ESS\"\n",
    "        \n",
    "        row = (f\"{param_name:<15} {stats['mean']:<8.3f} {stats['std']:<8.3f} \"\n",
    "               f\"{stats['R_hat']:<8.4f} {stats['n_eff']:<8.0f} {stats['mcse']:<8.4f} \"\n",
    "               f\"{stats['geweke_pvalue']:<8.3f} {status:<12}\")\n",
    "        print(row)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"判定基準:\")\n",
    "    print(\"  R-hat < 1.01: Good, 1.01-1.1: Caution, > 1.1: Poor\")\n",
    "    print(\"  Geweke: p-value > 0.05 で定常性仮説を棄却しない\")\n",
    "    print(\"  N_eff: 有効サンプルサイズ (目安: > 100)\")\n",
    "\n",
    "# 診断統計量の計算と表示\n",
    "print(\"診断統計量の計算中...\")\n",
    "\n",
    "# 正規分布の診断\n",
    "diagnostics_normal = comprehensive_diagnostics(chains_normal, ['Normal'])\n",
    "print(\"\\n正規分布の診断結果:\")\n",
    "print_diagnostics_table(diagnostics_normal)\n",
    "\n",
    "# 多峰性分布の診断\n",
    "diagnostics_multimodal = comprehensive_diagnostics(chains_multimodal, ['Multimodal'])\n",
    "print(\"\\n多峰性分布の診断結果:\")\n",
    "print_diagnostics_table(diagnostics_multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 実践的な診断手順\n",
    "\n",
    "実際のMCMC分析で推奨される診断手順をまとめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_diagnostic_workflow(target_log_pdf, initial_values, n_samples, \n",
    "                           step_size=1.0, parameter_names=None, \n",
    "                           true_values=None, verbose=True):\n",
    "    \"\"\"\n",
    "    MCMC診断の標準ワークフロー\n",
    "    \n",
    "    Returns:\n",
    "    - diagnostics: 診断結果の辞書\n",
    "    - recommendations: 推奨事項のリスト\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"MCMC診断ワークフロー開始...\")\n",
    "        print(f\"チェーン数: {len(initial_values)}\")\n",
    "        print(f\"サンプル数: {n_samples}\")\n",
    "        print(f\"ステップサイズ: {step_size}\")\n",
    "    \n",
    "    # Step 1: 複数チェーンの実行\n",
    "    if verbose:\n",
    "        print(\"\\nStep 1: 複数チェーンでのサンプリング実行中...\")\n",
    "    \n",
    "    chains, acceptance_rates = run_multiple_chains(\n",
    "        target_log_pdf, initial_values, n_samples, step_size\n",
    "    )\n",
    "    \n",
    "    # Step 2: 基本統計の確認\n",
    "    if verbose:\n",
    "        print(\"\\nStep 2: 基本統計の確認\")\n",
    "        print(f\"平均受理率: {np.mean(acceptance_rates):.3f} (範囲: {np.min(acceptance_rates):.3f} - {np.max(acceptance_rates):.3f})\")\n",
    "    \n",
    "    # Step 3: 視覚的診断\n",
    "    if verbose:\n",
    "        print(\"\\nStep 3: 視覚的診断\")\n",
    "    \n",
    "    plot_multiple_chains(chains, \"Diagnostic Workflow - Visual Inspection\", \n",
    "                        parameter_name=\"Parameter\", \n",
    "                        true_mean=true_values[0] if true_values else None)\n",
    "    \n",
    "    # Step 4: 数値診断\n",
    "    if verbose:\n",
    "        print(\"\\nStep 4: 数値診断統計量の計算\")\n",
    "    \n",
    "    diagnostics = comprehensive_diagnostics(chains, parameter_names)\n",
    "    print_diagnostics_table(diagnostics)\n",
    "    \n",
    "    # Step 5: 推奨事項の生成\n",
    "    recommendations = []\n",
    "    \n",
    "    for param_name, stats in diagnostics.items():\n",
    "        # 受理率の確認\n",
    "        avg_acceptance = np.mean(acceptance_rates)\n",
    "        if avg_acceptance < 0.2:\n",
    "            recommendations.append(f\"{param_name}: 受理率が低すぎます ({avg_acceptance:.3f}). ステップサイズを小さくしてください.\")\n",
    "        elif avg_acceptance > 0.7:\n",
    "            recommendations.append(f\"{param_name}: 受理率が高すぎます ({avg_acceptance:.3f}). ステップサイズを大きくしてください.\")\n",
    "        \n",
    "        # R-hat の確認\n",
    "        if stats['R_hat'] > 1.1:\n",
    "            recommendations.append(f\"{param_name}: R-hat = {stats['R_hat']:.4f} > 1.1. 収束していません. より多くのサンプルまたは異なる初期値を試してください.\")\n",
    "        elif stats['R_hat'] > 1.01:\n",
    "            recommendations.append(f\"{param_name}: R-hat = {stats['R_hat']:.4f} > 1.01. 注意が必要です. 追加のサンプルを検討してください.\")\n",
    "        \n",
    "        # 有効サンプルサイズの確認\n",
    "        if stats['n_eff'] < 100:\n",
    "            recommendations.append(f\"{param_name}: 有効サンプルサイズが小さすぎます ({stats['n_eff']:.0f}). より長いチェーンまたは薄化を検討してください.\")\n",
    "        elif stats['n_eff'] < 400:\n",
    "            recommendations.append(f\"{param_name}: 有効サンプルサイズがやや小さいです ({stats['n_eff']:.0f}). 信頼区間の精度に注意してください.\")\n",
    "        \n",
    "        # MCSE の確認\n",
    "        relative_mcse = stats['mcse'] / stats['std']\n",
    "        if relative_mcse > 0.1:\n",
    "            recommendations.append(f\"{param_name}: Monte Carlo標準誤差が大きいです (相対誤差: {relative_mcse:.3f}). より多くのサンプルが必要です.\")\n",
    "        \n",
    "        # Geweke 検定\n",
    "        if not np.isnan(stats['geweke_pvalue']) and stats['geweke_pvalue'] < 0.05:\n",
    "            recommendations.append(f\"{param_name}: Geweke検定で非定常性が検出されました (p = {stats['geweke_pvalue']:.3f}). バーンイン期間を長くしてください.\")\n",
    "    \n",
    "    # 一般的な推奨事項\n",
    "    total_samples = len(initial_values) * n_samples\n",
    "    total_eff = sum([stats['n_eff'] for stats in diagnostics.values()])\n",
    "    \n",
    "    overall_efficiency = total_eff / total_samples\n",
    "    if overall_efficiency < 0.1:\n",
    "        recommendations.append(f\"全体的な効率が低いです ({overall_efficiency:.3f}). アルゴリズムまたはパラメタリゼーションの見直しを検討してください.\")\n",
    "    \n",
    "    # Step 6: 推奨事項の表示\n",
    "    if verbose:\n",
    "        print(\"\\nStep 6: 推奨事項\")\n",
    "        if recommendations:\n",
    "            print(\"以下の改善を検討してください:\")\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"{i}. {rec}\")\n",
    "        else:\n",
    "            print(\"診断結果は良好です！\")\n",
    "    \n",
    "    return {\n",
    "        'chains': chains,\n",
    "        'acceptance_rates': acceptance_rates,\n",
    "        'diagnostics': diagnostics,\n",
    "        'recommendations': recommendations,\n",
    "        'overall_efficiency': overall_efficiency\n",
    "    }\n",
    "\n",
    "# ワークフローの実行例\n",
    "print(\"=== 正規分布での診断ワークフロー ===\")\n",
    "results_normal = mcmc_diagnostic_workflow(\n",
    "    target_log_pdf=lambda x: stats.norm.logpdf(x, 0, 1),\n",
    "    initial_values=[-1, 0, 1],\n",
    "    n_samples=2000,\n",
    "    step_size=1.0,\n",
    "    parameter_names=['Normal'],\n",
    "    true_values=[0.0]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== 多峰性分布での診断ワークフロー ===\")\n",
    "results_multimodal = mcmc_diagnostic_workflow(\n",
    "    target_log_pdf=multimodal_log_pdf,\n",
    "    initial_values=[-4, 0, 4],\n",
    "    n_samples=2000,\n",
    "    step_size=2.0,\n",
    "    parameter_names=['Multimodal']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 演習問題\n",
    "\n",
    "### 問題1：収束診断の実践\n",
    "以下の困難な分布に対してMCMCを実行し、収束診断を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題1: 困難な分布からのサンプリング\n",
    "def challenging_log_pdf(x):\n",
    "    \"\"\"\n",
    "    挑戦的な分布：高い相関を持つ2変量分布\n",
    "    \"\"\"\n",
    "    if len(x) != 2:\n",
    "        return -np.inf\n",
    "    \n",
    "    # 非常に細長い分布（高相関）\n",
    "    mu = np.array([0, 0])\n",
    "    cov = np.array([[1, 0.99], [0.99, 1]])\n",
    "    \n",
    "    diff = x - mu\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        log_det = 2 * np.sum(np.log(np.diag(chol)))\n",
    "        solve = np.linalg.solve(chol, diff)\n",
    "        mahalanobis_sq = np.sum(solve**2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf\n",
    "    \n",
    "    return -0.5 * (2 * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n",
    "\n",
    "# ここで2変量MHサンプリングを実装し、診断してください\n",
    "# ヒント：\n",
    "# 1. 複数の初期値から開始\n",
    "# 2. 異なるステップサイズを試す\n",
    "# 3. 収束診断を実行\n",
    "# 4. 問題点を特定し、改善策を提案\n",
    "\n",
    "pass  # 学習者が実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題2：適応的診断\n",
    "リアルタイムで収束を監視し、自動的に停止条件を判定するシステムを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題2: 適応的収束診断\n",
    "def adaptive_mcmc_with_diagnostics(target_log_pdf, initial_values, \n",
    "                                 max_samples=10000, check_interval=500,\n",
    "                                 r_hat_threshold=1.01, min_eff_samples=1000):\n",
    "    \"\"\"\n",
    "    適応的収束診断付きMCMC\n",
    "    \n",
    "    Parameters:\n",
    "    - target_log_pdf: 目標分布\n",
    "    - initial_values: 初期値のリスト\n",
    "    - max_samples: 最大サンプル数\n",
    "    - check_interval: 診断をチェックする間隔\n",
    "    - r_hat_threshold: R-hat の収束閾値\n",
    "    - min_eff_samples: 最小有効サンプル数\n",
    "    \n",
    "    実装のヒント:\n",
    "    1. check_interval ごとに R-hat と有効サンプルサイズを計算\n",
    "    2. 収束条件を満たしたら早期停止\n",
    "    3. 収束の履歴をプロット\n",
    "    \"\"\"\n",
    "    # ここに実装してください\n",
    "    pass  # 学習者が実装\n",
    "\n",
    "# テスト\n",
    "# result = adaptive_mcmc_with_diagnostics(\n",
    "#     lambda x: stats.norm.logpdf(x, 0, 1),\n",
    "#     [-2, -1, 0, 1, 2]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "この章では、MCMCの収束診断と性能評価について包括的に学習しました：\n",
    "\n",
    "### 重要な診断手法\n",
    "\n",
    "1. **視覚的診断**：\n",
    "   - トレースプロット：混合と収束の確認\n",
    "   - ランニング平均：収束の安定性\n",
    "   - 密度プロット：時間窓別の分布比較\n",
    "\n",
    "2. **数値診断**：\n",
    "   - **Gelman-Rubin統計量（R-hat）**：< 1.01 で良好\n",
    "   - **有効サンプルサイズ（ESS）**：> 100 が目安、理想的には > 400\n",
    "   - **Monte Carlo標準誤差（MCSE）**：推定精度の指標\n",
    "   - **Geweke診断**：定常性の検定\n",
    "\n",
    "3. **自己相関分析**：\n",
    "   - 統合自己相関時間：効率の指標\n",
    "   - 薄化（thinning）の効果\n",
    "   - 指数的減衰のフィッティング\n",
    "\n",
    "### 実践的な診断手順\n",
    "\n",
    "1. **複数チェーンの実行**：異なる初期値から開始\n",
    "2. **視覚的検査**：トレースプロットとヒストグラム\n",
    "3. **数値診断**：R-hat、ESS、MCMCの計算\n",
    "4. **問題の特定**：収束不良の原因分析\n",
    "5. **改善策の実施**：パラメータ調整や手法変更\n",
    "\n",
    "### 一般的な問題と対策\n",
    "\n",
    "| 問題 | 症状 | 対策 |\n",
    "|------|------|------|\n",
    "| 収束不良 | R-hat > 1.1 | より多くのサンプル、異なる初期値 |\n",
    "| 混合不良 | 高い自己相関 | ステップサイズ調整、異なるアルゴリズム |\n",
    "| 効率低下 | 低いESS | 薄化、ブロック更新、HMC |\n",
    "| 多峰性 | モード間移動なし | 長いチェーン、並列焼きなまし |\n",
    "\n",
    "### ベストプラクティス\n",
    "\n",
    "- **複数チェーン**：最低3-4チェーン、できれば8-10チェーン\n",
    "- **バーンイン**：総サンプル数の25-50%\n",
    "- **診断の自動化**：定期的なチェックと早期停止\n",
    "- **保守的な判定**：R-hat < 1.01、ESS > 400を目標\n",
    "- **可視化重視**：数値だけでなく必ず視覚的確認\n",
    "\n",
    "次の章では、これまで学んだMCMC手法を実際のベイズ推論問題に適用する実践例を学習します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}