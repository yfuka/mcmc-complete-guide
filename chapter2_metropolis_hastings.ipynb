{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 2: ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³• - ç›´æ„Ÿçš„ç†è§£ã‹ã‚‰å®Ÿè£…ã¾ã§\n\n## å­¦ç¿’ç›®æ¨™\n- ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã®**é©æ–°çš„ã‚¢ã‚¤ãƒ‡ã‚¢**ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹\n- å—ç†ç¢ºç‡ã®å°å‡ºã¨**æ•°å­¦çš„ç¾ã—ã•**ã‚’ä½“æ„Ÿã™ã‚‹\n- ææ¡ˆåˆ†å¸ƒã®é¸æŠã¨**æ¢ç´¢åŠ¹ç‡**ã¸ã®å½±éŸ¿ã‚’å®šé‡çš„ã«è©•ä¾¡ã§ãã‚‹\n- å®Ÿè£…ã¨æ€§èƒ½è©•ä¾¡æ–¹æ³•ã‚’ç¿’å¾—ã—ã€**å®Ÿè·µçš„è¨ºæ–­ã‚¹ã‚­ãƒ«**ã‚’èº«ã«ã¤ã‘ã‚‹\n- æ§˜ã€…ãªåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¾‹ã‚’é€šã˜ã¦**æ±ç”¨æ€§**ã‚’ä½“é¨“ã™ã‚‹\n\n## ğŸš€ ãªãœMHæ³•ã¯é©å‘½çš„ãªã®ã‹ï¼Ÿ\n\n### å¾“æ¥ã®æ•°å€¤ç©åˆ†ã¨ã®æ±ºå®šçš„é•ã„\n\n**å¾“æ¥ã®æ•°å€¤ç©åˆ†**:\n- æ ¼å­ç‚¹ã‚’è¦å‰‡çš„ã«é…ç½®ã—ã¦f(x)ã‚’è©•ä¾¡\n- æ¬¡å…ƒãŒå¢—ãˆã‚‹ã¨æ ¼å­ç‚¹æ•°ãŒæŒ‡æ•°çš„ã«å¢—åŠ ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰\n- 10æ¬¡å…ƒã§å„è»¸100ç‚¹â†’10^20 æ ¼å­ç‚¹ï¼ˆç¾å®Ÿçš„ã«ä¸å¯èƒ½ï¼‰\n\n**MHæ³•ã®é©æ–°**:\n- **é©å¿œçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: é‡è¦ãªé ˜åŸŸã«ã‚ˆã‚Šå¤šãã®ç‚¹ã‚’é…ç½®\n- **ç¢ºç‡çš„æ¢ç´¢**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªç©ºé–“æ¢ç´¢\n- **è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶**: ç›®æ¨™åˆ†å¸ƒã«ç¢ºå®Ÿã«åæŸã™ã‚‹ç†è«–ä¿è¨¼\n\n### ã€Œè³¢ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€ã¨ã—ã¦ã®ç›´æ„Ÿ\n\nMHæ³•ã¯ã€**ç¢ºç‡åœ°å½¢å›³**ã‚’è¦‹ãªãŒã‚‰æ­©ãè³¢ã„ãƒã‚¤ã‚«ãƒ¼ã§ã™ï¼š\n\n- ğŸ”ï¸ **é«˜ã„ç¢ºç‡ã®é ˜åŸŸ**ï¼ˆå±±é ‚ï¼‰: ç©æ¥µçš„ã«å‘ã‹ã†\n- ğŸœï¸ **ä½ã„ç¢ºç‡ã®é ˜åŸŸ**ï¼ˆè°·ï¼‰: æ™‚ã€…è¨ªã‚Œã‚‹ãŒçŸ­æ™‚é–“\n- ğŸ¯ **ãƒãƒ©ãƒ³ã‚¹**: å„é ˜åŸŸã®æ»åœ¨æ™‚é–“ âˆ ãã®é ˜åŸŸã®ç¢ºç‡\n\nã“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹ã®ãŒ**å—ç†ç¢ºç‡Î±**ã®å·§å¦™ãªè¨­è¨ˆã§ã™ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ï¼šMCMCã®æœ€ã‚‚åŸºæœ¬çš„ã§æ±ç”¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n\nãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ï¼ˆMHæ³•ï¼‰ã¯ã€MCMCã®ä¸­ã§ã‚‚æœ€ã‚‚åŸºæœ¬çš„ã‹ã¤æ±ç”¨æ€§ã®é«˜ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã¨ã„ã†å¼·åŠ›ãªè¨­è¨ˆæŒ‡é‡ã‚’å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦å®Ÿè£…ã—ãŸã€çœŸã«é©å‘½çš„ãªæ‰‹æ³•ã§ã™ã€‚\n\n### MHæ³•ã®æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢\n\nMHæ³•ãŒç”»æœŸçš„ã§ã‚ã‚‹ç†ç”±ã¯ã€ãã®å·§å¦™ãªè¨­è¨ˆã«ã‚ã‚Šã¾ã™ï¼š\n\n1. **æ­£è¦åŒ–å®šæ•°ã®ç›¸æ®º**: ç›®æ¨™åˆ†å¸ƒã®æ¯”ç‡ã‚’ä½¿ã†ã“ã¨ã§ã€è¨ˆç®—å›°é›£ãªæ­£è¦åŒ–å®šæ•°ãŒç›¸æ®ºã•ã‚Œã‚‹\n2. **è©³ç´°é‡£ã‚Šåˆã„ã®å……è¶³**: æ¡æŠ/æ£„å´ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒè©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’è‡ªå‹•çš„ã«æº€ãŸã™\n3. **æ±ç”¨æ€§**: ç›®æ¨™åˆ†å¸ƒã®å¯†åº¦é–¢æ•°ï¼ˆæ­£è¦åŒ–ä¸è¦ï¼‰ãŒè¨ˆç®—ã§ãã‚Œã°é©ç”¨å¯èƒ½\n\n### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æµã‚Œ\n\n1. **åˆæœŸåŒ–**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ $x^{(t)}$ ã‚’è¨­å®š\n2. **ææ¡ˆ**: ææ¡ˆåˆ†å¸ƒ $q(x'|x^{(t)})$ ã‹ã‚‰æ¬¡ã®çŠ¶æ…‹å€™è£œ $x'$ ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n3. **å—ç†ç¢ºç‡ã®è¨ˆç®—**:\n   $$\\alpha = \\min\\left(1, \\frac{p(x')q(x^{(t)}|x')}{p(x^{(t)})q(x'|x^{(t)})}\\right)$$\n4. **æ¡æŠ/æ£„å´**: ç¢ºç‡ $\\alpha$ ã§ææ¡ˆã‚’æ¡æŠã€ãã†ã§ãªã‘ã‚Œã°ç¾åœ¨ã®çŠ¶æ…‹ã«ç•™ã¾ã‚‹\n5. **ç¹°ã‚Šè¿”ã—**: ã‚¹ãƒ†ãƒƒãƒ—2ã«æˆ»ã‚‹\n\n### ã€Œè³¢ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€ã¨ã—ã¦ã®ç›´æ„Ÿ\n\nMHæ³•ã¯ã€ç›®æ¨™åˆ†å¸ƒã¨ã„ã†ã€Œåœ°å½¢å›³ã€ã‚’å¸¸ã«è¦‹ãªãŒã‚‰æ­©ãã€è³¢ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã¨è¦‹ãªã›ã¾ã™ï¼š\n- **ç¢ºç‡ã®é«˜ã„å ´æ‰€**ï¼ˆæ¨™é«˜ã®é«˜ã„å ´æ‰€ï¼‰ã¸ã¯ç©æ¥µçš„ã«ç§»å‹•ï¼ˆé«˜ã„æ¡æŠç¢ºç‡ï¼‰\n- **ç¢ºç‡ã®ä½ã„å ´æ‰€**ï¼ˆæ¨™é«˜ã®ä½ã„å ´æ‰€ï¼‰ã¸ã¯ãŸã‚ã‚‰ã„ãªãŒã‚‰ç§»å‹•ï¼ˆä½ã„æ¡æŠç¢ºç‡ï¼‰\n\nã“ã®ã€Œè³¢ã•ã€ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã®ãŒæ¡æŠç¢ºç‡ $\\alpha$ ã§ã™ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 MHæ³•ã®æœ€ã‚‚å·§å¦™ãªç‚¹ï¼šæ­£è¦åŒ–å®šæ•°ã®ç›¸æ®º\n\n### ãƒ™ã‚¤ã‚ºæ¨è«–ã«ãŠã‘ã‚‹å•é¡Œã®å†ç¢ºèª\n\nãƒ™ã‚¤ã‚ºæ¨è«–ã§å›°ã£ã¦ã„ãŸã®ã¯ã€äº‹å¾Œåˆ†å¸ƒã®æ­£è¦åŒ–å®šæ•°ãŒè¨ˆç®—ã§ããªã„ã“ã¨ã§ã—ãŸï¼š\n\n$$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{p(X)} = \\frac{1}{C} \\cdot f(\\theta)$$\n\nã“ã“ã§ï¼š\n- $f(\\theta) = p(X|\\theta)p(\\theta)$ï¼ˆè¨ˆç®—å¯èƒ½ï¼‰\n- $C = p(X) = \\int p(X|\\theta)p(\\theta)d\\theta$ï¼ˆè¨ˆç®—å›°é›£ï¼‰\n\n### MHæ³•ã«ã‚ˆã‚‹è¦‹äº‹ãªè§£æ±º\n\nMHæ³•ã®æ¡æŠç¢ºç‡ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š\n\n$$\\alpha = \\min\\left(1, \\frac{p(\\theta')q(\\theta^{(t)}|\\theta')}{p(\\theta^{(t)})q(\\theta'|\\theta^{(t)})}\\right)$$\n\næ¯”ç‡ $\\frac{p(\\theta')}{p(\\theta^{(t)})}$ ã‚’è¨ˆç®—ã™ã‚‹ã¨ï¼š\n\n$$\\frac{p(\\theta')}{p(\\theta^{(t)})} = \\frac{\\frac{1}{C} \\cdot f(\\theta')}{\\frac{1}{C} \\cdot f(\\theta^{(t)})} = \\frac{f(\\theta')}{f(\\theta^{(t)})}$$\n\n**æœªçŸ¥ã®æ­£è¦åŒ–å®šæ•° $C$ ãŒåˆ†å­ã¨åˆ†æ¯ã§è¦‹äº‹ã«ç›¸æ®ºã•ã‚Œã¾ã™ï¼**\n\n### ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨\n\n1. **äº‹å¾Œåˆ†å¸ƒã®æ­£ç¢ºãªå¼ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é€²ã‚ã‚‰ã‚Œã‚‹\n2. **è¨ˆç®—å›°é›£ãªç©åˆ†ã‚’å›é¿**ã§ãã‚‹\n3. **æ¯”ä¾‹é–¢ä¿‚** $p(\\theta) \\propto f(\\theta)$ ã ã‘åˆ†ã‹ã‚Œã°ååˆ†\n\nã“ã‚ŒãŒã€MHæ³•ãŒã€Œç©åˆ†ã®å£ã€ã‚’å›é¿ã§ãã‚‹æœ€å¤§ã®ç†ç”±ã§ã™ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 åŸºæœ¬å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(target_log_pdf, proposal_sampler, proposal_log_pdf, \n",
    "                       initial_value, n_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã®æ±ç”¨å®Ÿè£…\n",
    "    \n",
    "    Parameters:\n",
    "    - target_log_pdf: ç›®æ¨™åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦é–¢æ•°\n",
    "    - proposal_sampler: ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼é–¢æ•° (current_state) -> proposed_state\n",
    "    - proposal_log_pdf: ææ¡ˆåˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦é–¢æ•° (proposed, current) -> log_q\n",
    "    - initial_value: åˆæœŸå€¤\n",
    "    - n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    - verbose: è©³ç´°æƒ…å ±ã®è¡¨ç¤º\n",
    "    \n",
    "    Returns:\n",
    "    - samples: ã‚µãƒ³ãƒ—ãƒ«é…åˆ—\n",
    "    - acceptance_rate: å—ç†ç‡\n",
    "    - log_probs: å„ã‚µãƒ³ãƒ—ãƒ«ã®å¯¾æ•°ç¢ºç‡\n",
    "    \"\"\"\n",
    "    # åˆæœŸåŒ–\n",
    "    if np.isscalar(initial_value):\n",
    "        samples = np.zeros(n_samples)\n",
    "        dim = 1\n",
    "    else:\n",
    "        samples = np.zeros((n_samples, len(initial_value)))\n",
    "        dim = len(initial_value)\n",
    "    \n",
    "    current = np.copy(initial_value)\n",
    "    current_log_prob = target_log_pdf(current)\n",
    "    n_accepted = 0\n",
    "    log_probs = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # æ–°ã—ã„çŠ¶æ…‹ã‚’ææ¡ˆ\n",
    "        proposed = proposal_sampler(current)\n",
    "        proposed_log_prob = target_log_pdf(proposed)\n",
    "        \n",
    "        # å—ç†ç¢ºç‡ã‚’è¨ˆç®—ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§å®‰å…¨ã«è¨ˆç®—ï¼‰\n",
    "        log_alpha = (proposed_log_prob + proposal_log_pdf(current, proposed) - \n",
    "                    current_log_prob - proposal_log_pdf(proposed, current))\n",
    "        alpha = min(1.0, np.exp(log_alpha))\n",
    "        \n",
    "        # å—ç†/æ£„å´ã‚’æ±ºå®š\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposed\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        if dim == 1:\n",
    "            samples[i] = current\n",
    "        else:\n",
    "            samples[i] = current\n",
    "        log_probs[i] = current_log_prob\n",
    "        \n",
    "        if verbose and (i + 1) % (n_samples // 10) == 0:\n",
    "            print(f\"Progress: {i+1}/{n_samples}, Acceptance Rate: {n_accepted/(i+1):.3f}\")\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_samples\n",
    "    return samples, acceptance_rate, log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ä¾‹1ï¼šæ··åˆæ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "ã¾ãšã€1æ¬¡å…ƒã®æ··åˆæ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··åˆæ­£è¦åˆ†å¸ƒã®å®šç¾©\n",
    "def mixture_log_pdf(x):\n",
    "    \"\"\"2ã¤ã®æ­£è¦åˆ†å¸ƒã®æ··åˆã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    component1 = stats.norm.logpdf(x, -2, 0.5)\n",
    "    component2 = stats.norm.logpdf(x, 2, 1.0)\n",
    "    # log(0.3 * exp(component1) + 0.7 * exp(component2))\n",
    "    max_comp = np.maximum(component1, component2)\n",
    "    return max_comp + np.log(0.3 * np.exp(component1 - max_comp) + \n",
    "                            0.7 * np.exp(component2 - max_comp))\n",
    "\n",
    "# å¯¾ç§°ãªææ¡ˆåˆ†å¸ƒï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰\n",
    "def random_walk_sampler(current, step_size=0.5):\n",
    "    return current + np.random.normal(0, step_size)\n",
    "\n",
    "def symmetric_proposal_log_pdf(proposed, current):\n",
    "    return 0.0  # å¯¾ç§°ãªææ¡ˆåˆ†å¸ƒã®å ´åˆã€æ¯”ã¯1ï¼ˆå¯¾æ•°ã§0ï¼‰\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "samples, acceptance_rate, log_probs = metropolis_hastings(\n",
    "    target_log_pdf=mixture_log_pdf,\n",
    "    proposal_sampler=lambda x: random_walk_sampler(x, 0.8),\n",
    "    proposal_log_pdf=symmetric_proposal_log_pdf,\n",
    "    initial_value=0.0,\n",
    "    n_samples=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\næœ€çµ‚å—ç†ç‡: {acceptance_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã®å¯è¦–åŒ–\n",
    "def plot_mcmc_results_1d(samples, target_log_pdf, burnin=1000, title=\"MCMC Results\"):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    axes[0, 0].plot(samples[:2000], alpha=0.7, linewidth=0.8)\n",
    "    axes[0, 0].set_title('Trace Plot (first 2000 samples)')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨çœŸã®åˆ†å¸ƒã®æ¯”è¼ƒ\n",
    "    axes[0, 1].hist(samples[burnin:], bins=60, density=True, alpha=0.7, \n",
    "                    color='skyblue', label='MCMC samples')\n",
    "    x_range = np.linspace(samples.min(), samples.max(), 1000)\n",
    "    true_density = np.exp(target_log_pdf(x_range))\n",
    "    axes[0, 1].plot(x_range, true_density, 'r-', linewidth=2, label='True distribution')\n",
    "    axes[0, 1].set_title('Sample Distribution vs True Distribution')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # è‡ªå·±ç›¸é–¢é–¢æ•°\n",
    "    from statsmodels.tsa.stattools import acf\n",
    "    lags = min(200, len(samples[burnin:]) // 10)\n",
    "    autocorr = acf(samples[burnin:], nlags=lags, fft=True)\n",
    "    axes[0, 2].plot(autocorr)\n",
    "    axes[0, 2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].axhline(0.05, color='r', linestyle='--', alpha=0.5, label='5%')\n",
    "    axes[0, 2].axhline(-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].set_title('Autocorrelation Function')\n",
    "    axes[0, 2].set_xlabel('Lag')\n",
    "    axes[0, 2].set_ylabel('ACF')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # ç´¯ç©å¹³å‡\n",
    "    cumulative_mean = np.cumsum(samples[burnin:]) / np.arange(1, len(samples[burnin:]) + 1)\n",
    "    axes[1, 0].plot(cumulative_mean)\n",
    "    true_mean = np.sum([0.3 * (-2), 0.7 * 2])  # æ··åˆåˆ†å¸ƒã®ç†è«–å¹³å‡\n",
    "    axes[1, 0].axhline(true_mean, color='r', linestyle='--', label=f'True mean = {true_mean:.2f}')\n",
    "    axes[1, 0].set_title('Cumulative Mean')\n",
    "    axes[1, 0].set_xlabel('Iteration')\n",
    "    axes[1, 0].set_ylabel('Mean')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # å—ç†ç‡ã®æ¨ç§»\n",
    "    window_size = len(samples) // 100\n",
    "    running_acceptance = []\n",
    "    for i in range(window_size, len(samples), window_size):\n",
    "        # ç°¡æ˜“çš„ãªå—ç†ç‡è¨ˆç®—ï¼ˆé€£ç¶šã™ã‚‹å€¤ã®å¤‰åŒ–ã§åˆ¤å®šï¼‰\n",
    "        window_samples = samples[i-window_size:i]\n",
    "        changes = np.sum(np.diff(window_samples) != 0)\n",
    "        running_acceptance.append(changes / window_size)\n",
    "    \n",
    "    axes[1, 1].plot(running_acceptance)\n",
    "    axes[1, 1].axhline(acceptance_rate, color='r', linestyle='--', \n",
    "                       label=f'Overall: {acceptance_rate:.3f}')\n",
    "    axes[1, 1].set_title('Running Acceptance Rate')\n",
    "    axes[1, 1].set_xlabel('Window')\n",
    "    axes[1, 1].set_ylabel('Acceptance Rate')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # QQãƒ—ãƒ­ãƒƒãƒˆï¼ˆç†è«–åˆ†å¸ƒã¨ã®æ¯”è¼ƒã¯å›°é›£ãªã®ã§ã€æ­£è¦æ€§ã®ãƒ†ã‚¹ãƒˆï¼‰\n",
    "    from scipy.stats import probplot\n",
    "    probplot(samples[burnin:], dist=\"norm\", plot=axes[1, 2])\n",
    "    axes[1, 2].set_title('Q-Q Plot (vs Normal)')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mcmc_results_1d(samples, mixture_log_pdf, title=\"Mixture Gaussian Sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.4 ææ¡ˆåˆ†å¸ƒã®é¸æŠï¼šã€Œæ­©å¹…ã€ã®ç§‘å­¦\n\nææ¡ˆåˆ†å¸ƒ $q(x'|x^{(t)})$ ã¯ã€MCMCã‚¦ã‚©ãƒ¼ã‚«ãƒ¼ã®ã€Œæ­©å¹…ã€ã‚’æ±ºå®šã—ã¾ã™ã€‚ã“ã®æ­©å¹…ã®é©åˆ‡ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã€æ¢ç´¢åŠ¹ç‡ã‚’åŠ‡çš„ã«å·¦å³ã™ã‚‹æ¥µã‚ã¦é‡è¦ãªè¦ç´ ã§ã™ã€‚\n\n### ğŸš¶â€â™‚ï¸ æ­©å¹…ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ï¼šè¡—æ­©ãã‹ã‚‰ã®æ´å¯Ÿ\n\nimagine you're exploring an unknown city to find popular areas:\n\n**ğŸŒ æ­©å¹…ãŒå°ã•ã™ãã‚‹ï¼ˆæ…é‡ã™ãã‚‹ã‚¦ã‚©ãƒ¼ã‚«ãƒ¼ï¼‰:**\n- ä¸€æ­©ãšã¤æ…é‡ã«é€²ã‚€ â†’ ã©ã“ã«è¡Œã£ã¦ã‚‚ã€Œã¾ã‚ã€ã„ã„ã‹ã€ã¨å—ã‘å…¥ã‚Œã‚‹\n- âœ… æ¡æŠç‡ã¯é«˜ã„ï¼ˆã»ã¼100%è¿‘ãï¼‰\n- âŒ åŒã˜ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã¡ã¾ã¡ã¾ã¨æ­©ãå›ã‚‹ã ã‘\n- âŒ è¡—å…¨ä½“ã®äººæ°—ã‚¨ãƒªã‚¢ã‚’æŠŠæ¡ã§ããªã„ï¼ˆéåŠ¹ç‡ãªæ¢ç´¢ï¼‰\n\n**ğŸƒâ€â™‚ï¸ æ­©å¹…ãŒå¤§ãã™ãã‚‹ï¼ˆå†’é™ºçš„ã™ãã‚‹ã‚¦ã‚©ãƒ¼ã‚«ãƒ¼ï¼‰:**\n- å¤§ããã‚¸ãƒ£ãƒ³ãƒ—ã—ã¦ã¿ã‚‹ â†’ ã§ã‚‚æ®†ã©ã®å ´æ‰€ãŒã€Œå¾®å¦™...ã€ã§å…ƒã«æˆ»ã‚‹\n- âŒ æ¡æŠç‡ãŒæ¥µç«¯ã«ä½ã„ï¼ˆ10%ä»¥ä¸‹ï¼‰\n- âŒ ã»ã¼åŒã˜å ´æ‰€ã«ç•™ã¾ã‚Šç¶šã‘ã‚‹\n- âŒ æ–°ã—ã„å ´æ‰€ã‚’å—ã‘å…¥ã‚Œã¦ã‚‚ã‚‰ãˆãªã„ï¼ˆæ¢ç´¢åœæ»ï¼‰\n\n**ğŸ¯ æ­©å¹…ãŒé©åˆ‡ï¼ˆè³¢ã„ã‚¦ã‚©ãƒ¼ã‚«ãƒ¼ï¼‰:**\n- ç¨‹ã‚ˆã„ã‚¹ãƒ†ãƒƒãƒ—ã§ç€å®Ÿã«æ¢ç´¢ â†’ è‰¯ã„å ´æ‰€ã¯å—ã‘å…¥ã‚Œã€æ‚ªã„å ´æ‰€ã¯æ™‚ã€…ã‚¹ã‚­ãƒƒãƒ—\n- âœ… é©åº¦ãªæ¡æŠç‡ï¼ˆç†è«–æœ€é©å€¤ï¼š1Dâ†’44%, é«˜æ¬¡å…ƒâ†’23%ï¼‰\n- âœ… è¡—å…¨ä½“ã‚’åŠ¹ç‡ã‚ˆãæ¢ç´¢\n- âœ… äººæ°—ã‚¨ãƒªã‚¢ã«ã¯é•·ãæ»åœ¨ã€ãã†ã§ãªã„å ´æ‰€ã‚‚ãƒãƒ©ãƒ³ã‚¹ã‚ˆãè¨ªå•\n\n### ğŸ“Š æ¡æŠç‡ã¨æ¢ç´¢åŠ¹ç‡ã®é–¢ä¿‚\n\n```\næ¡æŠç‡ 100% â†’ ç§»å‹•è·é›¢ãŒæ¥µå° â†’ æ¢ç´¢ãŒéåŠ¹ç‡\næ¡æŠç‡  50% â†’ é©åº¦ãªç§»å‹•è·é›¢ â†’ è‰¯å¥½ãªãƒãƒ©ãƒ³ã‚¹  \næ¡æŠç‡  10% â†’ ç§»å‹•ãŒã»ã¼åœæ­¢ â†’ æ¢ç´¢ãŒåœæ»\næ¡æŠç‡   0% â†’ å®Œå…¨ã«åœæ­¢    â†’ æ¢ç´¢ä¸å¯èƒ½\n```\n\n### ğŸ”¬ ç†è«–æœ€é©å€¤ã®æ·±ã„æ„å‘³\n\næœ€é©æ¡æŠç‡ï¼ˆ1D: 44%, é«˜æ¬¡å…ƒ: 23%ï¼‰ã¯ã€**æ¢ç´¢è·é›¢ã®2ä¹—å¹³å‡**ï¼ˆã¤ã¾ã‚ŠåŠ¹ç‡ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ï¼š\n\n$$\\text{åŠ¹ç‡} = \\lim_{t \\to \\infty} \\frac{E[|X_t - X_0|^2]}{t}$$\n\nã“ã®å€¤ã¯ã€ä»¥ä¸‹ã®å¾®å¦™ãªãƒãƒ©ãƒ³ã‚¹ã®æœ€é©è§£ï¼š\n- **æ¡æŠé »åº¦**: æ–°ã—ã„å ´æ‰€ã‚’å—ã‘å…¥ã‚Œã‚‹é »åº¦\n- **ç§»å‹•è·é›¢**: ä¸€å›ã®æ¡æŠã§ã®ç§»å‹•è·é›¢\n\næ¡æŠç‡ãŒé«˜ã™ãã‚‹ã¨ç§»å‹•è·é›¢ãŒå°ã•ãã€ä½ã™ãã‚‹ã¨æ¡æŠé »åº¦ãŒä½ããªã‚Šã¾ã™ã€‚\n\n### ğŸ¨ è¦–è¦šçš„ç†è§£ï¼šæ­©å¹…ã¨è»Œè·¡ãƒ‘ã‚¿ãƒ¼ãƒ³\n\nç•°ãªã‚‹æ­©å¹…è¨­å®šã§ã®å…¸å‹çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼š\n\n```\nå°ã•ã„æ­©å¹…: ~~~~~~~~ (ç´°ã‹ã„æŒ¯å‹•ã€å±€æ‰€çš„)\né©åˆ‡ãªæ­©å¹…: âˆ©âˆªâˆ©âˆªâˆ©âˆª (é©åº¦ãªèµ·ä¼ã€åºƒåŸŸæ¢ç´¢)  \nå¤§ãã„æ­©å¹…: ------   (å¹³å¦ã€åœæ»)\n```\n\næ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€ã“ã‚Œã‚‰ã®é•ã„ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_step_sizes(step_sizes, n_samples=5000):\n",
    "    \"\"\"\n",
    "    ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§ã®æ€§èƒ½æ¯”è¼ƒ\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for step_size in step_sizes:\n",
    "        print(f\"Testing step size: {step_size}\")\n",
    "        \n",
    "        samples, acc_rate, _ = metropolis_hastings(\n",
    "            target_log_pdf=mixture_log_pdf,\n",
    "            proposal_sampler=lambda x: random_walk_sampler(x, step_size),\n",
    "            proposal_log_pdf=symmetric_proposal_log_pdf,\n",
    "            initial_value=0.0,\n",
    "            n_samples=n_samples\n",
    "        )\n",
    "        \n",
    "        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—ï¼ˆè‡ªå·±ç›¸é–¢ã‚’è€ƒæ…®ï¼‰\n",
    "        from statsmodels.tsa.stattools import acf\n",
    "        burnin = n_samples // 5\n",
    "        autocorr = acf(samples[burnin:], nlags=min(200, len(samples[burnin:])//4), fft=True)\n",
    "        \n",
    "        # æœ€åˆã«0.05ã‚’ä¸‹å›ã‚‹ãƒ©ã‚°ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        tau_int = 1\n",
    "        for lag in range(1, len(autocorr)):\n",
    "            if autocorr[lag] < 0.05:\n",
    "                tau_int = lag\n",
    "                break\n",
    "        \n",
    "        eff_sample_size = len(samples[burnin:]) / (2 * tau_int + 1)\n",
    "        \n",
    "        results[step_size] = {\n",
    "            'samples': samples,\n",
    "            'acceptance_rate': acc_rate,\n",
    "            'autocorr_time': tau_int,\n",
    "            'eff_sample_size': eff_sample_size,\n",
    "            'efficiency': eff_sample_size * acc_rate  # ç·åˆåŠ¹ç‡ã®æŒ‡æ¨™\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§æ¯”è¼ƒ\n",
    "step_sizes = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "comparison_results = compare_step_sizes(step_sizes)\n",
    "\n",
    "# çµæœã®è¡¨ç¤º\n",
    "print(\"\\n=== Step Size Comparison ===\")\n",
    "print(f\"{'Step Size':<10} {'Acc Rate':<10} {'Autocorr':<10} {'Eff Size':<12} {'Efficiency':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for step_size in step_sizes:\n",
    "    result = comparison_results[step_size]\n",
    "    print(f\"{step_size:<10.1f} {result['acceptance_rate']:<10.3f} \"\n",
    "          f\"{result['autocorr_time']:<10d} {result['eff_sample_size']:<12.1f} \"\n",
    "          f\"{result['efficiency']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºæ¯”è¼ƒã®åŒ…æ‹¬çš„å¯è¦–åŒ–\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\n# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®æŠ½å‡º\nstep_sizes_plot = list(comparison_results.keys())\nacc_rates = [comparison_results[s]['acceptance_rate'] for s in step_sizes_plot]\nautocorr_times = [comparison_results[s]['autocorr_time'] for s in step_sizes_plot]\neff_sizes = [comparison_results[s]['eff_sample_size'] for s in step_sizes_plot]\nefficiencies = [comparison_results[s]['efficiency'] for s in step_sizes_plot]\n\n# 1. å—ç†ç‡ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\naxes[0, 0].semilogx(step_sizes_plot, acc_rates, 'bo-', markersize=8, linewidth=2)\naxes[0, 0].axhline(0.44, color='red', linestyle='--', alpha=0.8, linewidth=2, label='ç†è«–æœ€é© (44%)')\naxes[0, 0].axhspan(0.2, 0.7, alpha=0.2, color='green', label='æ¨å¥¨ç¯„å›²')\naxes[0, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=12)\naxes[0, 0].set_ylabel('å—ç†ç‡', fontsize=12)\naxes[0, 0].set_title('ğŸ¯ å—ç†ç‡ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=14)\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. è‡ªå·±ç›¸é–¢æ™‚é–“ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\naxes[0, 1].semilogx(step_sizes_plot, autocorr_times, 'go-', markersize=8, linewidth=2)\naxes[0, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=12)\naxes[0, 1].set_ylabel('è‡ªå·±ç›¸é–¢æ™‚é–“', fontsize=12)\naxes[0, 1].set_title('â±ï¸ è‡ªå·±ç›¸é–¢æ™‚é–“ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=14)\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\naxes[0, 2].semilogx(step_sizes_plot, eff_sizes, 'mo-', markersize=8, linewidth=2)\naxes[0, 2].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=12)\naxes[0, 2].set_ylabel('æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º', fontsize=12)\naxes[0, 2].set_title('ğŸ“Š æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=14)\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. ç·åˆåŠ¹ç‡ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\naxes[0, 3].semilogx(step_sizes_plot, efficiencies, 'ro-', markersize=8, linewidth=2)\noptimal_idx = np.argmax(efficiencies)\naxes[0, 3].scatter(step_sizes_plot[optimal_idx], efficiencies[optimal_idx], \n                  color='gold', s=200, marker='*', zorder=5, label=f'æœ€é©å€¤ ({step_sizes_plot[optimal_idx]})')\naxes[0, 3].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=12)\naxes[0, 3].set_ylabel('ç·åˆåŠ¹ç‡', fontsize=12)\naxes[0, 3].set_title('â­ ç·åˆåŠ¹ç‡ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º', fontsize=14)\naxes[0, 3].legend()\naxes[0, 3].grid(True, alpha=0.3)\n\n# 5-8. ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã®æ¯”è¼ƒï¼ˆæœ€åˆã®1000ã‚µãƒ³ãƒ—ãƒ«ï¼‰\ncolors = ['blue', 'green', 'orange', 'red']\nstep_examples = [0.1, 0.5, 1.0, 5.0]\ntitles = ['ğŸŒ éå°ã‚¹ãƒ†ãƒƒãƒ— (0.1)', 'âœ… é©åˆ‡ã‚¹ãƒ†ãƒƒãƒ— (0.5)', 'âš¡ ã‚„ã‚„å¤§ã‚¹ãƒ†ãƒƒãƒ— (1.0)', 'ğŸƒâ€â™‚ï¸ éå¤§ã‚¹ãƒ†ãƒƒãƒ— (5.0)']\n\nfor i, (step_size, title, color) in enumerate(zip(step_examples, titles, colors)):\n    if step_size in comparison_results:\n        samples = comparison_results[step_size]['samples'][:1000]\n        acc_rate = comparison_results[step_size]['acceptance_rate']\n        autocorr = comparison_results[step_size]['autocorr_time']\n        \n        axes[1, i].plot(samples, alpha=0.8, color=color, linewidth=1)\n        axes[1, i].set_title(f'{title}\\nAcc:{acc_rate:.3f}, ACT:{autocorr}', fontsize=11)\n        axes[1, i].set_xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')\n        axes[1, i].set_ylabel('å€¤')\n        axes[1, i].grid(True, alpha=0.3)\n\n# 9-12. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®æ¯”è¼ƒ\nx_range = np.linspace(-6, 6, 1000)\ntrue_density = np.exp(mixture_log_pdf(x_range))\n\nfor i, (step_size, color) in enumerate(zip(step_examples, colors)):\n    if step_size in comparison_results:\n        samples = comparison_results[step_size]['samples'][1000:]  # burniné™¤å»\n        \n        axes[2, i].hist(samples, bins=50, density=True, alpha=0.7, color=color, \n                       edgecolor='black', linewidth=0.5)\n        axes[2, i].plot(x_range, true_density, 'r-', linewidth=3, label='çœŸã®åˆ†å¸ƒ')\n        axes[2, i].set_title(f'ã‚¹ãƒ†ãƒƒãƒ— {step_size}: åˆ†å¸ƒæ¯”è¼ƒ', fontsize=11)\n        axes[2, i].set_xlabel('å€¤')\n        axes[2, i].set_ylabel('å¯†åº¦')\n        axes[2, i].legend()\n\nplt.tight_layout()\nplt.suptitle('ğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºæœ€é©åŒ–ã®åŒ…æ‹¬çš„åˆ†æ', fontsize=16, y=1.02)\nplt.show()\n\n# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤º\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ\")\nprint(\"=\"*80)\nprint(f\"{'ã‚¹ãƒ†ãƒƒãƒ—':<8} {'å—ç†ç‡':<8} {'è‡ªå·±ç›¸é–¢':<10} {'æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«':<12} {'ç·åˆåŠ¹ç‡':<10} {'è©•ä¾¡':<15}\")\nprint(\"-\"*80)\n\nfor i, step_size in enumerate(step_sizes_plot):\n    result = comparison_results[step_size]\n    acc_rate = result['acceptance_rate']\n    \n    # è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯\n    if 0.35 <= acc_rate <= 0.55:\n        rating = \"âœ… å„ªç§€\"\n    elif 0.25 <= acc_rate <= 0.65:\n        rating = \"âš¡ è‰¯å¥½\"\n    elif 0.15 <= acc_rate <= 0.75:\n        rating = \"âš ï¸ æ³¨æ„\"\n    else:\n        rating = \"âŒ ä¸è‰¯\"\n    \n    print(f\"{step_size:<8.1f} {acc_rate:<8.3f} {result['autocorr_time']:<10d} \"\n          f\"{result['eff_sample_size']:<12.1f} {result['efficiency']:<10.1f} {rating:<15}\")\n\nprint(\"-\"*80)\nprint(\"ğŸ’¡ æœ€é©åŒ–ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:\")\nprint(\"â€¢ å—ç†ç‡ 35-55%: ç†æƒ³çš„ãªç¯„å›²\")\nprint(\"â€¢ å—ç†ç‡ 25-65%: è¨±å®¹ç¯„å›²\") \nprint(\"â€¢ å—ç†ç‡ < 25% or > 65%: è¦èª¿æ•´\")\nprint(\"â€¢ è‡ªå·±ç›¸é–¢æ™‚é–“: å°ã•ã„ã»ã©è‰¯ã„\")\nprint(\"â€¢ ç·åˆåŠ¹ç‡: å—ç†ç‡ Ã— æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 ä¾‹2ï¼šéå¯¾ç§°ãªææ¡ˆåˆ†å¸ƒ\n",
    "\n",
    "ä»Šåº¦ã¯éå¯¾ç§°ãªææ¡ˆåˆ†å¸ƒã‚’ä½¿ã£ãŸä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡æ•°åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "def exponential_log_pdf(x, rate=1.0):\n",
    "    \"\"\"æŒ‡æ•°åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    if x < 0:\n",
    "        return -np.inf\n",
    "    return np.log(rate) - rate * x\n",
    "\n",
    "# éå¯¾ç§°ãªææ¡ˆåˆ†å¸ƒï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰\n",
    "def lognormal_proposal_sampler(current, sigma=0.5):\n",
    "    \"\"\"å¯¾æ•°æ­£è¦åˆ†å¸ƒã«ã‚ˆã‚‹ææ¡ˆ\"\"\"\n",
    "    return current * np.exp(np.random.normal(0, sigma))\n",
    "\n",
    "def lognormal_proposal_log_pdf(proposed, current, sigma=0.5):\n",
    "    \"\"\"å¯¾æ•°æ­£è¦ææ¡ˆåˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    if proposed <= 0 or current <= 0:\n",
    "        return -np.inf\n",
    "    log_ratio = np.log(proposed / current)\n",
    "    return -0.5 * (log_ratio / sigma)**2 - 0.5 * np.log(2 * np.pi * sigma**2) - np.log(proposed)\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "rate_param = 2.0\n",
    "samples_exp, acceptance_rate_exp, _ = metropolis_hastings(\n",
    "    target_log_pdf=lambda x: exponential_log_pdf(x, rate_param),\n",
    "    proposal_sampler=lambda x: lognormal_proposal_sampler(x, 0.3),\n",
    "    proposal_log_pdf=lambda p, c: lognormal_proposal_log_pdf(p, c, 0.3),\n",
    "    initial_value=1.0,\n",
    "    n_samples=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\næŒ‡æ•°åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å—ç†ç‡: {acceptance_rate_exp:.3f}\")\n",
    "print(f\"ç†è«–å¹³å‡: {1/rate_param:.3f}, ã‚µãƒ³ãƒ—ãƒ«å¹³å‡: {np.mean(samples_exp[2000:]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡æ•°åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "burnin = 2000\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "axes[0, 0].plot(samples_exp[:3000], alpha=0.7)\n",
    "axes[0, 0].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n",
    "axes[0, 0].set_title('Trace Plot')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨çœŸã®åˆ†å¸ƒ\n",
    "axes[0, 1].hist(samples_exp[burnin:], bins=50, density=True, alpha=0.7, \n",
    "                color='lightblue', label='MCMC samples')\n",
    "x_range = np.linspace(0, np.percentile(samples_exp[burnin:], 95), 1000)\n",
    "true_density = rate_param * np.exp(-rate_param * x_range)\n",
    "axes[0, 1].plot(x_range, true_density, 'r-', linewidth=2, label='True exponential')\n",
    "axes[0, 1].set_title('Sample Distribution vs True Distribution')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæŒ‡æ•°åˆ†å¸ƒã¨æ¯”è¼ƒï¼‰\n",
    "from scipy.stats import probplot\n",
    "probplot(samples_exp[burnin:], dist=stats.expon, sparams=(0, 1/rate_param), plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot vs Exponential Distribution')\n",
    "\n",
    "# ç´¯ç©åˆ†å¸ƒé–¢æ•°ã®æ¯”è¼ƒ\n",
    "sorted_samples = np.sort(samples_exp[burnin:])\n",
    "empirical_cdf = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)\n",
    "theoretical_cdf = 1 - np.exp(-rate_param * sorted_samples)\n",
    "\n",
    "axes[1, 1].plot(sorted_samples, empirical_cdf, 'b-', alpha=0.7, label='Empirical CDF')\n",
    "axes[1, 1].plot(sorted_samples, theoretical_cdf, 'r-', linewidth=2, label='Theoretical CDF')\n",
    "axes[1, 1].set_title('CDF Comparison')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 å¤šå¤‰é‡åˆ†å¸ƒã¸ã®æ‹¡å¼µ\n",
    "\n",
    "2æ¬¡å…ƒã®å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2æ¬¡å…ƒå¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ\n",
    "def multivariate_normal_log_pdf(x, mu, cov):\n",
    "    \"\"\"å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    k = len(mu)\n",
    "    diff = x - mu\n",
    "    \n",
    "    # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®è¨ˆç®—\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        log_det = 2 * np.sum(np.log(np.diag(chol)))\n",
    "        solve = np.linalg.solve(chol, diff)\n",
    "        mahalanobis_sq = np.sum(solve**2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf\n",
    "    \n",
    "    return -0.5 * (k * np.log(2 * np.pi) + log_det + mahalanobis_sq)\n",
    "\n",
    "# å¤šå¤‰é‡ææ¡ˆåˆ†å¸ƒ\n",
    "def multivariate_proposal_sampler(current, cov_proposal):\n",
    "    \"\"\"å¤šå¤‰é‡æ­£è¦ææ¡ˆåˆ†å¸ƒ\"\"\"\n",
    "    return np.random.multivariate_normal(current, cov_proposal)\n",
    "\n",
    "def multivariate_proposal_log_pdf(proposed, current, cov_proposal):\n",
    "    \"\"\"å¤šå¤‰é‡æ­£è¦ææ¡ˆåˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦ï¼ˆå¯¾ç§°ãªã®ã§0ï¼‰\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "mu_target = np.array([1.0, 2.0])\n",
    "cov_target = np.array([[1.0, 0.7], [0.7, 2.0]])\n",
    "cov_proposal = 0.5 * np.eye(2)\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "samples_mv, acceptance_rate_mv, _ = metropolis_hastings(\n",
    "    target_log_pdf=lambda x: multivariate_normal_log_pdf(x, mu_target, cov_target),\n",
    "    proposal_sampler=lambda x: multivariate_proposal_sampler(x, cov_proposal),\n",
    "    proposal_log_pdf=lambda p, c: multivariate_proposal_log_pdf(p, c, cov_proposal),\n",
    "    initial_value=np.array([0.0, 0.0]),\n",
    "    n_samples=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nå¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å—ç†ç‡: {acceptance_rate_mv:.3f}\")\n",
    "print(f\"ç†è«–å¹³å‡: {mu_target}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«å¹³å‡: {np.mean(samples_mv[2000:], axis=0)}\")\n",
    "print(f\"ç†è«–å…±åˆ†æ•£:\\n{cov_target}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£:\\n{np.cov(samples_mv[2000:].T)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šå¤‰é‡çµæœã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "burnin = 2000\n",
    "samples_clean = samples_mv[burnin:]\n",
    "\n",
    "# æ•£å¸ƒå›³\n",
    "axes[0, 0].scatter(samples_clean[:, 0], samples_clean[:, 1], alpha=0.6, s=1)\n",
    "axes[0, 0].set_xlabel('X1')\n",
    "axes[0, 0].set_ylabel('X2')\n",
    "axes[0, 0].set_title('Scatter Plot of Samples')\n",
    "axes[0, 0].set_aspect('equal')\n",
    "\n",
    "# ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆ\n",
    "x1_range = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 50)\n",
    "x2_range = np.linspace(samples_clean[:, 1].min(), samples_clean[:, 1].max(), 50)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "pos = np.dstack((X1, X2))\n",
    "\n",
    "# çœŸã®åˆ†å¸ƒã®ç­‰é«˜ç·š\n",
    "rv = stats.multivariate_normal(mu_target, cov_target)\n",
    "axes[0, 1].contour(X1, X2, rv.pdf(pos), colors='red', alpha=0.8)\n",
    "axes[0, 1].scatter(samples_clean[::10, 0], samples_clean[::10, 1], alpha=0.3, s=1)\n",
    "axes[0, 1].set_xlabel('X1')\n",
    "axes[0, 1].set_ylabel('X2')\n",
    "axes[0, 1].set_title('Samples with True Distribution Contours')\n",
    "\n",
    "# ãƒãƒ¼ã‚¸ãƒŠãƒ«åˆ†å¸ƒ\n",
    "axes[0, 2].hist(samples_clean[:, 0], bins=50, density=True, alpha=0.7, label='X1 samples')\n",
    "x1_theory = np.linspace(samples_clean[:, 0].min(), samples_clean[:, 0].max(), 100)\n",
    "axes[0, 2].plot(x1_theory, stats.norm.pdf(x1_theory, mu_target[0], np.sqrt(cov_target[0, 0])), \n",
    "                'r-', label='X1 true')\n",
    "axes[0, 2].set_title('Marginal Distribution X1')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "axes[1, 0].plot(samples_mv[:3000, 0], alpha=0.7, label='X1')\n",
    "axes[1, 0].plot(samples_mv[:3000, 1], alpha=0.7, label='X2')\n",
    "axes[1, 0].axvline(burnin, color='red', linestyle='--', alpha=0.7, label='Burn-in')\n",
    "axes[1, 0].set_title('Trace Plot')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# ç¬¬2ãƒãƒ¼ã‚¸ãƒŠãƒ«åˆ†å¸ƒ\n",
    "axes[1, 1].hist(samples_clean[:, 1], bins=50, density=True, alpha=0.7, label='X2 samples')\n",
    "x2_theory = np.linspace(samples_clean[:, 1].min(), samples_clean[:, 1].max(), 100)\n",
    "axes[1, 1].plot(x2_theory, stats.norm.pdf(x2_theory, mu_target[1], np.sqrt(cov_target[1, 1])), \n",
    "                'r-', label='X2 true')\n",
    "axes[1, 1].set_title('Marginal Distribution X2')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# ç›¸é–¢ã®åæŸ\n",
    "n_points = len(samples_clean)\n",
    "window_size = n_points // 100\n",
    "correlations = []\n",
    "for i in range(window_size, n_points, window_size):\n",
    "    window_samples = samples_clean[i-window_size:i]\n",
    "    corr = np.corrcoef(window_samples[:, 0], window_samples[:, 1])[0, 1]\n",
    "    correlations.append(corr)\n",
    "\n",
    "axes[1, 2].plot(correlations)\n",
    "true_corr = cov_target[0, 1] / np.sqrt(cov_target[0, 0] * cov_target[1, 1])\n",
    "axes[1, 2].axhline(true_corr, color='red', linestyle='--', \n",
    "                   label=f'True correlation = {true_corr:.3f}')\n",
    "axes[1, 2].set_title('Running Correlation')\n",
    "axes[1, 2].set_xlabel('Window')\n",
    "axes[1, 2].set_ylabel('Correlation')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 æ¼”ç¿’å•é¡Œ\n",
    "\n",
    "### å•é¡Œ1ï¼šãƒ™ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ $\\text{Beta}(\\alpha=2, \\beta=5)$ ã‹ã‚‰ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãªã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ1ã®è§£ç­”æ¬„\n",
    "def beta_log_pdf(x, alpha=2, beta=5):\n",
    "    \"\"\"ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦\"\"\"\n",
    "    if x <= 0 or x >= 1:\n",
    "        return -np.inf\n",
    "    return (alpha - 1) * np.log(x) + (beta - 1) * np.log(1 - x)\n",
    "\n",
    "# ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "# ãƒ’ãƒ³ãƒˆï¼š[0,1]åŒºé–“ã«åˆ¶ç´„ãŒã‚ã‚‹ã®ã§ã€ææ¡ˆãŒç¯„å›²å¤–ã®å ´åˆã®å‡¦ç†ãŒå¿…è¦\n",
    "\n",
    "pass  # å­¦ç¿’è€…ãŒå®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å•é¡Œ2ï¼šæœ€é©ãªå—ç†ç‡ã®èª¿æŸ»\n",
    "1æ¬¡å…ƒæ­£è¦åˆ†å¸ƒã«å¯¾ã—ã¦ã€ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§å—ç†ç‡ã¨åŠ¹ç‡ã‚’èª¿ã¹ã€æœ€é©ãªå—ç†ç‡ï¼ˆç´„23%ï¼‰ãŒå®Ÿéš›ã«åŠ¹ç‡çš„ã‹ã‚’ç¢ºèªã—ãªã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡Œ2ã®è§£ç­”æ¬„\n",
    "def standard_normal_log_pdf(x):\n",
    "    return -0.5 * x**2 - 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "# ã“ã“ã«å®Ÿè£…ã—ã¦ãã ã•ã„\n",
    "# ãƒ’ãƒ³ãƒˆï¼šè¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§å®Ÿé¨“ã—ã€å—ç†ç‡ã¨è‡ªå·±ç›¸é–¢æ™‚é–“ã®é–¢ä¿‚ã‚’èª¿ã¹ã‚‹\n",
    "\n",
    "pass  # å­¦ç¿’è€…ãŒå®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ã¾ã¨ã‚ï¼šMHæ³•ãƒã‚¹ã‚¿ãƒ¼ã¸ã®é“\n\nã“ã®ç« ã§ã¯ã€ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ãƒ»ãƒ˜ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹æ³•ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸï¼š\n\n### ğŸ§  æ ¸å¿ƒçš„ç†è§£\n\n1. **é©å‘½çš„ã‚¢ã‚¤ãƒ‡ã‚¢**ï¼š\n   - æ­£è¦åŒ–å®šæ•°ã®å·§å¦™ãªç›¸æ®ºãƒ¡ã‚«ãƒ‹ã‚ºãƒ \n   - è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã«ã‚ˆã‚‹ç†è«–çš„ä¿è¨¼\n   - ã€Œè³¢ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€ã¨ã—ã¦ã®ç›´æ„Ÿçš„ç†è§£\n\n2. **æ•°å­¦çš„ç¾ã—ã•**ï¼š\n   - å—ç†ç¢ºç‡Î± ã®å°å‡ºã¨ç‰©ç†å­¦çš„è§£é‡ˆ\n   - ç¢ºç‡æ¯”ã«ã‚ˆã‚‹åŠ¹ç‡çš„è¨ˆç®—\n   - å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ•°å€¤å®‰å®šæ€§\n\n### ğŸ”§ å®Ÿè·µçš„ã‚¹ã‚­ãƒ«\n\n3. **ææ¡ˆåˆ†å¸ƒã®è¨­è¨ˆå“²å­¦**ï¼š\n   - **ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º**: MCMCã‚¦ã‚©ãƒ¼ã‚«ãƒ¼ã®ã€Œæ­©å¹…ã€\n   - **æœ€é©å—ç†ç‡**: 1Dâ†’44%, é«˜æ¬¡å…ƒâ†’23%ã®ç†è«–çš„æ ¹æ‹ \n   - **åŠ¹ç‡æŒ‡æ¨™**: è‡ªå·±ç›¸é–¢æ™‚é–“ã¨æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º\n\n4. **æ€§èƒ½è¨ºæ–­ã®ä½“ç³»**ï¼š\n   - ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼šæ··åˆã¨åæŸã®è¦–è¦šçš„ç¢ºèª\n   - å—ç†ç‡ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½è©•ä¾¡\n   - åˆ†å¸ƒæ¯”è¼ƒï¼šç†è«–å€¤ã¨ã®æ•´åˆæ€§æ¤œè¨¼\n\n### ğŸš€ å®Ÿè£…ã«ãŠã‘ã‚‹é‡è¦ãƒã‚¤ãƒ³ãƒˆ\n\n**æŠ€è¡“çš„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**\n- **å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—**: æ•°å€¤ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿\n- **å¯¾ç§°æ€§ã®æ´»ç”¨**: è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›\n- **å¢ƒç•Œæ¡ä»¶ã®å‡¦ç†**: åˆ¶ç´„ä»˜ãåˆ†å¸ƒã§ã®å®Ÿè£…\n- **é©å¿œçš„èª¿æ•´**: å—ç†ç‡ã«åŸºã¥ãè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n\n**è¨­è¨ˆå“²å­¦:**\n- **æ±ç”¨æ€§**: ã‚ã‚‰ã‚†ã‚‹é€£ç¶šåˆ†å¸ƒã«é©ç”¨å¯èƒ½\n- **å …ç‰¢æ€§**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¤è¨­å®šã«å¯¾ã™ã‚‹è¨±å®¹åº¦\n- **æ‹¡å¼µæ€§**: é«˜æ¬¡å…ƒå•é¡Œã¸ã®è‡ªç„¶ãªæ‹¡å¼µ\n\n### ğŸ¯ å¿œç”¨æˆ¦ç•¥\n\n**åˆ†å¸ƒç‰¹æ€§ã«å¿œã˜ãŸæˆ¦ç•¥:**\n- **å˜å³°æ€§åˆ†å¸ƒ**: æ¨™æº–çš„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯\n- **å¤šå³°æ€§åˆ†å¸ƒ**: å¤§ãã‚ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º\n- **åˆ¶ç´„ä»˜ãåˆ†å¸ƒ**: å¢ƒç•Œåå°„ã¾ãŸã¯å¤‰æ•°å¤‰æ›\n- **é«˜æ¬¡å…ƒåˆ†å¸ƒ**: æˆåˆ†åˆ¥æ›´æ–°ã¾ãŸã¯ãƒ–ãƒ­ãƒƒã‚¯æ›´æ–°\n\n### ğŸ”® æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n\nMHæ³•ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã‚ãªãŸã¯ã€MCMCã®**åŸºç›¤æŠ€è¡“**ã‚’å®Œå…¨ã«ç†è§£ã—ã¾ã—ãŸã€‚\n\n**Chapter 3ï¼ˆã‚®ãƒ–ã‚¹ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰**ã§ã¯ï¼š\n- MHæ³•ã®**ç‰¹æ®ŠåŒ–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–**\n- æ¡ä»¶ä»˜ãåˆ†å¸ƒã®æ´»ç”¨æˆ¦ç•¥\n- é«˜æ¬¡å…ƒå•é¡Œã§ã®å®Ÿç”¨çš„è§£æ±ºç­–\n- éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã§ã®å¨åŠ›\n\n**Chapter 4ï¼ˆåæŸè¨ºæ–­ï¼‰**ã§ã¯ï¼š\n- MHæ³•ã®**å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ **\n- è‡ªå‹•è¨ºæ–­ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Š\n- å®Ÿéš›ã®ç ”ç©¶ã§ã®æ´»ç”¨æ³•\n\n### ğŸ’¡ é‡è¦ãªå“²å­¦\n\n> MHæ³•ã¯ã€Œå®Œç’§ãªè§£ã€ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€ã€Œååˆ†ã«è‰¯ã„è¿‘ä¼¼ã€ã‚’åŠ¹ç‡çš„ã«å¾—ã‚‹æ‰‹æ³•ã§ã™ã€‚ã“ã®å“²å­¦ã¯ã€ç¾ä»£ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹å…¨ä½“ã«é€šã˜ã‚‹é‡è¦ãªè€ƒãˆæ–¹ã§ã™ã€‚\n\n**è¨˜æ†¶ã™ã¹ãé‡‘è¨€:**\n- ã€Œå—ç†ç‡44%ã¯æ‹’çµ¶ã‚’æã‚Œãªã„å‹‡æ°—ã®è¨¼ã€\n- ã€Œå®Œç’§ãªææ¡ˆã‚ˆã‚Šã€é©åˆ‡ãªææ¡ˆã‚’ç¶™ç¶šã™ã‚‹ã€\n- ã€ŒåæŸã¯ç›®æ¨™ã§ã¯ãªãã€æ¢ç´¢ã®è³ªã‚’ç¤ºã™æŒ‡æ¨™ã€\n\nã‚ãªãŸã¯ä»Šã€MCMCã®å¿ƒè‡“éƒ¨ã§ã‚ã‚‹MHæ³•ã‚’ä½¿ã„ã“ãªã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ¬¡ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ‰‹æ³•ã§ã€ã•ã‚‰ã«é«˜ã„åŠ¹ç‡ã‚’è¿½æ±‚ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}